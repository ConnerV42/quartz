{"/":{"title":"cvwiki","content":"\nWelcome to cvwiki, my personal web of knowledge. Feel free to bookmark this web page. I update this site often, and continuously collect knowledge and refine ideas here.\n\nYou can perform a full text search by typing `ctrl + k`.\n\nEach new post (or update to an existing post) can be found in [here](/notes).\n\n- [Software Engineering Wiki](/notes/software-engineering.md)\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [Recipes](/notes/recipes.md)","lastmodified":"2022-11-06T05:20:38.302382278Z","tags":null},"/notes/Docker":{"title":"Docker","content":"\n## Writing a Dockerfile for Spring\n\n- Use docker multi stage build\n```Dockerfile\nFROM maven:3.6.3-openjdk-11-slim as builder\n\nWORKDIR /app\nCOPY pom.xml .\n# Use this optimization to cache the local dependencies. Works as long as the POM doesn't change\nRUN mvn dependency:go-offline\n\nCOPY src/ /app/src/\nRUN mvn package\n\n# Use AdoptOpenJDK for base image.\nFROM adoptopenjdk/openjdk11:jre-11.0.8_10-alpine\n\n# Copy the jar to the production image from the builder stage.\nCOPY --from=builder /app/target/*.jar /app.jar\n\n# Run the web service on container startup.\nCMD [\"java\", \"-jar\", \"/app.jar\"]\n```\n\n## Useful Commands\n- `docker container ls` lists all running containers\n- `docker container ls -a` lists all containers\n- `docker container prune`  add -f or --force to not prompt for confirmation\n- `docker images` lists all images that are present locally\n- `docker image rm IMAGE_ID` to remove an image\n- `docker image prune`  add -f or --force to not prompt for confirmation\n- `docker system df` shows disk usage and size of 'Build Cache'\n\nYou can run an interactive shell container using that image and explore whatever content that image has. For instance:\n```\ndocker run -it image_name sh\n```\n\nOr the following for images with an `entrypoint`:\n```\ndocker run -it --entrypoint sh image_name\n```\n\nOr, if you want to see how the image was build, meaning the steps in its `Dockerfile`, you can:\n```\ndocker image history --no-trunc image_name \u003e image_history\n```\n\nThe steps will be logged into the `image_history` file.\n- Load Docker Image from tar file\n```\ndocker load -i application-image.tar \n```\n\n- Inspect image\n```\ndocker inspect application\n```","lastmodified":"2022-11-06T05:20:38.302382278Z","tags":null},"/notes/air-fried-frozen-broccoli":{"title":"Air Fried Broccoli","content":"\n## Ingredients\n-   24 oz frozen broccoli florets\n-   1/2 teaspoon salt\n-   1/2 teaspoon pepper\n-   1 teaspoon garlic powder\n-   3/4 teaspoon onion powder\n-   olive oil spray\n\n## Cooking Equipment\n- Air Fryer\n\n## Instructions\n1. Make sure the air fryer is clean.\n2. Spray the air fryer basket with olive oil spray, then place the frozen broccoli florets in the basket. For best results, do not allow the broccoli to thaw at all prior to baking, place it straight from the freezer into the air fryer.\n3. Spray the broccoli florets with olive oil spray, then sprinkle on the salt, pepper, garlic powder, and onion powder. Give the basket a shake to mix the spices around.\n4. Air fry at 400 degrees for 10 minutes. Stir and shake the basket, then air fry an additional 5-15 minutes, until desired level of crispiness.\n5. Top with Parmesan cheese and serve.","lastmodified":"2022-11-06T05:20:38.302382278Z","tags":null},"/notes/awesome-git-repositories":{"title":"Awesome GitHub Projects","content":"\n- [GitHub Greatest hits](https://archiveprogram.github.com/assets/img/archive-repos.txt) list of archived repositories on GitHub:\n- [Nintendo Switch Emulator](https://github.com/yuzu-emu/yuzu)","lastmodified":"2022-11-06T05:20:38.302382278Z","tags":null},"/notes/aws/amazon-guardduty":{"title":"Amazon GuardDuty","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## Amazon GuardDuty\n- Continuous security monitoring service\n- Analyzes supported Data Sources\n- Uses AI/ML, plus threat intelligence feeds\n- Identifies unexpected and unauthorized activity\n- Attempts to learn your normal activity on its own, and spot activity outside of normal patterns\n- Notify or event-driven protection/remediation\n- Supports multiple accounts (MASTER and MEMBER)","lastmodified":"2022-11-06T05:20:38.302382278Z","tags":null},"/notes/aws/amazon-inspector":{"title":"","content":"### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## Amazon Inspector\n- Scans EC2 instance \u0026 the instance OS\n- Also containers\n- Detects vulnerabilities and deviations against best practice\n- Length: 15 minutes, 1 hour, 8/12 hours or 1 day\n- Provides a report of findings ordered by priority\n- Network Assessment (no agent required)\n- Network \u0026 Host Assessment (requires agent)\n\t- Host Assessment Rules Packages (Important for exam):\n\t\t- Common vulnerabilities and exposures (CVE)\n\t\t\t- List of CVE ids that are generated as part of a report for an instance or container\n\t\t- Center for Internet Security (CIS) Benchmarks\n\t\t\t- Best practices from CIS test against instance or container\n\t\t- Security best practices for Amazon Inspector\n- Network Reachability (no agent required)\n\t- Check readability end to end of EC2, ALB, DX, ELB, ENI, IGW, ACLs, RT's, SG's, Subnets, VPCs, VGWs \u0026 VPC Peering \n\t- Network Reachability Rules Packages (Important for exam):\n\t\t- `RecognizedPortWithListener`\n\t\t- `RecognizedPortNoListener`\n\t\t- `RecognizedPortNoAgent`\n\t\t- `UnrecognizedPortWithListener`\n- With Inspector, rules packages determine what is checked \n- Agent can provide additional OS visibility, such as \n\n\nAmazon Inspector is a vulnerability management service that continuously scans your AWS Workloads for vulnerabilities. Amazon Inspector automatically discovers and scans EC2 instances and container images residing in ECR for software vulnerabilities and unintended network exposure.\n\nWhen a software vulnerability or network issue is discovered, Amazon Inspector creates a finding. A finding describes the vulnerability, identifies the affected resource, rates the severity of the vulnerability, and provides remediation guidance. Details of a finding for your account can be analyzed in multiple ways using the Amazon Inspector console, or you can view and process your findings through other AWS services. For more information, see [Understanding findings in Amazon Inspector](https://docs.aws.amazon.com/inspector/latest/user/findings-understanding.html).","lastmodified":"2022-11-06T05:20:38.302382278Z","tags":null},"/notes/aws/api-gateway":{"title":"API Gateway","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **API Gateway**\n\n### WebSockets\n- **As a heads up as well to anyone doing IoT-related things. AWS API Gateway has a max connection time for WebSockets of 2 hours.** This may be a hard stop for some.\n- AWS SAM support for API Gateway local Websockets API testing is [literally non-existent](https://github.com/aws/aws-sam-cli/issues/896). This means you have to have a test server in order to test your application, but I guess that's fine.\n- Hard stop if you need [broadcasting functionality](https://stackoverflow.com/questions/55945205/aws-api-gateway-web-socket-api-broadcast-message-to-all-connected-clients) that scales\n- **Throttling Limits**: API Gateway provides throttling at multiple levels including global and by service call. Throttling limits can be set for standard rates and bursts. API Gateway tracks the number of requests per second. Any requests over the limit will receive a 429 HTTP response. The client SDKs generated by API Gateway retry calls automatically when met with this response.\n- API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds. Any requests over the limit will receive a 429 response.\n- You can add caching to API calls by provisioning an Amazon API Gateway cache and specifying its size in gigabytes. The cache is provisioned for a specific stage of your APIs. This improves performance and reduces the traffic sent to your back end. Cache settings allow you to control the way the cache key is built and the time-to-live (TTL) of the data stored for each method. API Gateway also exposes management APIs that help you invalidate the cache for each stage.","lastmodified":"2022-11-06T05:20:38.302382278Z","tags":null},"/notes/aws/athena":{"title":"Athena","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [S3](/notes/aws/s3/s3.md)\n- [S3 Select](/notes/aws/s3/s3-select.md)\n\n## **Athena** \n- Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL expressions. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries you run. Athena is easy to use. Simply point to your data in Amazon S3, define the schema, and start querying using standard SQL expressions. Most results are delivered within seconds. With Athena, there’s no need for complex ETL jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.","lastmodified":"2022-11-06T05:20:38.302382278Z","tags":null},"/notes/aws/aurora":{"title":"Aurora","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [Relational Database Service](/notes/aws/rds.md)\n\n## **Aurora** \n\n### Key Differences Compared to RDS\n- Aurora is hosted within RDS, but it has a different enough architecture that it deserves a dedicated page.\n- Uses a cluster, made up of a single primary instances, that has 0 or more replicas.\n\t- At first, this appears similar to normal RDS, however, the replicas within Aurora can be used for reads during normal operation.\n\t- In this way, it is *better* than the standby replica inside RDS. The replicas inside of Aurora can actually provide the benefits of both RDS Multi-AZ and RDS Read Replicas.\n\t\t- They can be used to improve availability, but they can also be used for read operations during normal operation of a cluster.\n\t- Replicas are deployed across multiple AZs.\n- Aurora doesn't use local storage for the compute instances. It uses a shared cluster storage volumes, which are SSD based, and available to all compute instance within a cluster.\n\t- This provides faster provisioning, improved availability and improved performance.\n\t- In a 3 AZ setup, the cluster volume has 6 replicas in total across multiple AZs\n\t\t- When data is written to the primary DB instance, aurora synchronously replicates that data across all of these 6 storage nodes spread across the AZs associated with your cluster.\n\t\t- This replication happens at the storage level, so no extra resources are consumed on the primary instance or the replicas instances.\n\t\t- The chances of disk related failure is greatly minimized with this architecture, and Aurora automatically detects failures in the disk volumes that make up the cluster shared storage, so that when a segments or part of a disk volume fails, Aurora immediately repairs that area of the disk, using the data inside the other storage nodes to bring it back into an operational state with no corruption.\n\t\t- The need to perform point-in-time restores or snapshot restores to recover from disk failure is greatly reduced.\n- Aurora allows up to 15 replicas, and any of them can be the fail over target for a fail over operation. This is much more than the single standby instance normal RDS allows. The failover operation will also be much faster, because it doesn't have to make any storage modifications.\n- Cluster volume storage is all SSD based, which provides high IOPS and low latency. No magnetic storage option\n- Storage is billed based on what is consumed, based on a high water mark\n- To reduce the high water mark, you have to migrate to a new cluster to lose that high water mark.\n\t- NOTE: This is no longer part of Aurora in newer versions\n- Aurora clusters, like RDS, use reader endpoints\n\t- The reader endpoints automatically load balance across read replicas\n\t\t- In addition, each read replica has a dedicated endpoint if needed\n\n### General Notes\n  - **Amazon Aurora Global Database** is specifically designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Aurora Global Database supports storage-based replication that has a latency of less than 1 second. If there is an unplanned outage, one of the secondary regions you assigned can be promoted to read and write capabilities in less than 1 minute. This feature is called **Cross-Region Disaster Recovery**.\n  - Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the hostname and port that you specify point to an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don't have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren't available.\n  - **Aurora Serverless** is an on-demand, auto-scaling configuration for Amazon Aurora. An Aurora Serverless DB cluster is a DB cluster that automatically starts up, shuts down, and scales up or down its compute capacity based on your application's needs. Aurora Serverless provides a relatively simple, cost-effective option for infrequent, intermittent, sporadic, or unpredictable workloads. It can provide this because it automatically starts up, scales compute capacity to match your application's usage and shuts down when it's not in use\n  - **Load Balancing Across Aurora DB Instances**\n    - Amazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the hostname and port that you specify point to an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don’t have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren't available.\n    - For certain Aurora tasks, different instances or groups of instances perform different roles. For example, the primary instance handles all data definition language (DDL) and data manipulation language (DML) statements. Up to 15 Aurora Replicas handle read-only query traffic.\n    - Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For example, to perform DDL statements you can connect to whichever instance is the primary instance. To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora Replicas. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance.\n  - **Failover** is automatically handled by Amazon Aurora so that your applications can resume database operations as quickly as possible without manual administrative intervention.\n    - If you have an Amazon Aurora Replica in the same or a different Availability Zone, when failing over, Amazon Aurora flips the canonical name record (CNAME) for your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary. Start-to-finish, failover typically completes within 30 seconds.\n    - If you are running Aurora Serverless and the DB instance or AZ become unavailable, Aurora will automatically recreate the DB instance in a different AZ.\n    - If you do not have an Amazon Aurora Replica (i.e. single instance) and are not running Aurora Serverless, Aurora will attempt to create a new DB Instance in the same Availability Zone as the original instance. This replacement of the original instance is done on a best-effort basis and may not succeed, for example, if there is an issue that is broadly affecting the Availability Zone.\n  - Handles highly transactional (OLTP) workloads\n\n### Cost Considerations\n- Aurora has no free-tier option\n- Aurora doesn't support micro instances\n- Beyond RDS single-AZ (micro) Aurora offers better value\n- Compute - hourly charge, billed per second, with a 10 minute minimum\n- Storage - GB-Month consumed, IO cost per request\n- 100% DB size in backups are included\n\n### Aurora Restore, Clone \u0026 Backtrack\n- Backups in Aurora work in the same way as RDS\n- Restores create a new cluster\n- Backtrack can be used which allow in-place rewinds to a previous point in time\n\t- Needs to be enabled on a per-cluster basis\n\t- Exclusive to Aurora at the time that this note was taken\n- Fast clones make a new database much faster than copying all the data - copy-on-write\n  ","lastmodified":"2022-11-06T05:20:38.302382278Z","tags":null},"/notes/aws/aws-app-runner":{"title":"AWS App Runner","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n### Useful Links\n- [AWS App Runner Pricing](https://aws.amazon.com/apprunner/pricing/)\n\n## AWS App Runner","lastmodified":"2022-11-06T05:20:38.302382278Z","tags":null},"/notes/aws/aws-appsync":{"title":"","content":"### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **AppSync**\n- AWS AppSync is a fully managed service that makes it easy to develop GraphQL APIs by handling the heavy lifting of securely connecting to data sources like AWS DynamoDB, Lambda, and more. Adding caches to improve performance, subscriptions to support real-time updates, and client-side data stores that keep offline client in sync are just as easy. Once deployed, AWS AppSync automatically scales your GraphQL API execution engine up and down to meet API request volumes.\n\n#aws","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-artifact":{"title":"AWS Artifact","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## AWS Artifact\n\n- AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to security and compliance reports from AWS and Independent Software Vendors (ISVs) who sell their products on AWS Marketplace. Reports available in AWS Artifact include Service Organization Control (SOC) reports, International Organization for Standardization (ISO) reports, and certifications from accreditation bodies across geographies and compliance verticals that validate the implementation and operating effectiveness of security controls. Additionally, it provides on-demand access to review, accept and manage select online agreements with AWS. Agreements available in AWS Artifact include the Business Associate Addendum (BAA) and the Nondisclosure Agreement (NDA).","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-backup":{"title":"AWS Backup","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [Relational Database Service](/notes/aws/rds.md)\n- [EFS](efs.md)\n\n## **Backup**\n- **AWS Backup** enables you to centralize and automate data protection across AWS services and hybrid workloads. AWS Backup offers a cost-effective, fully managed, policy-based service that further simplifies data protection at scale. AWS Backup also helps you support your regulatory compliance or business policies for data protection. Together with AWS Organizations, AWS Backup enables you to centrally deploy data protection policies to configure, manage, and govern your backup activity across your organization’s AWS accounts and resources, including Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon Elastic Block Store (Amazon EBS) volumes, Amazon Relational Database Service (Amazon RDS) databases (including Amazon Aurora clusters), Amazon DynamoDB tables, Amazon Neptune databases, Amazon DocumentDB (with MongoDB compatibility) databases, Amazon Elastic File System (Amazon EFS) file systems, Amazon FSx for Lustre file systems, Amazon FSx for Windows File Server file systems, and AWS Storage Gateway volumes, and VMware workloads on premises and in VMware CloudTM on AWS. AWS Backup also offers a preview for backup and restore of Amazon Simple Storage Service (Amazon S3) buckets.\n- AWS Backup is a centralized backup service that makes it easy and cost-effective for you to backup your application data across AWS services in the AWS Cloud, helping you meet your business and regulatory backup compliance requirements. AWS Backup makes protecting your AWS Storage volumes, databases and file systems simple by providing a central place where you can configure and audit the AWS resources you want to backup, automate backup scheduling, set retention policies, and monitor all recent backup and restore activity.\n- AWS Backup supports the following AWS Services\n\t- Amazon Aurora\n\t- Amazon DynamoDB\n\t- Amazon FSx\n\t- Amazon EFS\n\t- AWS Storage Gateway\n\t- Amazon RDS\n\t- Amazon EBS\n\t- Amazon EC2","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-batch":{"title":"AWS Batch","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Batch**\n- AWS Batch is aimed at the specific use case of executing batch jobs that are pulled from a queue. You would generally use Batch in your backend processes to take some data and then process it using containerized processes. The batch jobs in AWS Batch should run to completion then exit. AWS Batch is hyper focused on providing a good experience for backend batch workloads.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-certificate-manager":{"title":"AWS Certificate Manager","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Certificate Manager**\n  - AWS Certificate Manager (ACM) lets you import 3rd party certificates from the ACM console, as well as programmatically. If ACM is not available in your region, use AWS CLI to upload your 3rd party certificate to the IAM certificate store.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-config":{"title":"AWS Config","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## AWS Config\n- Primary function is to record configuration changes over time (configuration items) on resources, and grouping this information into configuration histories\n- Auditing of changes, compliance with standards\n- Does not prevent changes from happening, no protection\n- Regional service, supports cross-region and account aggregation (but not by default)\n- Changes can generate SNS notification and near-realtime events via EventBridge and Lambda\n\n- Config Rules can be created, which your resources are evaluated against. You can either use AWS Managed or custom (using Lambda).\n\n- **AWS Config** is a service that enables you to assess, audit and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With AWS Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting.\n- Primary function is to record changes over time on resources within an AWS account (great for auditing and compliance)\n- It does *not* prevent changes from happening, but can be used for automatic remediation\n- It's a regional service that does support cross-region and account aggregation\n- Changes can generate SNS notifications and near real-time events via EventBridge and Lambda\n- When enabled, AWS Config will record changes in a S3 bucket called a Config Bucket, but you can do a lot more with the product.\n- Instead of the default setup outlined above, you can set up config rules which use AWS Lambda to evaluate whether or not a change is compliant or non-compliant.\n- AWS Config can also be integrated with EventBridge, which can invoke Lambda function for automatic remediation.\n- AWS Config can integrate with Systems Manager as well\n\n## #sysops scenarios\n**Question**: A company has a tagging strategy for controlling access to Amazon EC2 across their AWS Organization units. The system administrator noticed that some tags do not follow the company's naming convention which causes permission issues.\n\nWhich solution can help the administrator identify the affected resources with non-compliant tags?\n\n**Answer**: Set up the `require-tags` managed rule in AWS Config. You can assign metadata to your AWS resources in the form of tags. Each tag is a label consisting of a user-defined key and value. Tags can help you manage, identify, organize, search for, and filter resources. You can create tags to categorize resources by purpose, owner, environment, or other criteria.\n\nYou can use tags to control access by restricting IAM permissions based on specific tags or tag values. For example, IAM user or role permissions can include conditions to limit EC2 API calls to specific environments (such as development, test, or production) based on their tags.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-control-tower":{"title":"AWS Control Tower","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## AWS Control Tower\n- Allows quick and easy setup of multi-account environment\n- Orchestrates other AWS services to provide this functionality\n- Leverages [Organizations](/notes/aws/organizations.md), [IAM Identity Center (formerly AWS SSO)](/notes/aws/iam-identity-center.md), [CloudFormation](cloudformation.md), [AWS Config](/notes/aws/aws-config.md) and [AWS Service Catalog](/notes/aws/aws-service-catalog.md)\n- Landing Zone - multi-account environment\n\t- SSO/ID Federation, Centralized Logging and Auditing\n- Guard Rails - Detect/Mandate rules/standards across all accounts\n\t- Config is used under the hood to implement these guardrails\n- Account Factory - Automates and Standardizes new account creation\n\t- Basic template is applied (CloudFormation is used under the hood for this)\n- Dashboard - single page oversight of the entire environment\n\n### Landing Zone\n- Allows for implementation of a Well Architected multi-account environment\n- Home Region - The region you initially deploy into\n\t- You can explicitly allow or deny the usage of other regions\n\t- Built with Organizations, Config and CloudFormation\n- Security OU - Log Archive \u0026 Audit Accounts (CloudTrail \u0026 Config Logs)\n- Sandbox OU - Test/less rigid security\n- You can create other OU's and Accounts\n- IAM Identity Center (AWS SSO) - SSO, multiple accounts, ID Federation (use existing identity stores)\n- Monitoring and Notifications - CloudWatch and SNS\n- End User account provisioning using Service Catalog\n\n### Guard Rails\n- Guardrails are rules - multi-account governance\n- Mandatory, Strongly Recommended or Elective\n- Preventative - Stop you from doing things (AWS ORG SCP)\n\t- enforced or not enabled\n\t- i.e allow or deny regions or disallow bucket policy changes\n- Detective - compliance checks (AWS Config Rules)\n\t- Clear, in violation, or not enabled\n\t- Detect CloudTrail enabled or EC2 Public IPv4\n\n### Account Factory\n- Automated Account Provisioning\n- Cloud Admins or end users (with appropriate permissions)\n- Guardrails - automatically added\n- Account admin given to a named user (IAM Identity Center)\n- Account \u0026 network standard configuartion\n- Accounts can be closed or repurposed\n- Can be fully integrated with a business SDLC using Account Factory / Control Tower APIs\n\n#aws #aws-sysops ","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-cost-explorer":{"title":"AWS Cost Explorer","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## AWS Cost Explorer\n- Capable of analyzing usage within the account, and giving recommendations about reserved instance purchases (if you have enough usage on the account)\n- Cost Anomaly Detection: Help reduces cost with machine learning by tracking costs and usage\n- Rightsizing Recommendations: Reviews historical EC2 usage to identify opportunities for greater cost and usage efficiency\n\n### Cost Allocation Tags\n- Cost allocation tags - have to be enabled individually\n- (per account for standard accounts or ORG Master for ORGS)\n- AWS Generated - e.g. `aws:createdBy` (details which identity created a resource, if already enabled) or `aws:cloudformation:stack-name`\n- Added to resources AFTER they're enabled by AWS, not retroactive\n- User-defined tags can also be enabled `user:something`\n- Both are visible in cost reports and can be used as a filter\n- Can take up to 24 hours to be visible and active","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-directory-service":{"title":"AWS Directory Service","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **AWS Directory Service**\n- AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft Active Directory (AD), enables your directory-aware workloads and AWS resources to use managed Active Directory (AD) in AWS. AWS Managed Microsoft AD is built on actual Microsoft AD and does not require you to synchronize or replicate data from your existing Active Directory to the cloud. You can use the standard AD administration tools and take advantage of the built-in AD features, such as Group Policy and single sign-on. With AWS Managed Microsoft AD, you can easily join Amazon EC2 and Amazon RDS for SQL Server instances to your domain, and use AWS End User Computing (EUC) services, such as Amazon WorkSpaces, with AD users and groups.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-firewall-manager":{"title":"AWS Firewall Manager","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [[aws-web-application-firewall]]\n\n#aws-sysops #aws-security","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-glue":{"title":"AWS Glue","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## AWS Glue\n- AWS Glue is a serverless, fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. you can create and run an ETL job with a few clicks in the AWS Management Console. You simply point AWS Glue to your data stored on AWS, and AWS Glue discovers your data and stores the associated metadata (e.g. table definition and schema) in the AWS Glue Data Catalog. Once cataloged, your data is immediately searchable, queryable, and available for ETL. AWS Glue generates the code to execute your data transformations and data loading processes.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-kms":{"title":"AWS KMS","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [CloudHSM](/notes/aws/cloudhsm.md)\n\n## **Key Management Service (KMS)**\n- Regional and Public Service\n\t- Every region is isolated when using KMS\n- Create, Store and Manage Cryptographic Keys\n\t- Allows creation of both symmetric and asymmetric keys\n\t- Allows you to perform cryptographic operations (encryption, decryption, etc.)\n\t\t- You don't perform the operations directly, you just pass instructions to KMS in the form of API requests, etc.\n\t- Cryptographic keys never actually leave KMS\n- KMS is very granular with permissions. You need specific permissions for each operation you attempt to do in the service. You also need specific permissions on specific keys in order to use those keys.\n- KMS provides FIPS 140-2 (Level 2) compliance\n\t- This is a US security standard\t\n- AWS Key Management Service allows you to protect against unauthorized access of your objects in Amazon S3. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service and your Region.\n\n### KMS Keys\n- KMS Keys are logical - ID, date, policy, desc \u0026 state\n- KMS Keys are backed by physical key material\n- Generated or Imported\n- KMS Keys can be used for up to 4KB of data\n- KMS keys are isolated to a region and never leave KMS\n\t- KMS does support multi-region keys\n\t- KMS uses AWS owned and Customer owned keys\n\t\t- AWS Owned keys are largely used by services in the background and don't concern us\n\t\t- Customer Owned Keys come in the form of AWS Managed or Customer Managed Keys\n\t\t\t- Customer Managed Keys are more configurable and must be created explicitly\n\t\t\t- KMS Keys support rotation\n\t\t\t- Rotation is optional with Customer Managed Keys\n\t\t\t- When rotation occurs, backing key and previous backing keys are retained\n\n### Data Encryption Keys\n- KMS can generate DEKs, or Data Encryption Keys, which are generated by a KMS key, and can be used on \u003e 4KB of data.\n- The `GenerateDataKey` KMS operation can be used to generate a DEK.\n- DEKs are linked to the KMS key that it was created with. KMS does NOT store the Data Encryption Key in any way. It provides it to you, or the service using it, and then discards it.\n- When DEK is generated, you get a plaintext version, that can be used immediately and discarded, and a Ciphertext version, meant to be saved, that can be decrypted in the future by KMS, so that the plaintext version can be retrieved once again by an API call to KMS.\n- KMS doesn't track usage of DEKs, or do the encryption/decryption itself\n\n### Key Policies and Security\n- KMS is different than other AWS services with regards to permissions.\n- Key Policies (Resource Policy)\n\t- Every Key has one, and for Customer Managed Keys, it can be changed\n\t- Unlike other AWS services, KMS has to specifically be told to trust that keys trust the AWS account that they are contained in. You always need this policy to be in place, in order to use KMS.\n\t- Key Policies are used in tandem with IAM identity policies","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-service-catalog":{"title":"AWS Service Catalog","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## AWS Service Catalog\nAWS Service Catalog enables a (usually internal user) to directly users or customers to deploy infrastructure with tight control in a self-service way\n\n### What is a Service Catalog?\n- A Document or Database created by an IT team\n- Organized collection of products\n- Offered by the IT Team\n- Key Product Information: Product Owner, Cost, Requirements, Support Information, Dependencies\n- Manage costs and scale service delivery\n\n### AWS Service Catalog\n- Self-Service Portal for 'end users'\n- Launch predefined (by admin) products\n- End user permissions can be controlled\n- Admins can define those products using CloudFormation Templates, as well as configuration of the Service Catalog product\n- Admins also define the permissions required to launch the infrastructure that the products use, so this is AWS permissions to, say, launch an EC2 instance or provision a load balancer, or any other permissions used by your products.\n- You can also define any end user permissions\n\t- Who can launch the products\n\t- Who has visibility of what, within the product\n- Build products into portfolios\n\n### Tag Management\nTo allow administrators to easily manage tags on provisioned products, AWS Service Catalog provides a TagOption library. A TagOption is a key-value pair managed in AWS Service Catalog. It is not an AWS tag but serves as a template for creating an AWS tag based on the TagOption.\n\nThe TagOption library makes it easier to enforce the following:\n- A consistent taxonomy\n- Proper tagging of AWS Service Catalog resources\n- Defined, user-selectable options for allowed tags\n\nAdministrators can associate TagOptions with portfolios and products. During a product launch (provisioning), AWS Service Catalog aggregates the associated portfolio and product TagOptions, and applies them to the provisioned product","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-shield":{"title":"AWS Shield","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [AWS Web Application Firewall](/notes/aws/aws-web-application-firewall.md)\n\n## **AWS Shield**\n\n### Types of Attack\n- Network Volumetric Attacks (L3) - Saturate Capacity\n\t- These types of attacks overwhelm a system by directing as much raw network data at a target as possible\n- Network Protocol Attacks (L4) - TCP SYN Flood\n\t- Flood large number of connections, leave connections open, preventing new ones\n\t- Analogy is people calling a call center, and staying on the phone lines, preventing real customers from talking to call center employees\n\t- L4 can also have a volumetric component\n- Application Layer Attacks (L7) - e.g. web request floods\n\n### Shield Standard\n- Free for all AWS customers, and enabled by default\n- Protection at the perimeter (region/VPC or at the AWS edge)\n- Common Network (L3) or Transport (L4) layer attacks\n- Best protection using R53, CloudFront, AWS Global Accelerator\n- No proactive or configurable protection\n\n### Shield Advanced\n- Commercial product, costs $3,000 per month, per organization (1 year lock-in + data (OUT) per month)\n- Protects CloudFront, R53, Global Accelerator, Anything associated with EIPs (i.e EC2), ALBs, CLBs, NLBs\n- Not automatic - must be explicitly enabled in Shield Advanced or AWS Firewall Manager Shield Advanced policy\n- Cost protection (i.e EC2 scaling) for unmitigated attacks (reimbursement for something Shield Advanced can cover, and should have covered)\n- Proactive Engagement \u0026 AWS Shield Response Team (SRT) access\n- **AWS Shield Advanced** also gives you 24x7 access to the AWS DDoS Response Team (DRT).\n- Integrates with AWS WAF - includes basic AWS WAF fees for Web ACLs, rules, and web requests.\n- Application Layer (L7) DDOS protection (uses WAF)\n- Real time visibility of DDOS events and attacks\n- Health-based detection - application specific health checks, used by proactive engagement team\n- Protection groups - creates groupings of resources which Shield Advanced protects, manage protection at group level, decreases admin overhead\n\n#aws #aws-security #aws-sysops #ddos","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-snowball":{"title":"AWS Snowball","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Snowball**\n- AWS Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of the AWS cloud. Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns. Transferring data with Snowball is simple, fast, secure, and can be as little as one-fifth the cost of high-speed Internet. The Snowball devices literally look like massive desktop computers and are physically delivered to you for the data transfer process.\n- Snowball is a strong choice for data transfer if you need to more securely and quickly transfer terabytes to many petabytes of data to AWS. Snowball can also be the right choice if you don't want to make expensive upgrades to your network infra, if you frequently experience large backlogs of data, if you're located in a physically isolated environment, or if you're in an area where high-speed Internet connections are not available or cost-prohibitive.\n- As a rule of thumb, if it takes more than one week to upload your data to AWS using the spare capacity of your existing Internet connection, then you should consider using Snowball. For example, if you have a 100 Mb connection that you can solely dedicate to transferring your data and need to transfer 100 TB of data, it takes more than 100 days to complete data transfer over that connection. You can make the same transfer by using multiple Snowballs in about a week.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-step-functions":{"title":"AWS Step Functions","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [Lambda](/notes/aws/lambda.md)\n- [AWS SWF](/notes/aws/aws-swf.md)\n\n### Helpful Links\n- [AWS Step Functions Pricing](https://aws.amazon.com/step-functions/pricing/)\n\n## **Step Functions**\n- AWS Step Functions provides serverless orchestration for modern applications. Orchestration centrally manages a workflow by breaking it into multiple steps, adding flow logic, and tracking the inputs and outputs between the steps. As your applications execute, Step Functions maintains application state, tracking exactly which workflow step your application is in, and stores an event log of data that is passed between application components. That means that if networks fail or components hang, your application can pick up right where it left off.\n- Application development is faster and more intuitive with Step Functions, because you can define and manage the workflow of your application independently from its business logic. Making changes to one does not affect the other. You can easily update and modify workflows in one place, without having to struggle with managing, monitoring and maintaining multiple point-to-point integrations. Step Functions frees your functions and containers from excess code, so your applications are faster to write, more resilient, and easier to maintain.\n\n### What Limitations of Lambda does Step Functions address?\n- Lambda is FaaS (Functions as a Service)\n- Lambda has a 15 minute max execution time\n\t- Although Lambdas can be chained, this gets messy at scale\n- Lambda Runtime Environments are stateless\n\n### State Machines - Coordinates the Work Occurring\n- A State Machine is a fancy term for a serverless workflows\n- You have a START -\u003e multiple STATES in between -\u003e and an END\n- States are the THINGS inside these workflows\n- Maximum Duration for a state machine is 1 year (Standard Workflow) or 5 minutes (Express Workflow)\n- Standard Workflow (default) and Express Workflow (High volume, IoT, streaming data)\n- Can be started via API Gateway, IOT Rules, EventBridge, Lambda, or Manually\n- You can create, and export state machines to your liking by using a templating language called Amazon States Language (ASL) - JSON Template\n- IAM Roles are used for permissions\n\n### State Types\n- SUCCEED\n- FAIL\n- WAIT - Waits a certain period of time, or until a specific date and time\n- CHOICE - Allows the State Machine to take a different path, depending on an input. A fork in the road.\n- PARALLEL - Allows for parallel branches within the State Machine\n- MAP - Accepts a list of things, like orders, for example. Performs a set of actions for each order in the list.\n- TASK - A single unit of work performed by a state machine. This can be integrated with Lambda, Batch, DynamoDB, ECS, SNS, SQS, Glue, SageMaker, EMR, Step Functions","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-swf":{"title":"AWS SWF","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [AWS Step Functions](/notes/aws/aws-step-functions.md)\n\n## AWS Simple Workflow Service (SWF)\n- Consider using AWS Step Functions for all your new applications, since it provides a more productive and agile approach to coordinating application components **using visual workflows**. If you require **external signals** (deciders) to intervene in your processes, or you would like to launch child processes that return a result to a parent, then you should consider Amazon SWF.\n- SWF and SQS are the services that you can use for creating a decoupled architecture in AWS. Decoupled architecture is a type of computing architecture that enables computing components or layers to execute independently while still interfacing with each other.\n- SWF provides useful guarantees around task assignments. It ensures that a task is never duplicated and is assigned only once. Thus, even though you may have multiple workers for a particular activity type (or a number of instances of a decider), Amazon SWF will give a specific task to only one worker (or one decider worker). Additionally, SWF keeps at most one decision task outstanding at a time for a workflow execution. Thus, you can run multiple decider instances without worrying about two instances operating on the same execution simultaneously. These facilities enable you to coordinate your workflow without worrying about duplicate, lost, or conflicting tasks.\n- The Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components. In Amazon SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing intertask dependencies, scheduling, and concurrency in accordance with the logical flow of the application. Amazon SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state.\n- When using Amazon SWF, you implement workers to perform tasks. These workers can run either on cloud infrastructure, such as Amazon Elastic Compute Cloud (Amazon EC2), or on your own premises. You can create tasks that are long-running, or that may fail, time out, or require restarts—or that may complete with varying throughput and latency. Amazon SWF stores tasks and assigns them to workers when they are ready, tracks their progress, and maintains their state, including details on their completion. To coordinate tasks, you write a program that gets the latest state of each task from Amazon SWF and uses it to initiate subsequent tasks. Amazon SWF maintains an application's execution state durably so that the application is resilient to failures in individual components. With Amazon SWF, you can implement, deploy, scale, and modify these application components independently.\n- Amazon SWF offers capabilities to support a variety of application requirements. It is suitable for a range of use cases that require coordination of tasks, including media processing, web application back-ends, business process workflows, and analytics pipelines.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-web-application-firewall":{"title":"AWS Web Application Firewall","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [AWS Firewall Manager](/notes/aws/aws-firewall-manager.md)\n\n## **Web Application Firewall (WAF)**\n\n- AWS Implementation of Layer 7 firewall (HTTP/HTTPS)\n- The WAF Web ACL can be associated with Global resources like CloudFront, and regional resources, like ALB, API Gateway, etc. to integrate with WAF.\n\n### Web Access Control Lists\n- Controls if traffic is blocked or allowed\n- Web ACL has a default action of ALLOW or BLOCK\n- Resource Type - CloudFront or Regional Service\n\t- ALB, API GW, AppSync requires picking a region\n\t- CloudFront is global\n- Add Rule Groups or Rules, which are processed in order\n- Web ACL Capacity Units (WCU) - Default 1500\n\t- Can be increased via support ticket\n- Web ACL's are associated with resources (this can take time)\n\t- Adjusting a Web ACL takes less time than associating one\n\t- A resource can have one Web ACL, but one Web ACL can be associated with many resources\n\t- You cannot associate a CloudFront Web ACL with a regional resource\n- Can be updated manually, or in an automated fashion using EventBridge to achieve Lambda Event Driven processing of logs or event subscriptions\n\t- CloudWatch Logs\n\t- S3\n\t- Firehose\n\n### Rule Groups\n- Contain rules, used by Web ACL as an admin container for rules\n- They don't have default actions, they're added to Web ACL's and the Web ACL itself has a default action.\n- Managed (AWS or Marketplace), Yours, Service Owned (i.e. Shield \u0026 Firewall Manager)\n\t- Managed Rule Groups are free for AWS WAF customers\n- Rule Groups obtained via Marketplace usually have subscriptions\n- Rule Groups can be referenced by multiple Web ACL (separate entity)\n- Have a WCU capacity (defined upfront, max 1500)\n\n### Rules\n- Structure\n\t- Type\n\t\t- Regular - Designed to match if something occurs\n\t\t- Rate-based - Designed to match if something occurs at a regular rate\n\t\t\t- Do something if someone tries to connect via SSH 5000 times within an interval\n\t- Statement: One or more things that match traffic or not\n\t\t- Defines (WHAT to match) or (COUNT ALL) or (WHAT \u0026 COUNT)\n\t\t\t- origin country, IP, label, header, cookies, query parameter, URI path, query string, body (first 8,192 bytes ONLY), HTTP method\n\t\t\t- Single, AND, OR, NOT\n\t\t\t- Example: Incoming TCP port 80\n\t- Action: What does WAF do if a rule is matched\n\t\t- Allow* (not valid for rate-based rules, custom header only)\n\t\t- Block (custom header and custom response)\n\t\t- Count (custom header only)\n\t\t- Captcha (custom header only)\n\t\t- Custom Response (optional, prefixed with `x-amzn-waf-` - used so that application can react to traffic which has been affected by your rule)\n\t\t- Label - Can be referenced later in the same Web ACL (internal to WAF only, don't persist outside)\n\t\t- ALLOW \u0026 BLOCK stop processing, Count/Captcha actions continue\n\t\t\n### Pricing\n- Web ACL - Monthly ($5 per month per Web ACL)\n- Rule on Web ACL - Monthly ($1 per month per rule)\n- Requests per Web ACL - Monthly ($0.60 per month per 1 million requests)\n- Intelligent Threat Mitigation\n- Bot Control - ($10 per month) \u0026 ($1 per 1 million requests per month)\n- Captcha - ($0.40 / 1,000 challenge attempts)\n- Fraud Control/Account Takeover ($10 per month \u0026 $1 per 1,000 login attempts)\n- Marketplace Rule Groups - Extra costs\n\n### Misc. Notes\n- AWS WAF is a web application firewall that lets you monitor HTTP and HTTPS requests that are forwarded to an API Gateway API, CloudFront, or an Application Load Balancer. AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, API Gateway, CloudFront or an ALB response to requests either with their requested content or with an HTTP 403 status code (forbidden). You also can configure CloudFront to return a custom error page when a request is blocked.\n- At the simplest level, AWS WAF lets you choose one of the following behaviors:\n\t- Allow all requests except the ones that you specify - This is useful when you want CloudFront or an Application Load Balancer to serve content for a public website, but you also want to block requests from attackers.\n\t- Block all requests except the ones that you specify - This is useful when you want to serve content for a restricted website whose users are readily identifiable by properties in web requests, such as the IP address that they use to browse the website.\n\t- Count the requests that match the properties that you specify - When you want to allow or block requests based on new properties in web requests, you first can configure AWS WAF to count the requests that match those properties without allowing or blocking those requests. This lets you confirm that you didn't accidentally configure AWS WAF to block all the traffic to your website. When you're confident that you specified the correct properties, you can change the behavior to allow or block requests.\n- AWS WAF is tightly integrated with Amazon CloudFront, the Application Load Balancer (ALB), API Gateway, AWS AppSync - services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn't come at the expense of performance. Blocked requests are stopped before they reach your web servers. When you use AWS WAF on regional services, such as Application Load Balancer, Amazon API Gateway, and AWS AppSync, your rules run in the region and can be used to protect Internet-facing resources as well as internal resources.\n- A rate-based rule tracks the rate of requests for each originating IP address and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span. You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-wiki":{"title":"AWS Wiki","content":"\n## AWS Wiki\n\n### Helpful Links\n- [AWS Pricing Calculator](https://calculator.aws/#/)\n- [AWS Documentation](https://docs.aws.amazon.com/index.html?nc2=h_ql_doc_do_v)\n- [AWS Extend Switch Roles](https://github.com/tilfinltd/aws-extend-switch-roles)\n\n---\n\n## AWS Services\n- [AWS Artifact](/notes/aws/aws-artifact.md)\n- [API Gateway](/notes/aws/api-gateway.md)\n- [AWS AppSync](/notes/aws/aws-appsync.md)\n- [Athena](/notes/aws/athena.md)\n- [Aurora](/notes/aws/aurora.md)\n- [AWS Backup](/notes/aws/aws-backup.md)\n- [AWS Batch](/notes/aws/aws-batch.md)\n- [AWS Cost Explorer](/notes/aws/aws-cost-explorer.md)\n- [AWS Certificate Manager](/notes/aws/aws-certificate-manager.md)\n- [Cloud Map](/notes/aws/cloud-map.md)\n- [CloudFront](/notes/aws/cloudfront.md)\n- [CloudHSM](/notes/aws/cloudhsm.md)\n- CloudEndure\n- [Data Lifecycle Manager](/notes/aws/data-lifecycle-manager.md)\n- [DataSync](/notes/aws/datasync.md)\n- [AWS Directory Service](/notes/aws/aws-directory-service.md)\n- [EBS](/notes/aws/ebs.md)\n- Elasticsearch\n- [EFS](/notes/aws/efs.md)\n- [EKS](/notes/aws/eks.md)\n- [Elastic Beanstalk](/notes/aws/elastic-beanstalk.md)\n- [Elastic Map Reduce](/notes/aws/elastic-map-reduce.md)\n- [FSx for Windows File Server](/notes/aws/fsx-for-windows-file-server.md)\n- [FSx for Lustre](/notes/aws/fsx-for-lustre.md)\n- [AWS Glue](/notes/aws/aws-glue.md)\n- [IoT core](/notes/aws/iot-core.md)\n- [Kinesis](/notes/aws/kinesis.md)\n- [MQ](/notes/aws/mq.md)\n- Amazon Neptune\n- [Redshift](/notes/aws/redshift.md)\n- [Resource Access Manager](/notes/aws/resource-access-manager.md)\n- [Route 53](/notes/aws/route-53.md)\n- [AWS Service Catalog](/notes/aws/aws-service-catalog.md)\n- [AWS Shield](/notes/aws/aws-shield.md)\n- [AWS SWF](/notes/aws/aws-swf.md)\n- [Simple Queue Service](/notes/aws/simple-queue-service.md)\n- [AWS Snowball](/notes/aws/aws-snowball.md)\n- [Simple Notification Service](/notes/aws/simple-notification-service.md)\n- [Storage Gateway](/notes/aws/storage-gateway.md)\n- [WorkDocs](/notes/aws/workdocs.md)\n- [AWS WorkSpaces](/notes/aws/aws-workspaces.md)\n\n### Compute\n- [Lambda](/notes/aws/lambda.md)\n- [EC2](/notes/aws/ec2.md)\n- [ECS](/notes/aws/ecs.md)\n- [Fargate](/notes/aws/fargate.md)\n\n### Serverless Workflows\n- [AWS Step Functions](/notes/aws/aws-step-functions.md)\n\n### S3\n- [S3](/notes/aws/s3/s3.md)\n- [S3 Select](/notes/aws/s3/s3-select.md)\n- [Macie](/notes/aws/macie.md) - ML-based Data Classification in S3\n\n### Resource Provisioning and Deployment Automation\n- [CloudFormation](/notes/aws/cloudformation.md)\n- [CodeBuild](/notes/aws/codebuild.md)\n- CodePipeline\n- CodeDeploy\n- [CDK](https://docs.aws.amazon.com/cdk/index.html)\n- [Systems Manager](/notes/aws/systems-manager.md)\n- [OpsWorks](/notes/aws/opsworks.md)\n\n### Cache\n- [Elasticache](/notes/aws/elasticache.md)\n\n### Database\n- [DynamoDB](/notes/aws/dynamodb.md)\n- [DynamoDB Accelerator](/notes/aws/dynamodb-accelerator.md)\n- [RDS](/notes/aws/rds.md)\n\n### Monitoring\n- [CloudWatch](/notes/aws/cloudwatch.md)\n- [AWS Config](/notes/aws/aws-config.md)\n- [CloudTrail](/notes/aws/cloudtrail.md)\n- [AWS X-Ray](/notes/aws/aws-x-ray.md)\n\n### User Management \u0026 Security\n- [IAM](/notes/aws/iam.md)\n- [Organizations](/notes/aws/organizations.md)\n- [IAM Identity Center](/notes/aws/iam-identity-center.md)\n- Cognito\n- [AWS Control Tower](/notes/aws/aws-control-tower.md)\n\n### Security\n- [Amazon GuardDuty](/notes/aws/amazon-guardduty.md)\n- [AWS Firewall Manager](/notes/aws/aws-firewall-manager.md)\n- [AWS Web Application Firewall](/notes/aws/aws-web-application-firewall.md)\n- [AWS KMS](/notes/aws/aws-kms.md)\n\n### Networking\n- [Direct Connect](/notes/aws/direct-connect.md)\n- [VPC](/notes/aws/vpc.md)\n- [NACLs and Security Groups](/notes/aws/nacls-and-security-groups.md)\n- [VPC Router](/notes/aws/vpc-router.md)\n- [VPC Peering](/notes/aws/vpc-peering.md)\n- [Internet Gateway](/notes/aws/internet-gateway.md)\n- [VPC Gateway Endpoints](/notes/aws/vpc-gateway-endpoints.md)\n- [VPC Interface Endpoints](/notes/aws/vpc-interface-endpoints.md)\n- [Egress Only Internet Gateway](/notes/aws/egress-only-internet-gateway.md)\n- [NAT Gateway](/notes/aws/nat-gateway.md)\n- [VPN CloudHub](/notes/aws/vpn-cloudhub.md)\n- [Transit Gateway](/notes/aws/transit-gateway.md)\n- [PrivateLink](/notes/aws/privatelink.md)\n- [Site-to-Site VPN](/notes/aws/site-to-site-vpn.md)\n- [Global Accelerator](/notes/aws/global-accelerator.md)\n\n# Miscellaneous\n\n## Public vs Private vs Multi vs Hybrid Cloud\n- Public Cloud = using 1 public cloud\n- Private Cloud = using on-premises *real* cloud\n- Multi-Cloud = using more than 1 public cloud\n- Hybrid Cloud = Public and Private Clouds\n\t- Hybrid Cloud is NOT public cloud + legacy on-premises\n\n## **Security Best Practices**\n- Q: An application is hosted in an Auto Scaling group of EC2 instances and a Microsoft SQL Server on Amazon RDS. This is a requirement that all in-flight data between your web servers and RDS should be secured.\n\t- Force all connections between your DB instance to use SSL by setting the rds.force_ssl parameter to true. Once done, reboot your DB instance.\n\t- Download the Amazon Root CA certificate. Import the certificate to your servers and configure your application to use SSL to encrypt the connection to RDS.\n## **Establishing a site-to-site VPN connection**\n  - By default, instances that you launch into a virtual private cloud (VPC) can't communicate with your own network. You can enable access to your network from your VPC by attaching a virtual private gateway to the VPC, creating a custom route table, updating your security group rules, and creating an AWS managed VPN connection.\n  - Although the term VPN connection is a general term, in the Amazon VPC documentation, a VPN connection refers to the connection between your VPC and your own network. AWS supports Internet Protocol security (IP sec) VPN connections.\n  - A **customer gateway** is a physical device or software application on your side of the VPN connection.\n  - To create a VPN connection, you must create a customer gateway resource in AWS, which provides information to AWS about your customer gateway device. Next, you have to set up an Internet-routable IP address (static) of the customer gateway's external interface.\n  - An AWS VPC needs an attached virtual private gateway, and your remote network includes a customer gateway, which you must configure to enable the VPN connection. You set up the routing so that any traffic from the VPC bound for your network is routed to the virtual private gateway.\n### **Auto Scaling**\n  - **What is the default termination policy for Auto Scaling groups?**\n    - The default termination policy is designed to ensure that your network architectures spans AZ's evenly. The default behavior is as follows:\n      - If there are instances in multiple AZ's, choose the AZ with the most instances, and at least one instance that is not protected from scale in. If there is more than one AZ with this number of instances, choose the AZ with the instances that use the oldest launch configuration.\n      - Determine which unprotected instances in the selected AZ use the oldest launch configuration. If there is one such instance, terminate it.\n      - If there are multiple instances to terminate based on the above criteria, determine which unprotected instances are closest to the next billing hour. If there is one such instance, terminate it.\n      - If there is more than one unprotected instance closest to the next billing hour, choose one of these instances at random.\n  - An Auto Scaling Group contains a collection of EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the EC2 Auto Scaling service. The size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. You can adjust its size to meet demand, either manually or by using automatic scaling. Step scaling polices and simple scaling polices are two of the dynamic scaling options available for you to use. Both requires you to create CloudWatch alarms for the scaling policies. Both require you to specify the high and low thresholds for the alarms. Both require you to define whether to add or remove instances, and how many, or set the group to an exact size. The main differences between the policy types is the step adjustments that you get with step scaling policies. When step adjustments are applied, and they increase or decrease the current capacity of your Auto Scaling group, the adjustments vary based on the size of the alarm breach.\n  - **Simple Scaling** lets you increase or decrease the current capacity of the group based on a single scaling adjustment. The primary issue with **simple scaling** is that after a scaling activity is started, the policy must wait for the scaling activity or health check replacement to complete and the cooldown period to expire before responding to additional alarms. Cooldown periods help to prevent the initiation of additional scaling activities before the effects of previous activities are visible. \n  - With a **Target Tracking Scaling** policy, you can increase or decrease the current capacity of the group based on a target value for a specific metric. The policy will help resolve the over-provisioning of your resources. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern.\n  - In Auto Scaling, the following statements are correct regarding the cooldown period:\n    1. It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect.\n    2. Its default value is 300 seconds.\n    3. It is a configurable setting for your Auto Scaling group.\n  - A launch configuration is a template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping.\n  - You can only have one region per load balancer. ELB is designed to only run in one region, not multiple regions.\n  - You can use the dynamic and predictive scaling features of EC2 Auto Scaling to add or remove EC2 instances. Dynamic scaling responds to changing demand and predictive scaling automatically schedules the right number of EC2 instances based on predicted demand. Dynamic scaling and predictive scaling can be used together to scale faster.\n\t- **Step Scaling** allows you to increase or decrease the current capacity of the group based on a set of scaling adjustments, known as step adjustments, that vary based on the size of the alarm breach. You can set multiple actions to vary the scaling depending on the size of the alarm breach. When you create a step scaling policy, you can also specify the number of seconds that it takes for a newly launched instance to warm up.\n## **Elastic Load Balancing**\n- [What is Elastic Load Balancing?](https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html)\n### Multi-Region support\n- Load Balancers do not support multi-region. However, this can be accomplished with Route 53 and Load Balancers together. #multi-region\n### **Application Load Balancers**\n- Application Load Balancers support Weighted Target Groups routing. With this feature, you will be able to do weighted routing of the traffic forwarded by a rule to multiple target groups. This enables various use cases like blue-green, canary, and hybrid deployments without the need for multiple load balancers. It even enables zero-downtime migration between on-premises and cloud or between different compute types like EC2 and Lambda.\n\t- When you create a target group in your Application Load Balancer, you specify its target type. This determines the type of target you specify when registering with this target group. You can select the following target types:\n\t\t1. instance - The targets are specified by instance ID.\n\t\t2. ip - The targets are IP addresses.\n\t\t3. Lambda - The target is a Lambda function.\n- Host-based Routing: You can route a client request based on the Host field of the HTTP header allowing you to route to multiple domains from the same load balancer.\n- Path-based Routing: You can route a client request based on the URL path of the HTTP header.\n- HTTP header-based routing: You can route a client request based on the value of any standard of custom HTTP header.\n- HTTP method-based routing: You can route a client request based on any standard or custom HTTP method.\n- Query string parameter-based routing: You can route a client request based on query string or query parameters.\n- Source IP address CIDR-based routing: You can route a client request based on source IP address CIDR from where the request originates.\n- Handles WebSocket connections.\n- **Path-based Routing**: If your application is composed of several individual services, an ALB can route a request to a service based on the content of the request such as Host field, Path URL, HTTP header, HTTP method, Query string, or Source IP address. Path-based routing allows you to route a client request based on the URL path of the HTTP header. Each path condition has one path pattern. If the URL in a request matches the path pattern in a listener rule exactly, the request is routed using that rule.\n### **Network Load Balancers**\n- Handles WebSocket connections.\n- Does not support path-based routing or host-based routing.\n### **ALB vs NLB**   \n- You typically want an ALB for an HTTP/HTTPS web application\n- NLB's work at layer 4 only and can handle TCP and UDP. Its main feature is very high performance. It uses static IP addresses and can be assigned Elastic IPs, not possible with with ALB or ELB. NLB would be used for anything that ALBs don't cover, like near real-time data streaming services (video, stock quotes, etc.). Another use case is if your application uses non-HTTP protocols.\n- Q: A client is hosting their company website on a cluster of web servers that are behind a public-facing load balancer. The client also uses Amazon Route 53 to manage their public DNS. How should the client configure the DNS zone apex record to point to the load balancer?\n\t- Create an A record aliased to the load balancer DNS name.\n\t- _Why?_: Route 53's DNS implementation connects user requests to infrastructure running inside (and outside) of AWS. For example, if you have multiple web servers running on EC2 instances behind an Elastic Load Balancing load balancer, Route 53 will route all traffic addressed to your website (e.g. www.tutorialsdojo.com) to the load balancer DNS name (e.g. elbtutorialsdojo123.elb.amazonaws.com). Additionally, Route 53 supports the alias resource record set, which lets you map your **zone apex** (e.g. tutorialsdojo.com) DNS name to your load balancer DNS name. IP addresses associated with Elastic Load Balancing can change at any time due to scaling or software updates. Route 53 responds to each request for an Alias record set with one IP address for the load balancer.\n### **Alias records**\nIn the response to a dig or nslookup query, an alias record is listed as the record type that you specified when you created the record, such as A or AAAA.\n\n## #sysops Scenarios\n#### Concepts I don't understand:\n- route tables, what are they for?\n- difference between virtual private gateway and a customer gateway.\n- What is a network acl?\n- Networking in AWS is a huge hole in my knowledge honestly... Gotta complete the entire video course on this section and really go deep.\n\n**Question**: As part of the yearly AWS data cleanup, you need to delete all unused S3 buckets and their contents. The `tutorialsdojo` bucket, which contains several educational video files, has both the Versioning and MFA Delete features enabled. One of your Systems Engineers who has an Administrator account tried to delete an S3 bucket using the `aws s3 rb s3://tutorialsdojo` command. However, the operation fails even after repeated attempts.\n**Answer**: You can delete a bucket that contains objects using the AWS CLI only if the bucket does not have versioning enabled. If your bucket does not have versioning enabled, you can use the `rb` (remove bucket) AWS CLI command with `--force` parameter to remove a non-empty bucket. An IAM Administrator account can suspend Versioning on an S3 bucket but only the bucket owner can enable/suspend the MFA-Delete on the objects. You can configure lifecycle on your bucket to expire objects and request that Amazon S3 delete expired objects. You can add lifecycle configuration rules to expire all or a subset of objects with a specific key name prefix. For example, to remove all objects in a bucket, you can set a lifecycle rule to expire objects one day after creation. If your bucket has versioning enabled, you can also configure the rule to expire non-current objects.\nAfter your objects expire, Amazon S3 deletes the expired objects. If you just want to empty the bucket and not delete it, make sure you remove the lifecycle configuration rule you added to empty the bucket so that any new objects you create in the bucket will remain in the bucket.\n\n---\n**Question**: A leading energy company is trying to establish a static VPN connection between an on-premises network and their VPC in AWS. As their SysOps Administrator, you created the required virtual private gateway, customer gateway and the VPN connection, including the router configuration on the customer side. Although the VPN connection status seems okay in the console, the connection is not entirely working when you connect to an EC2 instance in their VPC from one of the on-premises virtual machines.\n**Answer**: To enable instances in your VPC to reach your customer gateway, you must configure your route table to include the routes used by your VPN connection and point them to your virtual private gateway. You can enable route propagation for your route table to automatically propagate those routes to the table for you.\nFor static routing, the static IP prefixes that you specify for your VPN configuration are propagated to the route table when the status of the VPN connection is UP. Similarly, for dynamic routing, the BGP-advertised routes from your customer gateway are propagated to the route table when the status of the VPN connection is UP. \n\n---\n**Question**: A digital advertising company is planning to migrate its web-based data analytics application from its on-premises data center to AWS. You designed the architecture to use an Application Load Balancer and an Auto Scaling group of On-Demand EC2 Instances which are deployed on a private subnet. The instances will be fetching data analytics from various API services over the Internet every 5 minutes. For security reasons, the EC2 instances should not allow any connections initiated from the Internet. What is the most scalable and highly available solution which should be implemented?\n**Answer**: You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances.\nTo create a NAT gateway:\n1. You must specify the public subnet in which the NAT gateway should reside.\n2. You must also specify an Elastic IP address to associate with the NAT gateway when you create it.\nAfter you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point Internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet.\n\n---\n**Question**: A document management system of a legal firm is hosted in AWS Cloud with an S3 bucket as the primary storage service. To comply with the security requirements, you are instructed to ensure that the confidential documents and files stored in AWS are secured.   \nWhich features can be used to restrict access to data in S3?\n**Answer**: By default, all Amazon S3 resources - buckets, objects, and related subresources (for example, lifecycle configuration and website configuration) are private: only the resource owner, an AWS account that created it, can access the resource. The resource owner can optionally grant access permissions to others by writing an access policy.\nAmazon S3 offers access policy options broadly categorized as resource-based policies and user policies. Access policies you attach to your resources (buckets and objects) are referred to as resource-based policies. you can also attach access policies to users in your account. These are called user policies. You may choose to use resource-based policies, user policies, or some combination of these to manage permissions to your Amazon S3 resources.\nHence, configuring the S3 bucket policy to only allow access to authorized personnel and configuring the S3 ACL on the bucket of each individual object are both correct answers.\n\n---\n**Question**: A company has a newly-hired DevOps Engineer that will assist the IT Manager in developing a fault-tolerant and highly available architecture, which is comprised of an Elastic Load Balancer and an Auto Scaling group of EC2 instances deployed on multiple AZ's. This will be used by a forex trading application that requires WebSockets, host-based and path-based routing, and support for containerized applications.\n\nWhich of the following is the most suitable type of Elastic Load Balancer that the DevOps Engineer should recommend to the IT Manager?\n\n**Answer**: Application Load Balancers support WebSockets, path-based routing, host-based routing, and support for containerized applications. Network Load Balancer are incorrect because it doesn't support path-based and host-based routing.\n\n\n---\n**Question**: An organization hosts an application across multiple Amazon EC2 instances backed by an Amazon Elastic File System (Amazon EFS) file system. While monitoring the instances, the SysOps administrator noticed that the file system's `PercentIOLimit` metric consistently hit 100% for 20 minutes or longer. This issue resulted in the poor performance of the application that reads and writes data into the file system. The SysOps admin needs to ensure high throughput and IOPS while accessing the file system.\n\nWhat step should the SysOps administrator perform to resolve the high `PercentIOLimit` metric on the file system?\n**Answer**:\n`PercentIOLimit` - Shows how close a file system is to reaching the I/O limit of the General Purpose performance mode. If this metric is at 100 percent more often than not, consider moving your application to a file system using the Max I/O performance mode.\nIf the `PercentIOLimit` percentage returned was at or near 100 percent for a significant amount of time during the test, your application should use the Max I/O performance mode. Otherwise, it should use the default General Purpose mode.\nTo move to a different performance mode, migrate the data to a different file system that was created in the other performance mode. You can use [[datasync]] to transfer files between 2 EFS file systems.\n\n---\n**Question**: An IT solutions company offers a service that allows users to upload and download files when needed. The files are retrievable for one year and are stored in Amazon S3 Standard. The SysOps administrator noticed that users frequently access the files stored on the bucket for the first 30 days, and from then on, the files are rarely accessed.\n\nThe SysOps administrator needs to implement a cost-effective S3 Lifecycle policy that maintains the object availability for users.\n\nWhich action should the SysOps administrator perform to achieve the requirements?\n\n**Answer**: Configure all buckets to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days. You may think the answer would be to configure an S3 Lifecycle policy that moves objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) class after 30 days, but this is incorrect because moving an object S3 One Zone-Infrequent will not maintain object availability for users. Amazon S3 Standard replicates data across a minimum of three AZs to protect against the loss of one entire AZ while the Amazon S3 One Zone-IA storage class replicates data within a single AZ only.\n\n---\n**Question**: A financial company is launching an online web portal that will be hosted in an Auto Scaling group of Amazon EC2 instances across multiple Availability Zones behind an Application Load Balancer (ALB). To allow HTTP and HTTPS traffic, the SysOps Administrator configured the Network ACL and the Security Group of both the ALB and EC2 instances to allow inbound traffic on ports 80 and 443. The EC2 cluster also connects to a third-party API that provides additional information on the site. However, the online portal is still unreachable over the public internet after the deployment.\n\nHow can the Administrator fix this issue?\n\n**Answer**: Allow ephemeral ports in the Network ACL by adding a new rule to allow outbound traffic on port 1024-65535.\n\nTo enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow outbound traffic from ephemeral ports. When a client connects to a service, a random port from the ephemeral port range (1024-65535) becomes the client's source port.\nThe designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL. By default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive then you need to explicitly allow traffic from the ephemeral port range.\n\n---\n**Question**: A live chat application is hosted in AWS which can be embedded as a widget in any website. It uses WebSockets to provide full-duplex communication between the users. The application is hosted on an Auto Scaling group of On-Demand EC2 instances across multiple Availability Zones with an Application Load Balancer in front to balance the incoming traffic. As part of the security audit of the company, there is a requirement that the client's IP address, latencies, request paths, and server responses are properly logged.\n\nHow can you meet the given requirement in this scenario?\n\n**Answer**: Do the following\n- Set up a standard S3 bucket where the load balancer will store the logs.\n- Enable access logging for the Application Load Balancer.\n\nElastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.\n\nAccess logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your application load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. You can disable access logging at any time.\n\nWhen you enable access logging, you must set up a standard S3 bucket where the load balancer will store the logs. The bucket must be located in the same region as the load balancer.\n\n---\n**Question**: A leading tech consultancy firm has an AWS Virtual Private Cloud (VPC) with one public subnet and a new blockchain application that is deployed to an m3.large EC2 instance. After a month, your manager instructed you to ensure that the application can support IPv6 address.\n\nWhich of the following should you do to satisfy the requirement?\n\n**Answer**:\n1. Associate an IPv6 CIDR Block with the VPC and Subnets - Associate an Amazon-provided IPv6 CIDR block with your VPC and with your subnets.\n2. Update the Route Tables - Update your route tables to route your IPv6 traffic. For a public subnet, create a route that routes all IPv6 traffic from the subnet to the Internet gateway. For a private subnet, create a route that routes all Internet-bound IPv6 traffic from the subnet to an egress-only Internet gateway.\n3. Update the Security Group Rules - Update your security group rules to includes rules for IPv6 addresses. This enables IPv6 traffic to flow to and from your instances. If you've created custom network ACL rules to control the flow of traffic to and from your subnet, you must include rules for IPv6 traffic.\n4. Change the instance type to m4.large - If your instance type does not support IPv6, change the instance type. If your instance type does not support IPv6, you must resize the instance to a supported instance type. In the example, the instance is an m3.large instance type, which does not support IPv6. you must resize the instance to a supported instance type, for example, m4.large.\n5. Assign IPv6 Addresses to the EC2 Instance - Assign IPv6 addresses to your instances from the IPv6 address range of your subnet.\n6. (Optional) Configure IPv6 on your Instances - If your instances was launched from an AMI that is not configured to use DHCPv6, you must manually configure your instance to recognize an IPv6 address assigned to the instance.\n\nTake note that the EC2 instance is an m3.large instance type, which does not support IPv6. you must resize the instance to a supported instance type, for example, m4.large. Remember that configuring an IPv6 is just an optional step.\nIf you have an existing VPC that supports IPv4 only, and resources in your subnet that are configured to use IPv4 only, you can enable IPv6 support for you VPC and resources. Your VPC can operate in dual-stack mode - your resources can communicate over IPv4, or IPv6, or both. IPv4 and IPv6 communication are independent of each other. You cannot disable IPv4 support for your VPC and subnets; this is the default IP addressing system for Amazon VPC and Amazon EC2.\n\n---\n%%%%\n\n---\n\n**Question**: A SysOps Administrator is managing a web application hosted in an Amazon EC2 instance. The security groups and network ACLs are configured to allow HTTP and HTTPS traffic in your instance. A manager has received a report that a customer cannot access the application. The Administrator is instructed to investigate if the traffic is reaching the instance. What is the best way to satisfy this requirement?\n\n**Answer**: Use Amazon VPC Flow Logs.\n\nVPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to AmazonCloudWatch Logs and Amazon S3. After you've created a flow log, you can retrieve and view it's data in the chosen destination.\n\nFlow logs can help you with a number of tasks, such as:\n- Diagnosing overly restrictive security group rules\n- Monitoring the traffic that is reaching your instance\n- Determining the direction of the traffic to and from the network interfaces\n\nTo ensure that the customer cannot access the application, you can use VPC flow logs. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored. Flow log data is collected outside of your network traffic path, and therefore does not affect network throughput or latency.\n\n---\n\n**Question**: An online stock trading application is extensively using an S3 bucket to store client data. To comply with the financial regulatory requirements, you need to generate a report on the replication and encryption status of all of the objects stored in your bucket. The report should show which type of server-side encryption is being used by each object.   \n\nAs the Systems Administrator of the company, how can you meet the above requirement with the least amount of effort?\n\n**Answer**: Use S3 Inventory to generate the required report.\n\nAmazon S3 inventory is one of the tools Amazon S3 provides to help manage your storage. You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs. You can also simplify and speed up business workflows and big data jobs using Amazon S3 inventory, which provides a scheduled alternative to the Amazon S3 synchronous List API operation.\n\nDo not use S3 Analytics, because S3 Analytics is primarily used to analyze storage access patterns to help you decide when to transition the right data to the right storage class. It does not provide a report containing the replication and encryption status of your objects.\n\nDo not use S3 Select, because S3 Select is only used to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object. It does not generate a detailed report, unlike S3 Inventory.\n\n---\n\n**Question**: A financial start-up has recently adopted a hybrid cloud infrastructure with AWS Cloud. They are planning to migrate their online payments system that supports an IPv6 address and uses an Oracle database in a RAC configuration. As the AWS Consultant, you have to make sure that the application can initiate outgoing traffic to the Internet but blocks any incoming connection from the Internet.\n\nWhich of the following options would you do to properly migrate the application to AWS?\n\n**Answer**: Migrate the Oracle database to an EC2 instance. Launch the application on a separate EC2 instance and then set up an egress-only Internet gateway.\n\nAn egress-only Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances.\n\nAn instance in your public subnet can connect to the Internet through the Internet gateway if it has a public IPv4 address or an IPv6 address. Similarly, resources on the Internet can initiate a connection to your instance using its public IPv4 address or its IPv6 address; for example, when you connect to your instance using your local computer.\n\nIPv6 addresses are globally unique, and are therefore public by default. If you want your instance to be able to access the Internet but want to prevent resources on the Internet from initiating communication with your instance, you can use an egress-only Internet gateway. To do this, create an egress-only Internet gateway in your VPC, and then add a route to your route table that points all IPv6 traffic (::/0) or a specific range of IPv6 address to the egress-only Internet gateway. IPv6 traffic in the subnet that's associated with the route table is routed to the egress-only Internet gateway.\n\nRemember that a NAT device in your private subnet does not support IPv6 traffic. As an alternative, create an egress-only Internet gateway for your private subnet to enable outbound communication to the internet over IPv6 and prevent inbound communication. An egress-only Internet gateway supports IPv6 traffic only.\n\nTake note that the application that will be migrated is using an Oracle database on a RAC configuration which is not supported by RDS.\n\n---\n\n**Question**: A company has several applications and workloads running on AWS that are managed by various teams. The SysOps Administrator has been instructed to configure alerts to notify the teams in the event that the resource utilization exceeded the defined threshold.\n\nWhich of the following is the MOST suitable AWS service that the Administrator should use?\n\n**Answer**: AWS Budgets.\n\n---\n\n**Question**: A leading national bank migrated its on-premises infrastructure to AWS. The SysOps Administrator noticed that the cache hit ratio of the CloudFront web distribution is less than 15%.\n\n**Answer**:\n- In the Cache Behavior settings of your distribution, configure to forward only the query string parameters for which your origin will return unique object.\n- Configure your origin to add a `Cache-Control max-age` directive to your object, and specify the longest practical value for `max-age` to increase your TTL.\n\n---\n\n**Question**: A microservice application is being hosted in the ap-southeast-1 and ap-northeast-1 regions. The ap-southeast-1 region accounts for 80% of traffic, with the rest from ap-northeast-1. As part of the company's business continuity plan, all traffic must be rerouted to the other region if one of the regions' servers fails.\n\nWhich solution can comply with the requirement?\n\n**Answer**: Set up an 80/20 weighted routing policy in the network load balancer and enable health checks.\n\nDo not set up a failover routing policy in AWS Route 53. This routing policy does not let you control how much traffic is routed across your resources.\n\n---\n\n**Question**: A leading media company plans to launch a data analytics application. The SysOps Administrator designed an architecture to use On-Demand EC2 instances in an Auto Scaling group that read messages from an SQS queue. A month after, the new application has been deployed to production but the Operations team noticed that when the incoming message traffic increases, the EC2 instances fall behind and it takes too long to process the messages.\n\nHow can the SysOps Administrator configure the current cloud architecture to reduce the latency during traffic spikes?\n\n**Answer**: Configure the Auto Scaling group to scale out based on the number of messages in the SQS queue.\n","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-workspaces":{"title":"AWS WorkSpaces","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **WorkSpaces**\n- Amazon WorkSpaces is a fully managed desktop virtualization service for Windows and Linux that enables you to access resources from any supported device.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/aws-x-ray":{"title":"AWS X-Ray","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **X Ray**\n- Distributed tracing application\n- Tracing Header - first service generates .. it's unique (traceID), used to track a request through your distributed application\n- Segments - Data blocks - host/ip, request, response work done (times), issues\n- Subsegments - more granular version of the above, calls to other services as part of a segment (endpoint, etc.)\n- Service Graph - JSON Document detailing services and resources which make up your application\n- Service Map - Visual version of the service graph showing traces\n- EC2 Usage - X-Ray Agent is required\n- ECS - Agent in tasks\n- Lambda - enable option\n- Beanstalk - agent preinstalled\n- API Gateway - per stage option\n- SNS \u0026 SQS\n- Requires IAM Permissions\n\n- You can use AWS X-Ray to trace and analyze user requests as they travel through your Amazon API Gateway APIs to the underlying services. API Gateway supports AWS X-Ray tracing for all API Gateway endpoint types: regional, edge-optimized, and private. You can use AWS X-Ray with Amazon API Gateway in all regions where X-Ray is available.\n- X-Ray gives you an end-to-end view of an entire request, so you can analyze latency in your APIs and their backend services. You can use an X-Ray service map to view the latency of an entire request and that of the downstream services that are integrated with X-Ray. And you can configure sampling rules to tell X-Ray which requests to record, at what sampling rates, according to criteria that you specify. If you call an API Gateway API from a service that’s already being traced, API Gateway passes the trace through, even if X-Ray tracing is not enabled on the API.\n- You can enable X-Ray for an API stage by using the API Gateway management console, or by using the API Gateway API or CLI.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/cloud-map":{"title":"AWS Cloud Map","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Cloud Map**\n- AWS Cloud Map is a cloud resource discovery service. With Cloud Map, you can define custom names for your application resources, and it maintains the updated location of these dynamically changing resources. This increases availability because your web service always discovers the most up-to-date locations of its resources.\n- Modern applications are typically composed of multiple services that are accessible over an API and perform a specific function. Each service interacts with a variety of other resources, such as databases, queues, object stores, and customer-defined micro-services, and it needs to be able to find the location of all the infrastructure resources on which it depends in order to function. In most cases, you manage all these resource names and their locations manually within the application code. However, manual resource management becomes time consuming and error-prone as the number of dependent infrastructure resources increases or the number of micro-services dynamically scale up and down based on traffic. You can also use third-party service discovery products, but this requires installing and managing additional software and infrastructure.\n- Cloud Map allows you to register any application resources, such as databases, queues, micro-services, and other cloud resources, with custom names. Cloud Map then constantly checks the health of resources to make sure the location is up-to-date. The application can then query the registry for the location of the resources needed based on the application version and deployment environment.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/cloudformation":{"title":"CloudFormation","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## Resources\n- [CloudFormation Docs](https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/Welcome.html)\n- [CloudFormation User Guide](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html)\n\t- [CloudFormation Template Reference](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-reference.html)\n\n## CloudFormation\n- CloudFormation begins with a template, which is a document written in JSON or YAML.\n```yaml\nResources:\n  Instance: ## Name of the logical resource (Can be anything)\n    Type: 'AWS::EC2::Instance'\n\t\tProperties:\n\t\t  ImageId: !Ref LatestAmiId\n\t\t  InstanceType: \"t3.micro\"\n\t\t\tKeyName: 'A4L' ## SSH key to use\n```\n- The above CloudFormation Template can be used to create many different Stacks. A Stack creates, updates, and deletes physical resources based on logical resources in the template.\n- Once a logical resource moves to `create_complete` (meaning the physical resource is active), it can be queried for attributes of the physical resource within the template.\n- Keep in mind that when a Stack is deleted, the physical resources that it manages in the AWS Cloud are also deleted.\n\n### Non-Portable Template Example\n```yaml\nResources:\n  Bucket: \n    Type: 'AWS::S3::Bucket'\n\t\tProperties:\n\t\t  BucketName: 'accatpics13333337'\n  Instance:\n    Type: 'AWS::EC2::Instance'\n\t\tProperties:\n\t\t\tKeyName: 'A4L'\n\t\t  InstanceType: 't2.micro'\n\t\t  ImageId: 'ami-04d29b6f966df1537' ## AMI ID for Amazon Linux 2\n```\n\n### Template Parameters and Pseudo Parameters\n- Both parameter types allow inputs into CloudFormation templates\n- These parameters can be used in a complementary way\n\n#### Template Parameters\n- Template Parameters accept input from the console / CLI / API when the stack is created or updated\n\t- Can be referenced from within Logical Resources to influence Physical Resources\n\t- Can be configured with Defaults, AllowedValues, Min and Max length, AllowedPatterns, NoEcho (masked passwords) \u0026 Type\n\t- Example of Template Parameters below\n```yaml\nParameters:\n  InstanceType:\n    Type: String\n    Default: 't3.micro'\n\t\tAllowedValues:\n\t\t  - 't3.micro'\n\t\t  - 't3.medium'\n\t\t  - 't3.large'\n\t\tDescription: 'Pick a supported InstanceType'\n\tInstanceAmiId:\n\t  Type: String\n\t\tDescription: 'AMI ID For Instances.'\n```\n\n#### Pseudo Parameters\n- Injected by AWS into the template or stack\n- `AWS::Region` and  `AWS::AccountId` are examples of Pseudo Parameters.\n\n#### Best Practices\n- Wherever possible, use Default Template Parameters and Pseudo Parameters to reduce user input. For example, hardcoding region names is a bad practice. Use `Fn::GetAZs`.\n\n### Intrinsic Functions\n- Allows you to gain access to data at runtime\n- Functions can be used together or in isolation\n- `Ref` \u0026 `Fn::GetAtt`\n\t- Using `!Ref` on template or pseudo parameters returns their value. When used with logical resources - the physical ID is usually returned, the primary value.\n\t\t- For an EC2 instance, `!Ref Instance` would return the physical ID of the resource: `i-1234567890abcdef0`\n\t- `!GetAtt` can be used to retrieve any attribute associated with the resource. Most logical resources return detailed configuration of the physical resource.\n\t\t- For an EC2 instance, `!GetAtt` LogicalResource.Attribute could be used to return the PublicIP `52.91.129.183` or PublicDnsName `52.91.129.183`.\n- `Fn::Join` \u0026 `Fn::Split`\n\t- Split accepts a single value delimiter and a string, and returns a list. Classic split function.\n\t- Join is the reverse, you provide a delimiter and list of values, and a string is returned.\n\t\t- Example Usage (to form a URL): `Value: !Join [ '', 'https://', !GetAtt Instance.DNSName ] ]` \n- `Fn::GetAZs` \u0026 `Fn::Select` (commonly used to pick from a list of availability zones in a given region)\n  - `Fn::GetAZs` is an environment aware function. \n\t  - Example Usage: `AvailabilityZone: !Select [ 0, !GetAZs '' ]`\n- Conditions (Fn::IF, AND, Equals, Not \u0026 Or)\n- `Fn::Base64` \u0026 `Fn::Sub`\n  - Base64 converted plaintext to Base64 encoded text\n\t- Sub replaces variables inside of a string with a variable\n\t\t- You can't do self references with Sub\n- `Fn::Cidr`\n  - In the below example, We're referencing the CIDR Block that we've just created to create a subnets. The 16 is how many subnet to generate, and the 12 is Bits per CIDR (32 - 12 = /20)\n```yaml\nVPC:\n  Type: AWS::EC2::VPC\n\tProperties:\n\t  CidrBlock: \"10.16.0.0/16\"\nSubnet1:\n  Type: AWS::EC2::Subnet\n  Properties:\n    CidrBlock: !Select [ \"0\", !Cidr [ !GetAtt VPC.CidrBlock, \"16\", \"12\" ] ]\n\t\tVpcId: !Ref VPC\n```\n\n### Mappings\n- Templates can contain a Mappings object which contain many mappings, which map keys to values, allowing lookup\n- Can have one key, or Top \u0026 Second Level\n- Mappings use the !FindInMap intrinsic function\n- Common use is to retrieve AMI for given region \u0026 architecture\n- Improves Template Portability\n- Example Syntax: `!FindInMap [ MapName, TopLevelKey, SecondLevelKey ]` might be used like this `!FindInMap [ \"RegionMap\", !Ref \"AWS::Region\", \"HVM64\"]` to retrieve an AMI for a certain region and architecture type for an EC2 instance.\n\t- The third parameter is not required, if omitted, the entire map object that corresponds to the TopLevelKey will be returned.\n\n### Outputs\n- Optional, but useful for providing status information\n- Values can be declared in this section that are visible as outputs when using the CLI\n- Visible as outputs in the console UI\n- Accessible from a parent stack when using nesting\n- Can be exported, allowing cross-stack references\n\nExample Output\n```yaml\nOutputs:\n  WordpressURL:\n    Description: \"Instance Web URL\"\n    Value: !Join [ '', 'https://', !GetAtt Instance.DNSName ] ]\n```\n- In this example, description will be visible from the CLI and Console UI \u0026 passed back to parent stack when nested stacks are used\n\n### Conditions\n- Allows a stack to react to certain conditions and change infrastructure when deployed, or specific configurations of that infrastructure\n- Declared in an optional section of the template, the `Conditions` sections. This section is processed before resources are created\n- Uses the other intrinsic functions of AND, EQUALS, IF, NOT, OR\n- Associated with logical resources to control if they are created or not\n- For example, PROD or DEV could control the size of instances created within a stack\n- Conditions can be nested, so that a condition is only true if two other conditions are true, for example\n\n```yaml\nParameters:\n  EnvType:\n    Default: 'dev'\n    Type: String\n    AllowedValues:\n      - 'dev'\n      - 'prod'\n```\n\n```yaml\nConditions:\n  IsProd: !Equals\n    - !Ref EnvType\n    - 'prod'\n```\n\n```yaml\nResources:\n  Wordpress:\n    Type: 'AWS::EC2::Instance'\n\t\tCondition: IsProd\n    Properties:\n      ImageId: 'whatever'\n```\n\n### Depends On\n- CloudFormation tries to run in parallel for create, update and delete operations\n- While doing this, it tries to determine dependency order automatically\n- This cannot always be done however. !Ref, for example, creates an implicit dependency\n- An Elastic IP requires an IGW attached to a VPC in order to work, but if there is no explicit dependency in the template using DependsOn, it won't work (This is on the #sysops exam).\n\n### Wait Conditions, Creation Policies \u0026 cfn-signal\n- Allows systems to provide more details about resource completion to CloudFormation\n- The cfn-signal command is included with the AWS CFN bootstrap package\n- Allows you to configure CloudFormation to wait for N number of success signals\n- Wait for Timeout H:M:S for those signals (12 hours max)\n- If all success signals are received, the CREATE_COMPLETE signal goes out!\n- cfn-signal is a utility running on the EC2 instance itself, explicitly sends a signal or signals to back to the CloudFormation service. If it communicates a failure, then the creation of the resource in the stack fails, and the creation of the stack itself fails.\n- If the timeout period is reached, the creation of the resource and the stack itself fails.\n- For provisioning EC2 or AutoScaling Groups, it's recommended to use a CreationPolicy. CreationPolicies are generally simpler to manage\n\n##### CreationPolicy Example:\n- In the below example, the CreationPolicy applies signal requirement of 3 and a timeout of 15 minutes.\n- The ASG provisions 3 EC2 instances, each signalling once via cfn-signal\n```yaml\nAutoScalingGroup:\nType: AWS::AutoScaling::AutoScalingGroup\nProperties:\n  ... (stuff here)\n  DesiredCapacity: '3'\n\tMinSize: '1'\n\tMaxSize: '4'\nCreationPolicy:\n  ResourceSignal:\n\t  Count: '3'\n\t\tTimeout: PT15M \n```\n\n```yaml\nLaunchConfig:\n  Type:\n  AWS::AutoScaling::LaunchConfiguration  \n\tProperties:\n\t  ... (stuff here)\n\t\tUserData:\n\t\t  \"Fn::Base64\"\n\t\t\t  !Sub |\n\t\t\t    #/bin/bash -xe\n\t\t\t\t\tyum update -y aws-cfn-bootstrap\n\t\t\t\t\t... ('some bootstrapping')\n\t\t\t\t\t/opt/aws/bin/cfn-signal -e $?\n\t\t\t\t\t--stack ${AWS::StackName}\n\t\t\t\t\t--resource AutoScalingGroup\n\t\t\t\t\t--region ${AWS::Region}\n\n```\n\n##### WaitCondition Example:\n- WaitCondition can depend on other resources. Other resources can depend on the WaitCondition.\n- The WaitHandle generates a PreSigned URL for resource signals\n```yaml\nWaitCondition:\n  Type: AWS::CloudFormation::WaitCondition\n  DependsOn: \"someresource\"\n\tProperties:\n\t  Handle: !Ref \"WaitHandle\"\n\t\tTimeout: \"300\"\n\t\tCount: '1'\n```\n\n```yaml\nWaitHandle:\n  Type: AWS::CloudFormation::WaitConditionHandle\n```\n\n### Nested Stacks (reuses the code, not the resources)\n- CloudFormation stacks are isolated by default. You cannot reuse resources in another stack or reference other stacks by default.\n- A CloudFormation stack is usually isolated, meaning it contains all the resources that are needed for an application.\n- In this paradigm, all resources in a single stack share a lifecycle.\n- However, there is a limit of 500 resources per stack.\n- Additionally, you can't easily reuse resources (e.g. A VPC)\n\n##### CFN Nested Stacks\n- Parent Stack / Root Stack is anything that has it's own nested stack\n- There is nothing special about this stack type. It is the same as any other stack, but it creates additional stacks like the one below.\n```yaml\nVPCSTACK:\n  Type: AWS::CloudFormation::Stack\n  Properties:\n    TemplateURL: https://someurl.com/template.yaml\n    Parameters:\n      Param1: !Ref SomeParam1\n      Param2: !Ref SomeParam2\n      Param3: !Ref SomeParam3\n```\n- By doing this, you can reference outputs from your nested VPC stack (NOTE: You cannot reference logical resources created in any of the nested stacks).\n- You can many many nested stacks created in your root stack, and if another stack depends on the VPCSTACK, for example, you can pass outputs from that stack into another nested stack that exists within the root stack.\n- By breaking up solutions into modular templates, it means these templates can be resources again and again for different deployments. Many nested stack architectures can use that template. Note that the resources themselves aren't being reused, just the template. A separate VPC would be created, for example, if you were to use this template in a different stack.\n- Only use Nested Stacks when the stacks are lifecycle linked.\n- Nested Stacks are used when lifecycles are linked. If they aren't you may want to use cross-stack references instead.\n\n### Cross-Stack References (reuse the actual resources in another stack)\n- If you want to use the same VPC across multiple architectures, cross-stack references are probably better suited for this aim.\n- Because of the isolation of stacks, outputs are normally not visible from other stacks\n- Outputs can be exported, making them visible from other stacks\n- Export is used to export the output of a stack. Exported outputs must have a unique name in the region\n- Anything that we want to use outside of the stack, we need to set as an export:\n```yaml\nOutputs:\n  SHAREDVPCID:\n    Description: Shared Services VPC\n\t\tValue: !Ref VPC\n\t\tExport:\n\t\t  Name: SHAREDVPC\n```\n- To reference an exported output in another stack, use the `Fn::ImportValue` instead of `Ref`. This can only be done in the same region and AWS account.\n\n### Stack Sets (CloudFormation stacks across AWS accounts \u0026 regions)\n- Deploy CFN stacks across many accounts \u0026 regions\n- StackSets are containers that live in an admin AWS account\n- StackSets can contain many Stack Instances, which are not Stacks. They are \"Reference Stacks\", which is a container for an individual stacks that run in a particular region, in a particular account.\n- Stack instances \u0026 stacks are in \"Target Accounts\"\n- Each stack = 1 region in 1 account\n- Permissions for these kind of cross account operations are granted via self-managed IAM Roles or service managed IAM roles within an organization\n- TERM: Concurrent Accounts, an integer\n\t- This defines how many individual AWS accounts can be deployed into, at the same time\n\t- The higher this value, the faster your cross account deployment\n- TERM: Failure Tolerance, an integer\n  - The amount of individual deployments that can fail, before the entire deployment is considered a failure\n- TERM: Retain Stacks, boolean\n  - Remove stack instances from a stack set, by default, any stacks will be deleted from the account.\n\t- If true, stacks will be retained when you remove the StackSet from an AWS account\nScenario: Enable AWS Config\nScenario: AWS Config Rules - MFA, EIPS, EBS Encryption\nScenario: Create IAM Roles for cross-account access\n\n### Deletion Policy: Tune resource deletion to take backups, to prevent data loss during stack delete operations\n- If you delete a logical resource from a template, by default, the physical resource is deleted\n- This can cause data loss, with RDS, example\n- With deletion policy, you can define on each resource\n- Delete (Default), Retain or (if supported) Snapshot\n- Some of the resources that support snapshots are:\n\t- EBS Volumes\n\t- Elasticache\n\t- Neptune\n\t- RDS\n\t- Redshift\n- If you delete a stack, with Snapshot selected, the Snapshot will persist past the deletion of the stack. It is your responsibility to clean up these snapshot resources.\n- The above applies to Delete, not Replace!\n\n### Stack Roles\n- CloudFormation uses the permissions of the logged in identity, which means you or the role you've assumed, need permissions to interact with Stacks, and the resources that the stacks create themselves\n- CloudFormation can assume a role to gain the permissions, which lets you implement role separation\n- The identity creating the stack doesn't need the resource permissions - only `PassRole`\n- Stack roles allow an IAM role to be passed into the stack via `PassRole`\n- A stack uses this role, rather than the identity interacting with the stack to create, update and delete AWS resources.\n- It allows role separation, and is a powerful security feature.\n\n### cfn-init: Run once to bootstrap an EC2 instance (configure, install dependencies, etc.)\n- `cfn-init` is run once as part of bootstrapping (user data) an EC2 instance (only run once, even if you update the template)\n- CloudFormation init is a native CloudFormation feature\n- Configuration directives stored in template\n- `AWS::CloudFormation::Init` part of logical resource, and here you can specify directives of what can happen on the system\n- User data is procedural - the HOW\n- Init on the other hand, is a desired state - the WHAT (works across platform, even across linux/windows)\n- If something already exists on instance that CloudFormation init wants, it will ignore it\n  - If apache is installed for example, and CloudFormation init wants to install apache, nothing will happen, since it's already installed\n- `cfn-init` helper script is installed on EC2 OS (makes it so). This is executed via user data\n```yaml\nEC2Instance:\n  Type: AWS::EC2::Instance\n  CreationPolicy: ...\n  Metadata:\n    AWS::CloudFormation::Init:\n      configSets: ... # defines which configkeys to use and in which order to apply\n      install_cfn: ...\n      software_install: ...\n      configure_instance: ...\n      install_wordpress: ...\n      configure_wordpress: ... # this is an example of a config key\n```\nThe below is ran on EC2 startup:\n```yaml\nUserData:\n  Fn::Base64: !Sub |\n    #!/bin/bash -xe\n    yum -y update\n    /opt/aws/bin/cfn-init -v --stack ${AWS::StackId} --resource EC2Instance --configsets wordpress_install --region ${AWS::Region}\n    /opt/aws/bin/cfn-signal -e --stack ${AWS::StackId} --resource EC2Instance --region ${AWS::Region}\n```\n\nConfigKey could contain the following:\n```\npackages:\n .. 'packages to install'\ngroups:\n .. 'local group mgmt'\nusers:\n .. 'local user mgmt'\nsources:\n .. 'download and extract archives'\nfiles:\n .. 'files to create'\ncommands:\n .. 'commands to execute'\nservices:\n .. 'services to enable'\n```\n\n### cfn-hup: Trigger a `cfn-init` operation when EC2 metadata changes are detected during stack update operations\n- `cfn-hup` helper is a daemon which must be installed and configured yourself (unlike `cfn-init`, which is natively available on EC2)\n- Detects changes in resource metadata, and runs configurable actions when a change is detected at stack update time\n- When a template is changed, an `UpdateStack` operation is ran, `cfn-hup` then checks metadata periodically, when updated, it calls `cfn-init`, and `cfn-init` applies the new configuration.\n- Normal user data is, by default, only executed once per stack. If you want to monitor the logical resource and initiate a new `cfn-init` when configuration updates occur in the metadata, you must install and configure `cfn-hup`.\n\n### ChangeSets: Preview changes to a template\n- When a stack is updated, 1 of 3 things may happen. No interruption, some interruption, or full replacement of resources. ChangeSets, let you apply a new template to a stack, such that when you're about to apply a stack, you can preview what is about to happen when the template is updated.\n- After creating a ChangeSet, you can then choose to apply the changes by executing the change set.\n- Similar to the Terraform plan command, which outputs a plan file that then can be passed to a Terraform apply command.\n\n### Custom Resources: Extend the functionality of CloudFormation beyond things it natively supports\n- CloudFormation doesn't support everything in AWS\n- Custom Resources are a type of logical resource that allows CloudFormation to do things that it doesn't natively support\n- Passes event data to something, and gets data back from something. This could be a Lambda function or SNS topic, for example.\n- The compute that is backing that custom resource, can pass a success or failure code back to CloudFormation.\n- The data that is sent back from the custom resource can then be referenced elsewhere in your template, just like any other resource.\n- One application of this, is automating the process by which objects are uploaded to an S3 bucket. Normally, S3 has a limitation such that it does not want to let CloudFormation delete a bucket that is not empty. How to use Custom Resources to get around this? We would use a Custom Resource, backed by a Lambda function, that uploads resources that we want inside of the S3 bucket. Then, we apply the template. Now, when we go to delete the bucket, CloudFormation knows the the CustomResource depends on the bucket. When CloudFormation deletes the stack, the Lambda now knows that the stack is being deleted, so it use that information to delete the objects in the bucket. At that point, the stack will delete the S3 bucket, which will succeed, because it is empty.\n\n## #sysops Scenarios\n\n**Question**: A SysOps Administrator needs to install and configure software applications to an EC2 instance that will be deployed using CloudFormation. The Administrator has to ensure that the applications are properly running before the stack creation proceeds. Which of the following options can satisfy the given requirement?\n\n**Answer**: Add a `CreationPolicy` attribute to the instance then send a success signal after the applications are installed and configured. Use the `cfn-signal` helper script to signal a resource.\n\nYou can associate the `CreationPolicy` attribute with a resource to prevent its status from reaching create complete until AWS CloudFormation receives a specified number of success signals or the timeout period is exceeded. To signal a resource, you can use the `cfn-signal` helper script or SignalResource API. AWS CloudFormation publishes valid signals to the stack events so that you track the number of signals sent.\n\nThe creation policy is invoked only when AWS CloudFormation creates the associated resource. Currently, the only AWS CloudFormation resources that support creation policies are `AWS::AutoScaling::AutoScalingGroup`, `AWS::EC2::Instance`, and `AWS::CloudFormation::WaitCondition`.\n\nUse the `CreationPolicy` attribute when you want to wait on resource configuration actions before the stack creation proceeds. For example, if you install and configure software applications running on an EC2 instance, you might want those applications to be running before proceeding. In such cases, you can add a `CreationPolicy` attribute to the instance, and then send a success signal to the instance after the applications are installed and configured.\n\n---\n\n**Question**: A SysOps Administrator needs to create a CloudFormation template that should automatically rollback in the event that the entire stack failed to launch. The application stack requires the pre-requisite packages to be installed first in order for it to run properly, which could take about an hour or so to complete.\n\nWhat should the Administrator add in the template to accomplish this requirement?\n\n**Answer**: In the `ResourceSignal` parameter of the `CreationPolicy` resource attribute, add a `Timeout` property with a value of 2 hours.\n\nAssociate the `CreationPolicy` attribute with a resource to prevent its status from reaching create complete until AWS CloudFormation receives a specified number of success signals or the timeout period is exceeded. To signal a resource, you can use the `cfn-signal` helper script or `ScriptResource` API. AWS CloudFormation publishes valid signals to the stack events so that you track the number of signals sent.\n\nThe creation policy is invoked only when AWS CloudFormation creates the associated resource. Currently, the only AWS CloudFormation resources that support creation policies `AWS::AutoScaling::AutoScalingGroup`, `AWS::EC2::Instance`, `AWS::CloudFormation::WaitCondition`.\n\nUse the `CreationPolicy` attribute when you want to wait on resource configuration actions before stack creation proceeds. For example, if you install and configure software applications on an EC2 instance, you might want those applications to be running before proceeding. In such cases, you can add a `CreationPolicy` attribute to the instance, and then send a success signal to the instance after the applications are installed and configured.\n```\nCreationPolicy:\n  AutoScalingCreationPolicy:\n    MinSuccessfulInstancesPercent: Integer\n  ResourceSignal:\n    Count: Integer\n    Timeout: String\n```\n\nThe `Timeout` property is the length of time that AWS CloudFormation waits for the number of signals that were specified in the `Count` property. The timeout period starts after AWS CloudFormation starts creating the resource, and the timeout expires no sooner than the time you specify but can occur shortly thereafter. The maximum time that you can specify is 12 hours.\n\n---\n**Question**: A SysOps Administrator has been instructed to handle the deployment of the cloud resources in a single AWS account using CloudFormation. The Administrator must develop a unified template that can be reused for multiple environments instead of manually copying and pasting the same configurations into the template. The dedicated template will be used and referenced from within other templates in the same AWS Region. If the template has been updated, any stack that is referencing it will automatically use the updated configuration.\n\nHow can the Administrator meet this requirement?\n\n**Answer**: Use Nested Stacks.\n\nNested stacks are stacks created as part of other stacks. You can create a nested stack within another stack by using the `AWS::CloudFormation::Stack` resource.\n\nAs your infrastructure grows, common patterns can emerge in which you declare the same components in multiple templates. You can separate out these commons components and create dedicated templates for them. Then use the resource in your template to reference other templates, creating nested stacks.\n\nFor example, assume that you have a load balancer configuration that you use for most of your stacks. Instead of copying and pasting the same configurations into your templates, you can create a dedicated template for the load balancer. Then, you just use the `AWS::CloudFormation::Stack` resource to reference that template from within other templates.\n\nIf the load balancer template is updated, any stack that is referencing it will use the updated load balancer (only after you update the stack). In addition to simplifying updates, this approach lets you use experts to create and maintain components that you might not necessarily be familiar with. All you need to do is reference their templates.\n\nStackSets would be incorrect here because a stack set simply lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. The scenario mentioned that the cloud resources are in a single AWS account and also, the dedicated template will be referenced from within other templates in the same AWS Region. For this kind of situation, using a nested stack is more suitable than StackSets.\n\nChangeSets would also be incorrect because a ChangeSet is primarily used to preview how the proposed changes to a stack might impact your running resources in AWS.\n\nStackPolicies is also incorrect because a StackPolicy is commonly used to prevent stack resources from being unintentionally updated or deleted during a stack update. A stack policy is a JSON document that defines the update actions that can performed on designated resources.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/cloudfront":{"title":"CloudFront","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **CloudFront**\n- CloudFront is a Content Delivery network (CDN) within AWS\n- Improves the delivery of content by caching and using an efficient global network\n- When a user makes a request, the following happens in the CloudFront Network:\n\t1. Checks closest Edge Location. If a cache hit, response is returned.\n\t2. Else, if a cache miss occurs, checks Regional Edge Cache. If a cache hit, response is returned.\n\t3. Else, if another cache miss occurs, fetches from Origin. Writes to both Regional Edge Cache, and Edge Location\n- CloudFront integrates with ACM (AWS Certificate Manager) so that you can use SSL certificates with CloudFront (enabling HTTPS)\n- **CloudFront is for download operations only.** Any uploads go direct to the origin. CloudFront performs read-only caching.\n\n### Helpful Links\n- [CloudFront Pricing](https://aws.amazon.com/cloudfront/pricing/)\n\n###  Origin \n- The source location of where your content lives\n- Can either be an S3 Origin or a Custom Origin (Anything else which runs a web server, and has a publicly routable IPv4 address)\n- Origin Groups provides resiliency across origins in a failover event\n\n#### Origin Types\n- S3 buckets (Note: An S3 bucket used for static website hosting, is viewed as a custom origin)\n- Webservers (custom origins)\n- AWS MediaPackage channel endpoints\n- AWS MediaStore container endpoints\n\n##### S3\n- Origin Path: Allows specification of a particular path in the bucket to be used as the top level\n- Origin Access Identity: Allows you to give CloudFront a virtual identity, which it can use to access the S3 origin. Only works for S3 origins.\n- S3 Origins have the same Viewer Protocol and Origin Protocol\n\n##### Custom Origins\nAllows you to configure the following (which isn't possible on S3 origins)\n- Minimum Origin SSL Protocol\n- Origin Protocol Policy (HTTP, HTTPS, Match Viewer Protocol Policy)\n- HTTP Port (can be custom)\n- HTTPS Port (can be custom)\n- Cannot use Origin Access Identities\n\n### Security - Origin Side (Between Origin \u0026 Edge Location)\nSecuring S3 Origins:\n- An OAI is a type of identity\n- It can be associated with CloudFront Distributions\n- CloudFront becomes that OAI\n- That OAI can be used in S3 Bucket Policies\n- DENY all BUT one or more OAI's\n\nSecuring Custom Origins:\n- Configuring Origin to require the presence of a Custom Header. Because we use HTTPS, nobody can spy on the headers, and the headers come from the Edge Location\n- Use a firewall that is configured to allow requests from CloudFront IPs, but nowhere else.\n- These approaches can be used in combination\n\n### Security - Viewer Side (Between Edge Location \u0026 Customer)\n- CloudFront can run in private or public mode (default is public)\n- Requests made to CloudFront must be made with signed URL or signed cookie in private mode\n- 1 Behavior - Whole Distribution PUBLIC or PRIVATE\n- Multiple Behaviors - each is PUBLIC or PRIVATE\n- A CloudFront Key is created by an Account Root User\n- That account is added as a TRUSTED SIGNER\n- What generates pre-signed cookies or URLs? Usually an application like an API Gateway with a Lambda Signer\n\n#### Signed URL\n- URL provides access to one object per URL\n- Legacy RTMP distributions can't use cookies\n- Use URLs if your client doesn't support cookies\n\n#### Signed Cookies\n- Cookies provide access to groups of objects\n- Use for groups of files/all files of a type - e.g. all cat gifs\n- Use if maintaining URLs is important in your application\n\n### Distribution \n- The configuration unit of CloudFront (Can have multiple origins, configured inside the distribution)\n\t- Distributions can be configured to use an alternate domain name, like `http://connerverret.com`\n\t- Once you've configured a Distribution, you can deploy that Distribution to the CloudFront Network, pushing it to all chosen Edge Locations, allowing the Edge Locations to be used by your customers, because the Edge Locations now have the configuration stored within the Distribution.\n\t- Most important configuration are actually configured with Behaviors, which is a sub-configuration within a Distribution.\n\n### Behavior (Cache Behavior, sub-configuration for a Distribution)\n- CloudFront Behaviors control much of the TTL, protocol, and privacy settings within CloudFront\n- A single distribution can have multiple behaviors\n- Includes a default(`*`) path pattern behavior (matches anything not matched by a more specific behavior)\n- For any requests incoming to an Edge Location, these requests are pattern matched against any behaviors in use for that Distribution using the path pattern\n- Once a path pattern is matched, it is then subject to any of the options, such as\n\t- Origin or Origin Group to use\n\t- Viewer Protocol Policy (HTTP and HTTPs, Redirect HTTP to HTTPS, or HTTPS only)\n\t- Allowed HTTP Methods (GET + HEAD, GET + HEAD + OPTIONS, GET + HEAD + OPTIONS + PUT + POST + PATCH + DELETE)\n\t- Field-level encryption\n\t- Cached HTTP Methods (GET, HEAD are cached by default)\n\t- Cache and origin request settings\n\t- Cache based on request headers (none, whitelist, or all)\n\t- **Restrict viewer access to a behavior**, uses Signed URLs or signed Cookies (**Important for exam**)\n\t\t- This requires Trusted Signers, which are accounts that able to generate a signed URL or signed Cookie\n\t- Associate Lambda @ Edge functions\n- Origins are used by behaviors as content sources\n- A distribution can have many behaviors which are configured with a path pattern, If requests match that pattern, that behavior is used, otherwise the default is used, for example private (`img/*`)\n- Origins, Origin Groups, TTL, Protocol Policies, restricted access are configured via Behaviors\n- Each behavior can have a precedence, with the default starting at 0 (lowest priority)\n\n#### TTL and Invalidations\n- Used to influence how long objects are stored at Edge Locations, and when they're ejected\n- When a cached object expires at an Edge Location, it isn't immediately discarded, but it becomes stale. If another customer requests this object, the request will be forwarded to the origin. If the origin detects that the stale object is still current, the origin returns a `304 Not Modified` to the Edge Location. If there a newer version, a `200 OK` is returned, along with the new version of the object.\n- More frequent cache hits = lower origin load (better user performance)\n- Default TTL is 24 hours (validity period, defined as a behavior)\n- You can set Minimum TTL and Maximum TTL values on a behavior (Lower and upper bounds for any TTLs set on an individual object basis using headers)\n- Object specific TTL values can be controlled used headers\n\t- Origin Header: `Cache-Control max-age` (seconds)\n\t- Origin Header: `Cache-Control s-maxage` (seconds) (same exact functionality as the above header)\n\t- Origin Header: `Expires` (Date \u0026 Time)\n\t- If any of these per object TTL values exceed either the Minimum TTL or Maximum TTL thresholds, the per object TTL is ignored, and either the Minimum or Maximum TTL is used\n- These values are either set on Custom Origin, or in the case of S3, set via object metadata (Can be set using the S3 API, CLI, or Console UI)\n- Cache Invalidation - performed on a Distribution, applies to all edge locations (takes time)\n- A specific object can be invalidated using a specific path or wildcard\n\t- `/images.whiskers1.jpg`\n\t- `/images.whiskers*`\n\t- `/images/*`\n\t- `/*` - Invalidates all objects in a Distribution\n- Cache Invalidation has a cost, so it should only be done to correct an error. If you fine yourself using it all the time, look into using versioned file names.\n- Versioned file names are better, because even if the objects are cached in a customers browser, you'll still get a new object. Also makes log files more useful.\n- Don't confuse versioned file names with S3 versioning\n\n### Edge Locations\n- Local cache of your data (Names of the pieces of global infrastructure where your content is cached. Located globally, in or around large cites, over 200 Edge Locations)\n\t- Smaller than region, generally in 3rd party data centers, primarily used for storage and caching of data\n\t- Cannot be used for an EC2 instance, for example\n\n### Regional Edge Cache\n- Larger versions of an edge location. Provides another layer of caching. Less of these exist, compared to Edge Locations. \n\t- These exist in between the Edge Locations and the Origin. \n\t- Support a number of Edge Locations in the same geographical area\n\n\n  - By using AWS WAF, you can configure web access control lists (Web ACLs) on your CloudFront distributions or Application Load Balancers to filter and block requests based on requests signatures. Each Web ACL consists of rules that you can configure to string match or regex match one or more request attributes, such as the URI, query-string, HTTP method, or header key. In addition, by using AWS WAF's rate-based rules, you can automatically block the IP addresses of bad actors when requests matching a rule exceed a threshold you define. It is recommend that you add web ACls with rate-based rules as part of your AWS Shield Advanced protection. These rules can alert you to sudden spikes in traffic that might indicate a potential DDoS.\n- CloudFront signed URLs and signed cookies provide the same basic functionality: they allow you to control who can access your content. If you want to serve private content through CloudFront and you're trying to decide whether to use signed URLs or signed cookies, consider the following:\n\t- Use **signed URLs** for the following cases:\n\t\t- You want to use an RMTP distribution. Signed cookies aren't supported for RMTP distributions.\n\t\t- You want to restrict access to individual files, for example, an installation download for your application.\n\t\t- Your users are using a client (for example, a custom HTTP client) that doesn't support cookies.\n\t- Use **signed cookies** for the following cases:\n\t\t- You want to provide access to multiple restricted files, for example all of the files for a video in HLS format or all the files in the subscriber's area of a website.\n\t\t- You don't want to change your current URLs.\n\n### CloudFront SSL\n- CloudFront Default Domain Name (CNAME)\n\t- Appears in this format: `https://d111111abcdef8.cloudfront.net/` or `http://d111111abcdef8.cloudfront.net`\n\t- SSL is supported by default ... `*.cloudfront.net` certificate (covers all default CF distributions, but most of the time, you'll want your own domain name)\n- You must verify ownership (optionally HTTPS) using a matching certificate\n- This can be done with the ACM, but it must be done in the us-east-1 region, since it's a global service\n- Behaviors can be set to allow HTTP or HTTPS, HTTP to HTTPS redirection, or restricted to HTTPS only (causes HTTP requests to fail)\n- Two SSL Connections: Viewer =\u003e CloudFront and CloudFront =\u003e Origin\n\t- Both of these connections need valid public certificates (and intermediate certs)\n\t\t- Self-signed certificates do not work with CloudFront, only public certificates can be used with CloudFront\n- Old browsers don't support SNI ... to support these older browsers, a dedicated IP is required, which CF charges extra for\n- SNI mode is free as part of the service, but a dedicated IP can be used at an Edge Location to support HTTPS on older browsers ($600 per month)\n- The Viewer Protocol (Browser to CloudFront) and Origin Protocol (CloudFront to Origin) BOTH require a public SSL certificate\n- If you use an ALB, it needs a publicly trusted certificate (ACM)\n- If you use a Custom Origin, you need a publicly trusted certificate, but it cannot be generated with ACM, because ACM doesn't support Custom Origins\n- The certificate needs to match the DNS name of the origin\n\n#### History of SSL\n- Historically (before 2003), every SSL enabled site needed its own IP\n- Encryption starts at the TCP connection\n- Host headers happens after that - Layer 7 / Application Layer\n- In 2003, an extension was added to TLS called SNI, which allows a host to be included\n- Server Name Indication adds the ability for a client to tell a server which domain name its attempting to access. This occurs within the TLS handshake, so before HTTP even gets involved\n- Resulting in many SSL certs/hosts using a shared IP\n\n### CloudFront Geo Restriction\n- Restrict content to a particular location\n- CF - Whitelist or Blacklist - Only works with countries. Uses GeoIP Database 99.8%+\n- Applies to the entire distribution\n\n### CloudFront 3rd Party Geolocation\n- Completely customizable\n- Requires a piece of compute in the architecture that acts as a decider\n- Requires CloudFront distribution to be configured as private\n- Requires either a signed URL or signed cookie\n- Can be used to allow/deny based on anything the application has exposure to\n\n### Lambda@Edge\n- Allows you to run lightweight Lambda functions at edge locations\n- Adjust data between the Viewer \u0026 Origin\n- Don't have full Lambda feature set\n- Only supports Node and Python\n- Runs in the AWS Public Space (not VPC)\n- Layers are not supported\n- Lambda can run during any of these periods\n\t- Viewer Request (Runs after CloudFront receives a request from a viewer)\n\t- Origin Request (Runs before CloudFront forwards a request to an origin)\n\t- Origin Response (Runs after CloudFront receives a response from an origin)\n\t- Viewer Response (Runs before response is forwarded to viewer)\n\nUse Cases:\n- A/B testing - Viewer Request - 2 different versions of an image\n- Migration between S3 Origins - Origin Request - Based on a weighted value\n- Different objects based on device - Origin Request\n- Content By Country - Origin Request","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/cloudhsm":{"title":"CloudHSM","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **CloudHSM** \n- A cloud based hardware security modules (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. With CloudHSM, you can manage your own encryption keys using FIPS 140-2 Level 3 validated HSMs. CloudHSM offers you the flexibility to integrate with your applications using industry-standard APIs, such as PKCS#11, Java Cryptography Extensions (JCE), and Microsoft CryptoNG (CNG) libraries.\n  - If the HSM is zeroized (which can happen if an administrator attempts to login 3 times and fails), the keys will be lost permanently if you do not have a copy.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/cloudtrail":{"title":"CloudTrail","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **CloudTrail**\n- Almost everything that can be done to an AWS account is logged by CloudTrail\n- Logs API calls/account activities as a CloudTrail Event\n- Actions taken by users, roles or services\n- 90 days stored by default in Event History, S3 CloudTrail logs storage are not included by default\n- Enabled by default - no cost for 90 day history\n- To customize the service, create 1 or more Trails\n- CloudTrail Logs are encrypted by default with Amazon S3 server-side encryption\n- CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. By default, CloudTrail is enabled on your AWS account when you create it. When activity occurs in your AWS account, that activity is recorded in a CloudTrail event. You can easily view recent events in the CloudTrail console by going to Event history. CloudTrail provides event history of your AWS account activity, including actions taken through AWS Management console, AWS SDKs, command line tools, API calls, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting.\n- CloudTrail cannot be used for real-time logging, there can be up to a 15 minute delay\n\n### Management Events\n- **Management Events** provide visibility into management operations that are performed on resources in your AWS account. These are also known as control plane operations. Management events can also include non-API events that occur in your account.\n- Things such as creating an EC2 instance, creating a VPC, terminating an EC2 instance\n- These are logged by CloudTrail by default\n\n### Data Events\n- **Data Events**, on the other hand, provide visibility into the resource operations performed on or within a resource. These are also known as data plane operations. It allows granular control of data event logging with advanced event selectors. You can currently log data events on different resource types such as Amazon S3 object-level API activity (e.g. GetObject, DeleteObject, and PutObject API opeations), AWS Lambda function execution activity (the Invoke API), DynamoDB Item actions, and many more.\n- Data Events are much higher volume than Management Events, and are not logged by default\n\n### CloudTrail Trail\n- Unit of configuration within the CloudTrail product\n- This is how you provide configuration to CloudTrail on how to operate\n- Logs events for the AWS region that it is created in, in the case of a One Region Trail, but an All Region Trail can be created\n- Most services log regions in the region that it was created in\n- Global Service Events is an option that needs to be enabled, if you want to log events from global services, such as IAM, STS and CloudFront.\n- When a Trail is created, you can store logs indefinitely in S3 as compressed JSON files. These files are parsable by any tools that parse JSON.\n- Logs can also be stored in CloudWatch\n- You can create an Organizational Trail, which is a single management point for all accounts in that Organization. It makes managing multi-account environments much easier.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/cloudwatch":{"title":"CloudWatch","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **CloudWatch**\nIngest, store and manage metrics\n- Public Service - public space endpoints (can be accessed both on-prem or on AWS)\n- Needs public internet access to access the public space endpoint\n- AWS Service integration - management place\n- Agent integration .. e.g. EC2 - richer metrics\n- On-Premises integration via Agent/API (custom metrics)\n- Application Integration via API/Agent (custom metrics)\n- View data via console UI, CLI, API, dashboards and anomaly detection\n- Alarms ... react to metrics, can be used to notify or perform actions\n- Instances in a VPC can connect to CloudWatch via an Internet Gateway or Interface Endpoint\n- Supports Billing Alarms\n- CloudWatch recently was updated so that it can now aggregate data across AWS Regions\n\n### CloudWatch - Data\n- Namespace = container for metrics e.g. `AWS/EC2` \u0026 `AWS/Lambda` (note: custom namespaces don't start with `AWS`)\n- Datapoint, the things that CloudWatch records = Timestamp, value, (optional) unit of measure\n- Metrics = time ordered set of data points, like `CPUUtilization`, `NetworkIn, DiskWriteBytes` (EC2)\n\t- Every metric has a MetricName (`CPUUtilization`) and a Namespace (`AWS/EC2`)\n- Dimension .. name/value pair\n\t- e.g. `CPUUtilization` Name=InstanceID, Value=i=111111111\n\t- `AutoScalingGroupName`, `ImageId`, `InstanceId`, `InstanceType`\n- Resolution .. Standard (60s granularity) .. High (1s - costs more)\n\t- Retention\n\t\t- sub 60s are retained for 3 hours\n\t\t- 60s retained for 15 days\n\t\t- 300s (5 minutes) retained for 63 days\n\t\t- 3600s (1 hour) retained for 455 days\n\t\t- As data ages, its aggregated and stored for longer with less resolution (detail matters less as data ages)\n- Statistics: aggregation over a period (e.g. `Min`, `Max`, `Sum`, `Average`)\n- Percentile - e.g. p95 and p99\n- Alarms - watches a metric over a time period\n\t- ALARM or OK are the two states of an alarm\n\t- value of a metric vs threshold over time\n\t- Alarms that trigger to ALARM state can trigger one or more actions, triggered by EventBridge\n\t- Alarm Resolution\n\n### CloudWatch Logs\n- Public Service - usable from AWS or on-premises\n- Store, Monitor and access logging data\n- AWS Integrations - EC2, VPC Flow Logs, Lambda, CloudTrail, R53 and more\n- OS logs on EC2 can be logged using the CloudWatch Agent (this also works for on-premises servers)\n- Can generate metrics based on logs - metric filter\n\n### CloudWatch Events and EventBridge\n- EventBridge is a superset of CloudWatch Events, and is replacing the service altogether. It can do additional things that CloudWatch Events alone couldn't, such as 3rd party events and custom applications.\n- If X happens, or at Y times(s) ... do Z\n- EventBridge is ... CloudWatch Events v2, start using EventBridge by default\n- A default Event bus for the account\n- In CloudWatch Events this is the only bus (implicit)\n- EventBridge can have additional buses\n- Rules match incoming events (or schedules)\n- Route the events to 1+ targets e.g. Lambda\n\nDefault Event Bus\n- When an EC2 changes state, an event is added to the event bus\n- Events themselves are just JSON structures, and can be passed to various targets (like Lambda)\n\n### Misc. Notes\n- CloudWatch has available Amazon EC2 Metrics for you to use for monitoring CPU utilization, Network utilization (network packets out of an EC2 instance), Disk performance, and Disk read/writes. In case you need to monitor the below items, you need to prepare a custom metric using a Perl or other shell script, as there are no ready to use metrics for:\n\t- Memory Utilization\n\t- Disk swap utilization\n\t- Disk space utilization\n\t- Page file utilization\n\t- Log collection\n- Take note that you can't use the default metrics of CloudWatch to monitor the SwapUtilization metric. To monitor custom metrics, you must install the CloudWatch agent on the EC2 instance. After installing the CloudWatch agent, you can now collect system metrics and log files of an EC2 instance.\n- Using CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your EC2 instances. You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs. You _could_ do something such as writing a python script that queries the EC2 API for each instance status check, writing a shell script that periodically shuts down and starts instances based on certain stats, etc. but these solutions are all unnecessary to go through such lengths when CloudWatch Alarms already has such a feature for you, offered at a low cost.\n- If you need to collect CPU utilization of individuals processes that are running on a Linux-based EC2 server, you should use the Amazon CloudWatch agent `procstat` plugin.\n- [[rds]] Enhanced Metrics that you can view in CloudWatch:\n\t- RDS Child Processes\n\t\t- Shows a summary of the RDS processes that support the DB instance, for example aurora for Amazon Aurora DB clusters and mysqld for MySQL DB instances. Process threads appear nested beneath the parent process. Process threads show CPU utilization only as other metrics are the same for all threads for the process. The console displays a maximum of 100 processes and threads. The results are a combination of the top CPU consuming and memory consuming processes and threads. If there are more than 50 processes and more than 50 threads, the console displays the top 50 consumers in each category. This display helps you identify which processes are having the greatest impact on performance.\n\t- RDS Processes\n\t\t- Shows a summary of the resources used by the RDS management agent, diagnostics monitoring processes, and other AWS processes that are required to support RDS DB instances.\n\t- OS Processes\n\t\t- Shows a summary of the kernel and system processes, which generally have minimal impact on performance.\n\n### Basic Monitoring and Detailed Monitoring\n- [AWS Docs: Basic monitoring and detailed monitoring](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-metrics-basic-detailed.html)\nCloudWatch provides two categories of monitoring: _basic monitoring_ and _detailed monitoring_.\n\nMany AWS services offer basic monitoring by publishing a default set of metrics to CloudWatch with no charge to customers. By default, when you start using one of these AWS services, basic monitoring is automatically enabled. For a list of services that offer basic monitoring, see [AWS services that publish CloudWatch metrics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/aws-services-cloudwatch-metrics.html).\n\nDetailed monitoring is offered by only some services. It also incurs charges. To use it for an AWS service, you must choose to activate it. For more information about pricing, see [Amazon CloudWatch pricing](http://aws.amazon.com/cloudwatch/pricing).\n\nDetailed monitoring options differ based on the services that offer it. For example, Amazon EC2 detailed monitoring provides more frequent metrics, published at one-minute intervals, instead of the five-minute intervals used in Amazon EC2 basic monitoring. Detailed monitoring for Amazon S3 and Amazon Managed Streaming for Apache Kafka means more fine-grained metrics.\n\nIn different AWS services, detailed monitoring also has different names. For example, in Amazon EC2 it is called detailed monitoring, in AWS Elastic Beanstalk it is called enhanced monitoring, and in Amazon S3 it is called request metrics.\n\nUsing detailed monitoring for Amazon EC2 helps you better manage your Amazon EC2 resources, so that you can find trends and take action faster. For Amazon S3 request metrics are available at one-minute intervals to help you quickly identify and act on operational issues. On Amazon MSK, when you enable the `PER_BROKER`, `PER_TOPIC_PER_BROKER`, or `PER_TOPIC_PER_PARTITION` level monitoring, you get additional metrics that provide more visibility.\n\nThe following list shows the services that offer detailed monitoring.\n- Amazon API Gateway\n- Amazon CloudFront\n- Amazon EC2\n- Elastic Beanstalk\n- Amazon Kinesis Data Streams\n- Amazon MSK\n- Amazon S3\n\n## #sysops Scenarios\n**Question**: A SysOps team is in the process of automating tasks to expedite the time to recover Amazon instances in the event of underlying hardware failure. The team must ensure that the attached Elastic IP and the private IP address of the original instance are retained after the instance is recovered. Additionally, an automated email notification should be in place to inform everyone in the SysOps team once a recovery process is triggered to run.\n**Answer**: You can create an Amazon CloudWatch alarm that monitors an Amazon [EC2](ec2.md) instance and automatically recovers the instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered.\nA recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance has a public IPv4 address, the instance retains the public IPv4 address after recovery. If the impaired instance is in a placement group, the recovered instance runs in the placement group.\nWhen the `StatusCheckFailed_System` alarm is triggered, and the recovery action is initiated, you will be notified by the Amazon SNS topic that you selected when you created the alarm and associated the recover action. During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost. When the process is complete, information is published to the SNS topic you've configured for the alarm. Anyone who is subscribed to this SNS topic will receive an email notification that includes the status of the recovery attempt and any further instructions. You will notice an instance reboot on the recovered instance.\nIt's worth noting here that the `StatusCheckFailed_Instance` metric in CloudWatch just monitors the software and network configuration of your individual instance. It does not detect if the underlying hardware failed.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/codebuild":{"title":"CodeBuild","content":"\n### Related Notes\n- [AWS Wiki]\n\n## CodeBuild\nYou can run CodeBuild locally to test out BuildSpecs without needing to put them onto the Cloud every time\n- [Run builds locally with the AWS CodeBuild agent](https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html)","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/data-lifecycle-manager":{"title":"Data Lifecycle Manager","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **DLM - Data Lifecycle Manager** (EBS Volume Snapshot Management)\n- You can use **Amazon Data Lifecycle Manager (Amazon DLM)** to automate the creation, retention, and deletion of snapshots taken to back up your Amazon [EBS](/notes/aws/ebs.md) volumes. Automating snapshot management helps you to:\n\t- Protect valuable data by enforcing a regular backup schedule.\n\t- Retain backups as required by auditors or internal compliance.\n\t- Reduce storage costs by deleting outdated backups.\n- Combined with the monitoring features of Amazon CloudWatch Events and AWS CloudTrail, Amazon DLM provides a complete backup solution for EBS volumes at no additional cost. It is the fastest and most cost effective way to solution for automatically backing up your EBS volumes.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/datasync":{"title":"DataSync","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **AWS DataSync** (*moves/migrates data*)\n- Fastest on-prem storage to cloud storage data migration service, used more for historical archival or long term storage\n- AWS DataSync allows you to copy large datasets with millions of files, without having to build custom solutions with open source tools, or license and manage expensive commercial network acceleration software. You can use DataSync to migrate active data to AWS, transfer data to the cloud for analysis and processing, archive data to free up on-premises storage capacity, or replicate data to AWS for business continuity.\n- AWS DataSync enables you to migrate your on-premises data to Amazon S3, Amazon EFS, and Amazon FSx for Windows File Server. You can configure DataSync to make an initial copy of your entire data set, and schedule subsequent incremental transfers of changing data towards Amazon S3. Enabling S3 Object Lock prevents your existing and future records from being deleted or overwritten.\n- AWS DataSync is primarily used to migrate existing data to Amazon S3. On the other hand, AWS Storage Gateway is more suitable if you still want to retain access to the migrated data and for ongoing updates from your on-premises file-based applications.\n- Transfers data between on-premises and AWS\n- Transfers data between AWS storage services\n- Transfers data between AWS and other locations\n- Securely migrate your data to AWS with end-to-end security, including data encryption and data integrity validation.\n- Reduce expensive on-premises data movement costs with a fully managed service that seamlessly scales as data loads increase.\n- Easily manage data movement workloads with bandwidth throttling, migration scheduling, and task filtering.\n- Rapidly migrate file and object data to the cloud for data replication or archival.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/direct-connect":{"title":"Direct Connect (DX)","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [Site-to-Site VPN](/notes/aws/site-to-site-vpn.md)\n- [Transit Gateway](/notes/aws/transit-gateway.md)\n\n## Direct Connect\nDirect Connect is AWS's physical private link connecting your business premises to its public and private services\n- A physical connection (1, 10, or 100 Gbps options) to an AWS Region\n- Business Premises =\u003e DX Location =\u003e AWS Region\n\t- AWS Regions have multiple DX Locations. These are normally major metro data centers.\n- Physical Port Allocation at a DX Location and authorization to connect to that port\n- Port Hourly Cost \u0026 Outbound Data transfer\n- Provisioning time ... physical cables \u0026 no resilience\n- Low \u0026 consistent latency + High Speeds\n- Can be used to access AWS Private Services (VPCs) and AWS Public Services - NO INTERNET\n- Many pros and cons compared to a Site-to-Site VPN\n\n### Resilience\n- DX Locations are connected to the AWS regions via redundant high speed connections. You can assume these are highly available.\n- Direct Connect is not resilient by default, but it can be if customized.\n- Improvements\n\t- Use 2 AWS DX Routers at the DX Location, each mapping to multiple Customer DX Routers, which run back to 2 Customer Premises Routers at the on-premises location. You are now safe from a failure from a router from either of the two paths. This isn't completely safe though. If either the DX Location or the Customer Premises location fails, you have an outage.\n\t- To improve upon this further, use two different DX locations and two different Customer Premises.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/disaster-recover-scenarios":{"title":"Disaster Recovery Scenarios","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Disaster Recovery Scenarios** (Route 53, CloudWatch)\n  - Use an active-passive failover configuration when you want a primary resource or group of resources to be available majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes only the healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries.\n  - To create an active-passive failover configuration with one primary record and one secondary record, you just create the records and specify Failover for the routing policy. When the primary resource in healthy, Route 53 responds to DNS queries using the primary record. When the primary resource in unhealthy, Route 53 responds to DNS queries using the secondary record.\n  - You can configure a health check that monitors an endpoint that you specify either by IP address or by domain name. At regular intervals that you specify, Route 53 submits automated requests over the internet to your application, server, or other resource to verify that it's reachable, available, and functional. Optionally, you can configure the health check to make requests similar to those that your users make, such as requesting a web page from a specific URL.\n  - When Route 53 checks the health of an endpoint, it sends an HTTP, HTTPS, or TCP request to the IP address and port that you specified when you created the health check. For a health check to succeed, your router and firewall rules must allow inbound traffic from the IP addresses that the Route 53 health checkers use.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/dynamodb":{"title":"DynamoDB","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [DAX - DynamoDB Accelerator](/notes/aws/dynamodb-accelerator.md)\n\n## **DynamoDB** \n- The partition key portion of a table's primary key determines the logical partitions in which a table's data is stored. This is turn affects the underlying physical partitions. Provisioned I/O capacity for the table is divided evenly among these physical partitions. Therefore a partition key design that doesn't distribute I/O requests evenly can create \"hot\" partitions that result in throttling and use your provisioned I/O capacity inefficiently. The optimal usage of a table's provisioned throughput depends not only on the workload patterns of individual items, but also on the partition-key design. This doesn't mean that you must access all partition key values to achieve an efficient throughput level, or even that the percentage of accessed partition key values must be high. It does mean that the more distinct partition key values tat your workload accesses, the more those requests will be spread across the partitioned space. In general, you will use your provisioned throughput more efficiently as the ratio of partition key values accessed to the total number of partition key values increases. One example for this is the use of partition keys with high-cardinality attributes, which have a large number of distinct values for each item.\n- **Scaling**\n\t- A relational database system does not scale well for the following reasons:\n\t\t- It normalizes data and stores it on multiple tables that require multiple queries to write to disk.\n\t\t- It generally incurs the performance costs of an ACID-compliant transaction system.\n\t\t- It uses expensive joins to reassemble required views of query results.\n\t- DynamoDB scales well due to these reasons:\n\t\t- It's schema flexibility lets DynamoDB stores complex hierarchical data within a single item. DynamoDB is not a totally *schemaless* database since the very definition of a schema is just the model or structure of your data.\n\t\t- Composite key design lets it store related items close together on the same table.\n- DynamoDB allows you to store session state data on DynamoDB\n- A **DynamoDB Stream** is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A *stream record* contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information such as the \"before\" and \"after\" images of modified items. DynamoDB is integrated with Lambda so that you can create *triggers* - pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables. If you enable Dynamo Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. The Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/dynamodb-accelerator":{"title":"DynamoDB Accelerator","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [DynamoDB](dynamodb.md)\n\n## **DAX - DynamoDB Accelerator**\n- DAX is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement - from milliseconds to microseconds - even at millions of requests per second.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/ebs":{"title":"EBS","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **EBS - Elastic Block Store**\n- EBS provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices. You can mount these volumes as devices on your instances. EBS volumes that are attached to an instance are exposed as storage volumes that persist independently from the life of the EC2 instances. EBS can be used as persistent storage. It is mainly used as the root volume to store the operating system of an EC2 instance.\n- All data moving between the volume and the instance are encrypted.\n- You can encrypt an EBS volume at rest, by using AWS KMS customer master keys for the encryption of both the boot and data volumes of an EC2 instance.\n- When you create an EBS volume in an AZ, it is automatically replicated within that zone to prevent data loss due to a failure of any single hardware component.\n- After you create a volume, you can attach it to any EC2 instance in the same AZ.\n- **EBS Multi-Attach** enables you to attach a single Provisioned IOPS SSD (io1) volume to multiple Nitro-based instances that are in the same AZ. However, other EBS types are not supported.\n- An EBS volume is off-instance storage that can persist independently from the life of an instance. You can specify not to terminate the EBS volume when you terminate the EC2 instance during instance creation by setting the `DeleteOnTermination` attribute of the EBS volumes to `False`.  By default EBS root device volumes are automatically deleted when the EC2 instance terminates.\n- An EBS volume can be used while a snapshot is in progress. An in-progress snapshot is not affected by ongoing reads and writes to the volume hence, you can still use the EBS volume normally.\n- EBS is an easy-to-use, high-performance block storage solution designed for use with EC2 for both throughput and transaction-intensive workloads at any scale. A broad range of workloads, such as relational and non-relational databases, enterprises applications, containerized applications, big data analytics engines, file systems, and media workflows are widely deployed on Amazon EBS.\n- When choosing an EBS type for your database, keep in mind that certain I/O characteristics drive the performance behavior for your EBS volumes. SSD-backed volumes, such as **General Purpose SSD (gp2)** and **Provisioned IOPS SSD (io1)**, deliver consistent performance whether an I/O operation is random or sequential. HDD-backed volumes like **Throughput Optimized HDD (st1)** (frequent access) and **Cold HDD (sc1)** (infrequent access) deliver optimal performance only when I/O operations are large and sequential.\n- SSD's are better for small, random I/O operations. SSD's can be used as a bootable volume. Suitable use cases are transactional workloads, critical business applications that require sustained IOPS performance, and large database workloads such as MongoDB, Oracle, Microsoft SQL Server, etc. Cost is high, and dominant performance attribute is IOPS.\n- HDD's are better for large, sequential operations. They can *not* be used as a bootable volume. Suitable use cases are large streaming workloads requiring consistent fast throughput at a low price, Big Data, Data warehouses, Log processing, Throughput-oriented storage for large volumes of data that is infrequently accessed. Cost is low, and dominant performance attribute is Throughput in (mib/s).\n- EBS volumes support live configuration changes while in production which means that you can modify the volume type, volume size, and IOPS capacity without service interruptions\n- EBS encryption uses 256-bit Advanced Encryption Standard algorithms (AES-256)\n- EBS Volumes offer 99.999% SLA.\n- EBS provides the *lowest* latency between EFS and S3 because it is attached directly to the EC2 instance that uses it. But it can only be attached to a single EC2 instance, unlike EFS, which can be accessed by multiple instances concurrently.\n- **IOPS**\n\t- Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers within 10 percent of the provisioned IOPS performance 99.9 percent of the time over a given year. (SSD - small, random I/O operations)\n\t- An io1 volume can range in size from 4 GiB to 16 TiB. You can provision from 100 IOPS up to 64,000 IOPS per volume on Nitro system instance families and up to 32,000 on other instance families. The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1.\n\t- For example, a 100 GiB volume can be provisioned with up to 5,000 IOPS. On a supported instance type, any volume 1,280 GiB in size or greater allows provisioning up to the 64,000 IOPS maximum (50 × 1,280 GiB = 64,000).\n\t- An io1 volume provisioned with up to 32,000 IOPS supports a maximum I/O size of 256 KiB and yields as much as 500 MiB/s of throughput. With the I/O size at the maximum, peak throughput is reached at 2,000 IOPS. A volume provisioned with more than 32,000 IOPS (up to the cap of 64,000 IOPS) supports a maximum I/O size of 16 KiB and yields as much as 1,000 MiB/s of throughput.\n\t- The volume queue length is the number of pending I/O requests for a device. Latency is the true end-to-end client time of an I/O operation, in other words, the time elapsed between sending an I/O to EBS and receiving an acknowledgment from EBS that the I/O read or write is complete. Queue length must be correctly calibrated with I/O size and latency to avoid creating bottlenecks either on the guest operating system or on the network link to EBS.\n\t- Optimal queue length varies for each workload, depending on your particular application’s sensitivity to IOPS and latency. If your workload is not delivering enough I/O requests to fully use the performance available to your EBS volume then your volume might not deliver the IOPS or throughput that you have provisioned.\n\t- Transaction-intensive applications are sensitive to increased I/O latency and are well-suited for SSD-backed io1 and gp2 volumes. You can maintain high IOPS while keeping latency down by maintaining a low queue length and a high number of IOPS available to the volume. Consistently driving more IOPS to a volume than it has available can cause increased I/O latency.\n\t- Throughput-intensive applications are less sensitive to increased I/O latency, and are well-suited for HDD-backed st1 and sc1 volumes. You can maintain high throughput to HDD-backed volumes by maintaining a high queue length when performing large, sequential I/O. (HDD - large, sequential I/O operations)\n\t- Therefore, for instance, a 10 GiB volume can be provisioned with up to 500 IOPS. Any volume 640 GiB in size or greater allows provisioning up to a maximum of 32,000 IOPS (50 × 640 GiB = 32,000).\n- EBS volumes attached to stopped EC2 instances still incur charges.\n\n### RAID Configurations\nWith Amazon EBS, you can use any of the standard RAID configurations that you can use with a traditional bare metal server, as long as that particular RAID configuration is supported by the operating system for your instance. This is because all RAID is accomplished at the software level.\n\nFor greater I/O performance than you can achieve with a single volume, RAID 0 can stripe multiple volumes together; for on-instance redundancy, RAID 1 can mirror two volumes together which can also offer fault tolerance.\n\nRAID 5 and RAID 6 are not recommended for Amazon EBS because the parity write operations of these RAID modes consume some of the IOPS available to your volumes. Depending on the configuration of your RAID array, these RAID modes provide 20-30% fewer usable IOPS than a RAID 0 configuration. Increased cost is a factor with these RAID modes as well; when using identical volume sizes and speeds, a 2-volume RAID 0 array can outperform a 4-volume RAID 6 array that costs twice as much.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/ec2":{"title":"EC2","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [EC2 Purchase Options](/notes/aws/ec2-purchase-options.md)\n- [EC2 Placement Groups](/notes/aws/ec2-placement-groups.md)\n\n## **EC2 - Elastic Compute Cloud**\n\n### Instance Types\n- **Instance Types**\n\t- C: Compute Optimized Instances\n\t\t- Cost-effective high performance at a low price per compute ratio\n\t\t- Note that `c1` type instance do not support IPv6\n\t- D: Storage Optimized Instances\n\t\t- High disk throughput\n\t- G: Accelerated Computing Instances\n\t\t- Graphics-intensive GPU instances\n\t- H: Storage Optimized Instances\n\t\t- HDD-based local storage for high disk throughput\n\t- I: Storage Optimized Instances\n\t\t- High storage instances, low latency, high random I/O performance, high sequential read throughput, and high IOPS\n\t- M: General Purpose Instances\n\t\t- Fixed performance\n\t\t- Note that `m3` type instance do not support IPv6\n\t- P: Accelerated Computing Instances\n\t\t- General purpose GPU instances\n\t- F: Accelerated Computing Instances\n\t\t-  Reconfigurable FPGA instances\n\t- R: Memory Optimized Instances\n\t\t- Memory-intensive applications\n\t- T: General Purpose Instances\n\t\t- Burstable performance instances\n\t- X: Memory Optimized Instances\n\t\t- Large-scale, enterprise-class, in-memory applications, and high-performance databases\n\n### EC2 Storage\n- Some EC2 instance types come with a form of directly attached, block-storage known as the instance store. The instance store is ideal for temporary storage, because the data stored in an instance store is not persistent through instance stops, terminations, or hardware failures. For data that you want to retain for longer, or if you want to encrypt the data, use EBS volumes instead. EBS volumes preserve their data through instance stops and terminations, can easily be backed up with EBS snapshots, can be removed from one instance and reattached to another, and support full-volume encryption.\n\n### EC2 Instance Lifecycle States\n- Below are the valid EC2 lifecycle instance states:\n\t- `pending`: The instance is preparing to enter the running state. An instance enters the pending state when it launches for the first time, or when it is restarted after being in the stopped state.\n\t- `running`: The instance is running and ready for use.\n\t- `stopping`: The instance is preparing to be stopped. Take note that you will not be billed if it is preparing to stop however, you will still be billed if it is just preparing to hibernate.\n\t- `stopped`: The instance is shut down and cannot be used. The instance can be restarted at any time.\n\t- `shutting-down`: The instance is preparing to be terminated.\n\t- `terminated`: The instance has been permanently deleted and cannot be restarted. Take note that Reserved Instances that applied to terminated instances are still billed until the end of their term according to their payment option.\n\n\n\n### Connecting to EC2 Instances\n- If connecting to an EC2 instance running windows, you'd use the Remote Desktop Protocol, or RDP. This runs on port 3389.\n- If connecting to an EC2 instance running linux, you'd use SSH. this runs on port 22.\n\n### General EC2 Notes\n- If you have the requirement that an EC2 instance can only be accessed from this IP: 110.238.98.71 via ssh, your Security Group Inbound Rule would have the following configuration: Protocol - TCP, Port Range - 22, Source 110.238.98.81/32 (NOTE: The /32 denotes one IP address, if you were to use /0, it would refer to the entire network. Take note that the SSH protocol uses TCP and port 22)\n- Instance metadata is the data about your instance that you can use to configure or manage the running instance. You can get the instance ID, public keys, public IP address and many other information from the instance metadata by firing a URL command in your instance to this URL: http://169.254.169.254/latest/meta-data\n- By default, a new EC2 instance uses an IPv4 addressing protocol.\n- By default, Amazon EC2 sends metric data to CloudWatch in 5-minute periods. To send metric data for your instance to CloudWatch in 1-minute periods, you can enable detailed monitoring on the instance.\n- A FinTech startup deployed an application on an Amazon EC2 instance with attached Instance Store volumes and an Elastic IP address. The server is only accessed from 8 AM to 6 PM and can be stopped from 6 PM to 8 AM for cost efficiency using Lambda with the script that automates this based on tags. What occurs when the EC2 instance is stopped?\n\t- All data on the attached instance store devices will be lost.\n\t- The underlying host for the instance is possibly changed\n\t- **What doesn't change?**\n\t\t- The ENI (Elastic Network Interface) is not detached\n\t\t- If you stopped an EBS-backed EC2 instance, the volume is preserved but the data in any attached instance store volume will be erased. Keep in mind that an EC2 instance has an underlying physical host computer. If the instance is stopped, AWS usually moves the instance to a new host computer. Your instance may stay on the same host computer if there are no problems with the host computer. In addition, its Elastic IP address is disassociated from the instance if it is an EC2-Classic instance. Otherwise, if it is an EC2-VPC instance, the Elastic IP address remains associated.\n- **Elastic Fabric Adapter** is a network device that you can attach to your EC2 instance to accelerate High Performance Computing (HPC) and machine learning applications. EFA enables you to achieve the application performance of an on-premises HPC cluster, with the scalability, flexibility, and elasticity provided by AWS. (Not supported on Windows instances)\n\n\n- The **Reserved Instance Marketplace** is a platform that supports the sale of third-party and AWS customers' unused Standard Reserved Instance, which vary in terms of lengths and pricing options. For example, you may want to sell Reserved Instances after moving instances to a new AWS region, changing to a new instance type, ending projects before the term expiration, when your business needs change, or if you have unneeded capacity.\n- You can use *placement groups* to influence the placement of a group of *interdependent* instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:\n\t- Cluster - packs instances close together inside an AZ. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.\n\t- Partition - spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.\n\t- Spread - strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.\n\t- When working with instance store volumes and temporary data, RAID-0 is more efficient than RAID-1. RAID-0 configuration enables you to improve your storage volumes' performance by distributing the I/O across the volumes in a stripe. Therefore, if you add a storage volume, you get the straight addition of throughput and IOPS. This configuration can be implemented on both EBS or instance store volumes. Since the main requirement in the scenario is storage performance, you need to use an instance store volume. It uses NVMe or SATA-based SSD to deliver high random I/O performance. This type of storage is a good option when you need storage with very low latency, and you don't need the data to persist when the instance terminates. RAID-1 is used for data mirroring and fault tolerance.\n- A *bastion host* is a special purpose computer on a network specifically designed and configured to withstand attacks. If you have a bastion host in AWS, it is basically just an EC2 instance. It should be in a public subnet with either a public or Elastic IP address with sufficient RDP or SSH access defined in the security group. Users log on to the bastion host via SSH or RDP and then use that session to manage other hosts in the private subnets.\n\n### Amazon Machine Images (AMI)\n- The Images of EC2. AMI's can be used to launch EC2 instances, they're used by the console UI itself, when you launch an EC2 instance. These are AWS or community provided, but you can create custom AMIs.\n- Contains the Boot Volume of an EC2 instance\n\t- The C volume in Windows\n\t- The root volume in linux\n- Contains a Block Device Mapping, a configuration which specifies which volume is the boot volume and which volume(s) are the data volumes\n- Marketplace AMIs can include commercial software, which can cost extra, due to licenses for commercial softwaer\n- Each region has a unique ID in the following format: `ami-0a887e401f7654935`\n- Permissions Options (Who can use an AMI?)\n\t- Public\n\t- Your Account\n\t- Specific Accounts\n- You can also create an AMI from an EC2 instance that you want to template\n- AMI = One Region, only works in that one region, but can be used in AZs all over that region\n- AMI Baking = creating an AMI from a configured instance + application\n- An AMI cannot be edited, you must launch an instance, update configuration and make a new AMI\n- Can be copied between regions (includes its snapshots)\n- Remember permissions (default = your account)\n- AMI Billing: AMIs contain EBS Snapshots, so you are billed for this storage capacity\n\n#### Lifecycle of an AMI\n1. Launch: Use an AMI to launch an EC2 instance, and/or add an EBS volume\n2. Configure: Take instance and attached EBS volumes, and apply customizations, like an OS that is heavily configured with an application\n3. Create Image: Take previously configured instance to produce an AMI\n4. Launch: The new instance will have new EBS volumes that are perfect copies of the original EBS volume snapshots from S3. They will have exactly the same data.\n\n## Scenarios\nA DevOps Engineer reported a problem accessing his EC2 instance with a private IP address of 172.31.8.11 from his corporate laptop. The EC2 instance is hosting a web application which works well but he is still experiencing an issue establishing a connection to manage the instance. As the SysOps Administrator, which of the following options is the most suitable solution in this scenario based on the VPC flow log entries below?\n```\n2 123456789010 eni-abc123de 110.217.100.70 172.31.8.11 49761 3389 6 20 4249 1418530010 1418530070 REJECT OK\n```\nAnswer: Based on the VPC flow log record provided, the RDP traffic (destination port 3389) to network interface `eni-abc123de` in the AWS account `123456789010` was rejected. The RDP connection request came from the DevOps engineer's laptop (with an IP address of `110.217.100.70`) and it is trying to access the EC2 instance with a private IP address of `172.31.8.11`.\n\nAlthough the scenario did not explicitly say what type of remote connection protocol the DevOps engineer used, it is quite clear in the VPC flow logs that the user is using Remove Desktop Protocol (RDP). The root cause of this issue is because the security group and the Network ACL of the EC2 instance do not allow RDP traffic. To solve this issue, you would simply have to configure the security group of the EC2 instance to allow incoming RDP traffic including the inbound and outbound rules in the Network ACL.\n\nA VPC flow log record is a space-separated string that has the following format:\n```\n\u003cversion\u003e \u003caccount-id\u003e \u003cinterface-id\u003e \u003csrcaddr\u003e \u003cdstaddr\u003e \u003csrcport\u003e \u003cdstport\u003e \u003cprotocol\u003e \u003cpackets\u003e \u003cbytes\u003e \u003cstart\u003e \u003cend\u003e \u003caction\u003e \u003clog-status\u003e\n```\n\n#aws #aws-compute ","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/ec2-placement-groups":{"title":"EC2 Placement Groups","content":"\n### Related Notes\n- [EC2](/notes/aws/ec2.md)\n\n## EC2 Placement Group\n- Allows you to influence physical placement of EC2 instances within AWS\n\n### Cluster Placement Groups (Performance)\n- Generally, instances in this placement group will use the same rack, and sometime even the same host.\n- Maximizes performance, all members have direct connections to each other.\n- 10 Gbps per stream compared to the 5 Gbps which is achievable normally\n- Lowest latency and max PPS possible in AWS\n- Offers little to no resilience\n- Can't span AZs - ONE AZ ONLY - locked when launching first instance\n- Can span VPC peers - but impacts performance\n- Requires a supported instance type\n- Use the same type of instance (not mandatory)\n- Launch at the same time (not mandatory ... very recommended)\n- Use case: Performance, fast speeds, low latency\n\n### Spread Placement Groups (Resilience)\n- Keep instances separated\n- Ensure maximum amount of availability and resiliency for an application\n- Located on isolated, distinct racks per AZ\n\t- Max of 7 instance per AZ, due to distinct rack placement\n- Provides infrastructure isolation\n- Each instance runs from a different rack\n- Each rack has its own network and power source\n- Not supported for Dedicated Instances or Hosts\n- Use Case: Small number of critical instances that need to be kept separated from each other\n\n### Partition Placement Groups (Topology Awareness)\n- Groups of instances, spread apart\n- Similar to spread placement groups, but for situations where you have more than 7 instances per AZ. There is no instance limit per AZ.\n- Divided into partitions, max 7 partitions per AZ\n- Each partition has its own racks, no sharing between partitions\n- Designed for huge scale parallel processing systems\n- You can control which instances are placed in which partitions - Topology Aware\n- Instances can be placed in a specific partition or auto placed\n- HDFS, HBase, and Cassandra","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/ec2-purchase-options":{"title":"EC2 Purchase Options","content":"\n### Related Notes\n- [EC2](/notes/aws/ec2.md)\n\n## EC2 Purchase Options (Launch Types)\n\n### On Demand\n- Default purchase option\n- Isolated instances, but multiple customer instances run on the same shared hardware\n- Instances of different sizes run on the same EC2 hosts - consuming a defined allocation of resources\n- Uses per-second billing while an instance is running. Associated resources such as storage consume capacity, so bill, regardless of instance state.\n- No interruption\n- No capacity reservation\n- Predictable pricing\n- No upfront cost\n- No discounts\n- Good for\n\t- Short term workloads\n\t- Unknown workloads\n\t- Apps which can't be interrupted\n\n### Spot\n- Cheapest way to access EC2 capacity\n- Spot pricing is AWS selling unused EC2 host capacity for up to 90% discount - the spot price is based on the spare capacity at a given time\n- Non time critical\n- Anything which can be rerun\n- Bursty capacity needs\n- Cost sensitive workloads\n- Anything which is stateless\n\n### Standard Reserved\n- Highest priority compute option\n- Long term, consistent usage of EC2\n- Reduces or removes per second price for that instance\n- Unused reservations are still billed\n- Reservations are for 1 year or 3 year terms\n\t- If you pay nothing upfront, you save money for agreeing to the term, but are still charged a per second fee.\n\t- If you pay all upfront, you gain the benefit of no per second fee. This is the cheapest option.\n\t- You can also pay partial upfront, for a reduced per second fee.\n- With reserved instances, you won't have interruption\n\n### Scheduled Reserved Instances\n- Scheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. You reserve the capacity in advance, so that you know it is available when you need it. You pay for the time that the instances are scheduled, even if you do not use them.\n- Ideal for long term usage which doesn't run constantly\n- Example: Batch Processing daily for 5 hours starting at 23:00\n- Example: Weekly data, sales analysis .. every Friday for 24 hours\n- Doesn't support all instance types or regions. 1,200 hours per year \u0026 1 year term minimums\n\n### Convertible Reserved Instances\nConvertible Reserved instances allow you to exchange for another Convertible Reserved instance with a different instance type and tenancy.\n\n### Dedicated Hosts\n- No instance charges\n- Pay for host\n- Host affinity links instances to hosts\n- Generally used when you have strict licensing requirements, based on Sockets/Cores\n\n### Dedicated Instances\n- Your Instances run on an EC2 host with other instances of yours, but no other customers can have access to that hardware.\n- You don't own, or share the host\n- You have extra charges for instances, but dedicated hardware\n- One-off hourly fee for any regions where you use Dedicated Instances\n- Used when you're required not to share hardware, but you don't want to manage the host itself.\n\n### Capacity Reservations\n- Different from Reserved Instance Purchases\n- Regional Reservation provides a billing discount for valid instances launched in any AZ in that region.\n\t- While this is flexible, it doesn't reserve capacity within an AZ - which is risky during major faults when capacity is limited.\n- Zonal Reservations only apply to one AZ providing billing discounts and capacity reservation in that AZ\n\t- Full Price \u0026 No Capacity Reservation if you launch in a different AZ\n- Both Regional and Zonal Reservations require a 1 or 3 year commitment to AWS.\n- On-Demand Capacity Reservations can be booked to ensure you always have access to capacity in an AZ when you need it - but at full on-demand price. No term limits - but you pay regardless of if you consume it or not.\n\t- No cost reductions","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/ecs":{"title":"ECS","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [Fargate](/notes/aws/fargate.md)\n\n## #ecs- Elastic Container Service\n\n### Avoid ECS Downtime\n- [StackOverflow: Restart ECS service with no downtime](https://stackoverflow.com/questions/42735328/aws-ecs-restart-service-with-the-same-task-definition-and-image-with-no-downtime)\nTo avoid downtime, you should manipulate 2 parameters: _minimum healthy percent_ and _maximum percent_:\n\u003e For example, if your service has a desired number of four tasks and a maximum percent value of 200%, the scheduler may start four new tasks before stopping the four older tasks (provided that the cluster resources required to do this are available). The default value for maximum percent is 200%.\n\nRegardless of whether your task definition changed and to what extent, there can be an \"overlap\" between the old and the new tasks, and this is the way to achieve resilience and reliability.\n```\naws ecs update-service --force-new-deployment --service my-service --cluster cluster-name\n```\n\n---\n\n### Secret Injection\n- Amazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in either AWS [[secrets-manager]] or AWS [[systems-manager]]. Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.\n\n---\n\n### FireLens\n- [FireLens](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_firelens.html) - Custom Log Routing\n- You can use FireLens for Amazon ECS to use task definition parameters to route logs to an AWS service or AWS Partner Network (APN) destination for log storage and analytics. FireLens works with Fluentd and Fluent Bit. We provide the AWS for Fluent Bit image or you can use your own Fluentd or Fluent Bit image.\n\nCreating Amazon ECS task definitions with a FireLens configuration is supported using the AWS SDKs, AWS CLI, and AWS Management Console.\n\n---\n\n### ECS Exec\n- [ESC Exec](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html) to run commands in or get a shell to a container running on an Amazon EC2 instance or on AWS Fargate. This makes it easier to collect diagnostic information and quickly troubleshoot errors. For example, in a development context, you can use ECS Exec to easily interact with various process in your containers and troubleshoot your applications. And, in production scenarios, you can use it to gain break-glass access to your containers to debug issues.\n\n### Misc.\n- [ECS Troubleshooting Page](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/troubleshooting.html)\n- [ecs vs fargate](https://cloudonaut.io/ecs-vs-fargate-whats-the-difference/)","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/efs":{"title":"EFS - Elastic File System","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [EC2](ec2.md)\n\n## **EFS - Elastic File System**\n- Moves EC2 to be closer to being stateless\n- Implementation of NFSv4\n- EFS Filesystems can be mounted in Linux only\n\t- Uses POSIX permissions, a standard understood by all linux systems\n- Shared between many EC2 Instances\n- Private service, via mount targets inside a VPC\n\t- Mount targets must be present in each AZ that a VPC uses to provide high availability\n\t- Mount targets are what the EC2 instances use to connect to the EFS\n- Can be accessed from on-premises via VPN or DX\n- General Purpose and Max I/O performance mode\n\t- Max I/O is good for highly parallel services\n- Bursting and Provisioned Throughput Modes\n- Storage Classes\n\t- Standard (default)\n\t- Infrequent Access (lower cost)\n\t- Lifecycle Policies can be used with classes\n- Amazon EFS is a fully-managed service that makes it easy to set up and scale file storage in the Amazon Cloud. With a few clicks in the AWS Management Console, you can create file systems that are accessible to Amazon EC2 instances via a file system interface (using standard operating system file I/O APIs) and supports full file system access semantics (such as strong consistency and file locking).\n- Amazon EFS file systems can automatically scale from gigabytes to petabytes of data without needing to provision storage. Tens, hundreds, or even thousands of Amazon EC2 instances can access an Amazon EFS file system at the same time, and Amazon EFS provides consistent performance to each Amazon EC2 instance. Amazon EFS is designed to be highly durable and highly available.\n- Provides simple, scalable, elastic file storage for use with AWS Cloud services and on-premises resources. When mounted on EC2 instances, an EFS file system provides a standard file interface and file system access semantics, allowing you to seamlessly integrate EFS with your existing applications and tools. Multiple EC2 instances can access an EFS file system at the same time, allowing EFS to provide a common data source for workloads and applications running on more than one EC2 instance.\n- When to choose EFS over EBS?\n\t- When the scenario requires concurrently-accessible storage (think multiple EC2 instances accessing the same storage system). In this scenario, EFS is better than EBS. But EBS provides lower latency. But EBS volumes can only be attached to one instance at a time. EFS can be attached to multiple.\n- Note of Caution: You cannot, say, modify an existing EFS file system configuration and activate Max I/O performance mode. This is because you cannot change the performance mode configuration of an EFS file system right away. You need to migrate the data to another file system configured with your desired performance mode. [DataSync](/notes/aws/datasync.md) can facilitate this.\n\n### EFS Encryption\n- An existing EFS cannot be encrypted. You must create a new EFS with encryption enabled, and copy over the data from the existing EFS.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/egress-only-internet-gateway":{"title":"Egress-Only Internet Gateway","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [NAT Gateway](/notes/aws/nat-gateway.md)\n\n### Egress-Only Internet Gateway\n- Egress-Only Internet Gateway is an **outbound-only** Internet Gateway that allows private IPv6 resources in your VPC's private subnets to initiate a connection to the public internet and public AWS services _without_ allowing the Internet to initiate a connection (egress) to your resources.\n- Why is this necessary? A [NAT Gateway](/notes/aws/nat-gateway.md) already allows resources with a private IPv4 address to access public networks or public AWS services, without allowing externally initiated connections. BUT, if you use a NAT Gateway with an IPv6 address, all IPs are allowed IN and OUT.\n- This is due to all IPv6 addresses being public, whereas IPv4 addresses can be public or private.\n- Architecture is exactly the same as a normal Internet Gateway\n- Egress-Only Internet Gateway's are High Availability by default across all AZs in the region - scales as required.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/eks":{"title":"EKS","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **EKS - Elastic Kubernetes Service**\n- Amazon EKS provisions and scales the Kubernetes control plane, including the API servers and backend persistence layer, across multiple AWS availability zones for high availability and fault tolerance. Amazon EKS automatically detects and replaces unhealthy control plane nodes and provides patching for the control plane. Amazon EKS is integrated with many AWS services to provide scalability and security for your applications. These services include Elastic Load Balancing for load distribution, IAM for authentication, Amazon VPC for isolation, and AWS CloudTrail for logging.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/elastic-beanstalk":{"title":"","content":"Parent Wiki: [[aws-wiki]]\n\nElastic Beanstalk’s main benefits include timesaving server configuration, powerful customization, and a cost-effective price point.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/elastic-map-reduce":{"title":"Elastic Map Reduce","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **EMR - Amazon Elastic Map Reduce**\n- Amazon EMR is a manged cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data. By using these frameworks and related open-source projects, such as Apache Hive and Apache Pig, you can process data for analytics purposes and business intelligence workloads. Additionally, you can use Amazon EMR to transform and move large amounts of data into and out of other AWS data stores and databases.\n- Amazon Redshift is the most widely used cloud data warehouse. It makes it fast, simple and cost-effective to analyze all your data using standard SQL and your existing Business Intelligence (BI) tools. It allows you to run complex analytic queries against terabytes to petabytes of structured and semi-structured data, using sophisticated query optimization, columnar storage on high-performance storage, and massively parallel query execution.\n- The key phrases in the scenario are \"big data processing frameworks\" and \"various business intelligence tools and standard SQL queries\" to analyze the data. To leverage big data processing frameworks, you need to use Amazon EMR. The cluster will perform data transformations (ETL) and load the processed data into Amazon Redshift for analytic and business intelligence applications.\n- Amazon EMR provides a managed Hadoop framework that makes it easy, fast and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. It securely and reliably handles a broad set of big data use cases, including log analysis, web indexing, data transformations (ETL), machine learning, financial analysis, scientific simulation, and bioinformatics. You can also run other popular distributed frameworks such as Apache Spark, HBase, Presto, and Flink in Amazon EMR, and interact with data in other AWS data stores such as Amazon S3 and Amazon DynamoDB.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/elasticache":{"title":"Elasticache","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Elasticache**\n- Amazon Elasticache improves the performance of your database through caching query results.\n- The purpose of an in-memory key-value store is to provide ultra-fast (submillisecond latency) and inexpensive access to copies of data. Most data stores have areas of data that are frequently accessed but seldom updated. Additionally, querying a database is always slower and more expensive than locating a key in a key-value pair cache. Some database queries are especially expensive to perform, for example, queries that involve joins across multiple tables or queries with intensive calculations.\n- By caching such query results, you pay the price of the query once and then are able to quickly retrieve the data multiple times without having to re-execute the query.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/fargate":{"title":"Fargate","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Fargate**\n- AWS Fargate is a general purpose container compute platform. You can run a persistent container that stays up for weeks, and binds to an an IP address to receive HTTP requests and serve web pages, or supply a REST API. You can also use it like AWS Batch though, by using the RunTask API to launch a containerized task that runs to completion and exits. Fargate is general purpose and you can do more things with it.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/fsx-for-lustre":{"title":"FSx for Lustre","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **FSx for Lustre**\n- Amazon FSx for Lustre is a high-performance file system for fast processing of workloads. Lustre is a popular open-source **parallel file system** which stores data across multiple network file servers to maximize performance and reduce bottlenecks.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/fsx-for-windows-file-server":{"title":"FSx for Windows File Server","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **FSx for Windows File Server**\n- Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotes, end-user file restore, and Microsoft AD integration. Amazon FSx is accessible from Windows, Linux, and MacOS compute instances and devices. Thousands of compute instances and devices can access a file system concurrently.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/global-accelerator":{"title":"Global Accelerator","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [CloudFront](/notes/aws/cloudfront.md)\n\n## **Global Accelerator**\n- Similar to CloudFront, in that they both improve performance, but they each do so in different ways, for different reasons.\n- AWS Global Accelerator is a networking service that improves the performance of your users' traffic by up to 60% using AWS' global network infrastructure. When the internet is congested, AWS Global Accelerator optimizes the path to your application to keep packet loss, jitter and latency consistency low.\n- With Global Accelerator, you are provided 2 global static public Anycast IP addresses that act as a fixed entry point to your application, improving availability. On the back end, add or remove your AWS application endpoints, such as Application Load Balancers, Network Load Balancers, EC2 instances, and Elastic IP's without making user-facing changes. Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure.\n\t- Anycast IP addresses allow a single IP to be in multiple locations. Routing moves traffic to the closest Edge Locations.\n- Customer traffic initially traverses the public internet, but only until it reaches the closest Edge Location. At that point, AWS Global Accelerator takes users' traffic off of the public internet, and onto Amazon's private global network through 90+ global edge locations, which are then directed to your application origins.\n\t- Traversing the AWS global network yields significantly better performance.\n- You can improve the latency and availability of single region applications, allow for simplified and resilient routing for multi-Region applications, as well as accelerated and simplified storage for multi-Region applications.\n\n### Differences From CloudFront\n- Global Accelerator moves the AWS network itself closer to customers, while CloudFront specifically moves the content closer by caching it at the Edge Locations.\n- Connections enter at edge, using Anycast IPs\n- Transit over AWS backbone to 1+ locations\n- Global Accelerator is a network product, can can be used for NON HTTP/S (TCP/UDP) - This is a difference from CloudFront\n- Global Accelerator doesn't cache anything, and it doesn't understand the protocol for HTTP/S","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/iam":{"title":"IAM","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Identity Access Management**\n\n### IAM Identity Policy Document\n- List of statements that grant or deny permissions to AWS services\n\t- SID: Optional, but best practice to use\n\t- Effect\n\t- Action\n\t- Resource\n- The identity needs to prove to AWS who it is (authentication)\n- Explicit allows take effect, unless there is also an explicit deny\n- Explicit deny statements overrule everything, nothing can overrule it\n\t- A deny statement on an IAM policy always takes precedence over an allow statement. For example, if your policy allows PUT and DELETE actions to a DynamoDB table, but your policy also denies all DynamoDB actions, the policy will not allow any actions on your AWS accounts DynamoDB tables.\n- Default is a implicit deny (If you're not allowed, you're denied)\n\n### Managed Policies\n- Reusable\n- Low management overhead\n- Good for granting access across many people that will be using AWS in your organization\n- AWS Managed Policies can be used, as well as Custom Managed Policies\n\n### IAM Users\n- IAM Users are an identity used for anything requiring long term AWS access e.g. Humans, Applications or service accounts\n- Principal = An unidentified entity, not yet authenticated or authorized, attempting to access an AWS account\n- Principals make requests to IAM in order to be authenticated\n- Principal claims to be Sally, and proves by authenticating using a Username/Password (Console UI) or Access Key (application/cli)\n- Authenticated Identity is an entity that has proved it is authenticated\n- Once an authenticated Identity tries to do something like terminate an EC2 instance, or upload to an S3 bucket, AWS checks that that Identity is authorized to perform that action, and that is the process of authorization\n\n#### IAM User Limits\n- 5,000 IAM Users per account\n- IAM Users can only be a member of 10 groups\n- This has systems design impacts...\n- Internet-scale applications, large orgs \u0026 org merges\n- IAM Roles \u0026 Identity Federation fix this\n\n### IAM Groups\n- IAM Groups are containers for IAM Users\n- You cannot log into a group\n- Used solely for organization of IAM Users\n- There is no \"All Users\" group in IAM, unless you create one and manage it yourself\n- No nested groups\n- Groups are not a true identity. They can't be referenced as a principal in a policy\n- Groups offer less functionality than you would think, keep this in mind\n\n### IAM Roles\n- A role is a type of identity that exists in AWS\n- If you cannot identify the number of principals that use an IAM User, it could be a good candidate for a role\n\t- One example is a `company-microservice-dev` role, which is used by multiple IAM Users (in the form of software engineers, that want to use that role to access resources related to that particular service, in a development account)\n- Roles are generally used on a temporary basis, to use the role's permissions to perform some actions, and then stop using that role\n- IAM users can have identity permissions policies attached to them via inline JSON or managed policies\n- IAM roles have 2 types of policies that can be attached\n\t- Trust Policy - Controls which identities can assume the role\n\t- Permissions Policy - Controls which permissions the role has authorization to perform\n- If a role is assumed by something which is allowed to assume it, AWS generates a temporary set of security credentials using the AWS STS (Secure Token Service), which are time limited\n- Roles can be referenced within resource policies\n\nWhen might you use roles?\n- AWS Lambda (Lambda assumes Lambda Execution Role at invocation time to read/write to an S3 bucket)\n- Single Sign-on or \u003e5000 Identities\n- External accounts can't be used in AWS Directly\n- Designing an architecture for a popular mobile application with millions of users\n\t- Because of the 5000 user limit, your users would need a role to interact\n\t\t- This is good because there are no AWS credentials on the app\n\t\t\t- Scales to 100,000,000's of accounts\n- Cross account permissions\n\n### Service Linked Roles\n- An IAM Role linked to a specific AWS service\n- Provides a set of permissions, which are predefined by a service\n- Provides permissions that a service needs to interact with other AWS services on your behalf\n- Service might create/delete the role or allow you to create the role during the setup or within IAM\n- Key Difference: You cannot delete a service linked role until it's no longer required\n- To create a role, you need `iam:CreateServiceLinkedRole` attached to an AWS service role\n- You also need `iam:AttachRolePolicy` and `iam:PutRolePolicy`\n\n### SAML2.0 Identity Federation\n- Identity Federation is the process of using an identity from another identity provider to access AWS resources.\n- In AWS though, this can't be direct. Only AWS credentials can be used to access AWS resources.\n- So, some form of exchange is required.\n- SAML 2.0 stands for Security Assertion Markup Language (V2 of the standard)\n- Open standard used by many ldP's (e.g. MS ADFS)\n- Indirectly use on-premises IDs with AWS (Console \u0026 CLI)\n- SAML is used, when you currently use an Enterprise Identity Provider that is also SAML 2.0 Compatible\n- You wouldn't use it with Google, Facebook, Twitter\n- You would use it if you have an existing identity management team\n- You would use it if you require a single source of truth with more than 5,000 users\n- Uses IAM Roles \u0026 AWS Temporary Credentials (12 hour validity)\n- YOU CANNOT USE EXTERNAL IDENTITIES TO DIRECTLY ACCESS AWS. There is always an exchange, and you get temporary role based credentials.\n\n#### API/CLI Credential Process\n- The Identity Provider (e.g ADFS) establishes a two way trust with IAM\n\t- AWS registered in IDP, SAML IdP created in IAM - two way trust\n\t- AWS Account is configured to allow SAML2.0 based federation\n- SAML communicates with IAM to fetch credentials by using the `sts:AssumeRoleWithSAML` permission\n- IAM Role Assumed and Temporary Security Credentials are returned\n- Now the on-premises application can access, say, a DynamoDB using said credentials\n\n#### Console Credential Process\n- Mostly the same, but now it's a user who wants to access the API console\n- There is still a trust configured, but this time it is between the Identity Provider (e.g ADFS) and the SAML/SSO endpoint in AWS Account\n- Temporary security credentials are created, and a temporary set of sign on credentials are created, as well as a console sign-in URL with credentials\n- SAML endpoint constructs this sign-in URL, and gives it to the user/client\n\n### ARNs\n- Uniquely identify resources within any AWS accounts\n- `arn:partition:service:region:account-id:resource-type/resource-id`\n\n## #aws-sysops Scenarios\n---\n**Question**: A SysOps Administrator needs to grant a user the ability to pass any of the approved set of roles to the Amazon EC2 service upon launching an instance. This will enable the user to start an EC2 instance with an assigned role. In effect, the applications running on the instance can access temporary credentials for the role through the instance profile metadata. What must the Administrator do to accomplish this requirement?\n**Answer**: To configure many AWS services, you must pass an IAM role to the service. This allows the service to later assume the role and perform actions on your behalf. You only have to pass the role to the service once during setup, and not every time the service assumes the role. For example, assume that you have an application running on an Amazon EC2 instance. That application requires temporary credentials for authentication, and permissions to authorize the application to perform actions in AWS. When you set up the application you must pass a role to EC2 to use with the instance that provides those credentials. You define the permissions for the applications running on the instance by attaching an IAM policy to the role. The application assumes the role every time it needs to perform the actions that are allowed by the role.\nTo pass a role (and it's permissions) to an AWS service, a user must have permissions to pass the role to the service. This helps administrators ensure that only approved users can configure a service with a role that grants permissions. To allow a user to pass a role to an AWS service, you must grant the `PassRole` permission to the user's IAM user, role, or group.\nIf you want to grant a user the ability to pass any of an approved set of roles to the Amazon EC2 service upon launching an instance, you need these three elements:\n1. An IAM permissions policy attached to the role determines what the role can do. You should scope permissions to only the actions that the role must perform, and to only the resources that the role needs for those actions.\n2. A trust policy for the role that allows the service to assume the role. You could attach a trust policy to the role with the `UpdateAssumeRolePolicy` action. With a trust policy, it allows Amazon EC2 to use the role and the permissions attached to the role.\n3. Another IAM permissions policy which is attached to the IAM user allows the user to pass only those roles that are approved. The `iam:PassRole` permission usually is accompanied by `iam:GetRole` permission so that the user can get the details of the role to be passed.\n\n---\n**Question**: An administrator has launched new AWS accounts. Management wants that IAM users across all accounts be able to sign in using a single login URL as shown below:\n\n`https://connerv.signin.aws.amazon.com/console`\n\nHow can the administrator meet the requirement?\n\n**Answer**: Having a single login URL for different AWS accounts is not possible.\n\nThe AWS account root user and AWS Identity and Access Management (IAM) users in the account sign in using a web URL. The sign-in page URL for your account's IAM users has the following format, by default: `https://123456789123.signin.aws.amazon.com/console/`\n\nIf you create an AWS account alias for your AWS account ID, the IAM user sign-in page URL looks like the following example: `https://connerv.signin.aws.amazon.com/console`\n\nYour AWS account can only have one alias. If you create a new alias for your AWS account, the new alias overwrites the previous alias. The URL containing the previous alias stops working. Also, the account alias must be unique across all AWS products and must contain only lowercase letters, digits, and hyphens.","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/iam-identity-center":{"title":"IAM Identity Center","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [IAM](/notes/aws/iam.md)\n\n## IAM Identity Center (Formerly AWS SSO)\n- Recommended solution for any workforce style identity federation requirements\n- Manage SSO Access - AWS Accounts and External Applications\n- Flexible Identity Source - SSO Integrates with a range of workplace Identity Sources ranging from built-in SSO identities through to self-managed on-premises Active Directory\n- Preferred by AWS vs traditional workforce identity federation (SAML2.0 is mostly only around to support legacy clients that haven't migrated)\n- SSO extends past AWS, and also delivers SSO solutions for Slack, Dropbox, Office 365, and Salesforce\n- This means that is handles SSO \u0026 permissions for both AWS accounts and external applications for a business\n- This product is completely free\n- Note that this applies to enterprise or workplace identities, not customer identities (web applications using Twitter, Google, Facebook, or any other web identity). For customer identities, a tool like AWS Cognito is the best fit.\n\n### Supported Identity Stores\n- AWS SSO itself - Built-in identity store\n- AWS Managed Microsoft AD\n- On-premises Microsoft AD (Two way trust or AD Connector)\n- External Identity Provider - SAML2.0","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/internet-gateway":{"title":"Internet Gateway","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [VPC](/notes/aws/vpc.md)\n- [VPC Router](/notes/aws/vpc-router.md)\n\n## Internet Gateway\n- Region resilient gateway attached to a VPC\n- You do not need a gateway per availability zone\n- 1 VPC = 0 or 1 IGW, 1 IGW = 0 or 1 VPC\n- Runs from within the AWS Public Zone\n- Gateways traffic between the VPC and the Internet or AWS Public Zone (S3, SQS, SNS, etc.)\n- Managed - AWS handles performance (it simply works)\n\n### Creation of a Public Subnet\n1. Create IGW\n2. Attache IGW to VPC\n3. Create custom RT\n4. Associate RT\n5. Default Routes =\u003e IGW\n6. Subnet allocate IPv4","lastmodified":"2022-11-06T05:20:38.306382416Z","tags":null},"/notes/aws/iot-core":{"title":"IoT Core","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **IoT Core**\n- AWS IoT Core is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core provides secure communication and data processing across different kinds of connected devices and locations so you can easily build IoT applications. AWS IoT Core allows you to connect multiple devices to the cloud and to other devices without requiring you to deploy or manage any servers. You can also filter, transform, and act upon device data on the fly based on the rules you define. With AWS IoT Core, your applications can keep track of and communicate with all of your devices, all the time, even when they aren't connected.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/kinesis":{"title":"Kinesis","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Kinesis Data Streams**\n- Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. You can use an AWS Lambda function to process records in Amazon KDS. By default, Lambda invokes your function as soon as records are available in the stream. Lambda can process up to 10 batches in each shard simultaneously. If you increase the number of concurrent batches per shard, Lambda still ensures in-order processing at the partition-key level.\n- Since Kinesis Data Streams is a real-time data streaming service that requires the provisioning of shards, if there is no requirement for real-time processing in your scenario, replacing Kinesis Data Streams with something like Amazon SQS, a cheaper option, is advantageous because you only pay for what you use.\n- By default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream.\n- In Amazon Kinesis Data Streams, consumers (such as a custom application running on EC2, or a Kinesis Data Firehose delivery stream) can store their results using an AWS service such as DynamoDB, Redshift, or Amazon S3) can store their results using an AWS service such as DynamoDB, Redshift, or Amazon S3.\n## **Kinesis Data Firehose**\n- Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytic tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytic with existing business intelligence tools and dashboards you are already using today.\n- It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.\n- You can gather data from anywhere, and use Kinesis Data Firehose to prepare and load the data. S3 will be used as a method of durably storing the data for analytic and the eventual ingestion of data for output using analytical tools.\n- You can use Amazon Kinesis Data Firehose in conjunction with Amazon Kinesis Data Streams if you need to implement real-time processing of streaming big data. Kinesis Data Streams provides an ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis Data Stream (for example, to perform counting, aggregation, and filtering).\n- Amazon SQS is different from Amazon Kinesis Data Firehose. SQS offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are protected independently (with message-level ack/fail semantics), such as automated workflows. Amazon Kinesis Data Firehose is primarily used to load streaming data into data stores and analytic tools.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/lambda":{"title":"AWS Lambda","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Lambda**\n- AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. You can run code for virtually any type of application or backend service - all with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can setup your code to automatically trigger from other AWS services or call it directly from any web or mobile app.\n\t- A nice feature about Lambda, it handle multiple requests. All of the docs specifically mention loading static resources/db connections/etc outside the response handler so they'll be reused between requests.\n- The first time you invoke your function, AWS Lambda creates an instance of the function and runs its handler method to process the event. When the function returns a response, it stays active and waits to process additional events. If you invoke the function again while the first event is being processed, Lambda initializes another instance, and the function processes the two events concurrently. As more events come in, Lambda routes them to available instances and creates new instances as needed. When the number of requests decreases, Lambda stops unused instances to free up the scaling capacity for other functions.\n- Your functions' concurrency is the number of instances that serve requests at a given time. For an initial burst of traffic, your functions' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region.\n- Lambda can scale faster than the regular Auto Scaling feature of EC2, Elastic Beanstalk, or ECS. This is because Lambda is more lightweight than other computing services. Under the hood, Lambda can run your code to the thousands of available AWS-managed EC2 instances (that could already be running) within seconds to accommodate traffic. This is faster than the Auto Scaling process of launching new EC2 instances that could take a few minutes or so. An alternative is to overprovision your compute capacity but that will incur significant costs.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/macie":{"title":"Macie","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [S3](/notes/aws/s3/s3.md)\n\n## **Macie**\n- is an ML-powered security service that helps you prevent data loss by automatically discovering, classifying, and protecting sensitive data stored in Amazon S3. Amazon Macie uses machine learning to recognize sensitive data such as personally identifiable information (PII) or intellectual property, assigns a business value, and provides visibility into where this data is stored and how it is being used in your organization.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/mq":{"title":"MQ","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **MQ**\n- Amazon MQ is a messaging service that is suitable for anyone using messaging with existing applications and wanting to move their messaging service to the cloud quickly and easily. It supports industry standard APIs and protocols like JMS, NMS, AMQP, STOMP, MQTT, and WebSocket. so you can switch from any standards-based message broker to Amazon MQ without rewriting the messaging code in your applications. ","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/nacls-and-security-groups":{"title":"NACLs and Security Groups","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [VPC](/notes/aws/vpc.md)\n\n## Network Access Control List (NACL)\n- NACLs are associated with subnets\n- Connections between resources within a subnet are NOT affected by NACLs\n- Each NACL contains 2 sets of rules\n\t- Inbound Rules - Affect data entering the subnet\n\t- Outbound Rules - Affect data leaving the subnet\n\t- These rule sets are only focused on the direction of the traffic, not necessarily whether it's a request or a response. A request can be either inbound or outbound.\n- NACLs are #stateless, so they don't know whether traffic is a request or response.\n- NACLs offer both explicit allows and explicit deny's\n- Only impacts data crossing the subnet boundary, communication between instances in the same subnet is not affected.\n- Allows you to block specific IP's and specific IP ranges\n- Not aware of any logical resources, they only allow you to use IP's, CIDR ranges, ports and protocols.\n- NACLs cannot be assigned to logical resources, they can only be assigned to subnets\n- NACLs are often used together with Security Groups to add explicit DENY (Bad IPs/Nets)\n- Each subnet can have one NACL (Default or Custom)\n- A NACL can be associated with MANY Subnets\n\n## Security Groups\n- Security Groups have #state, so they are aware of inbound/outbound requests\n- Security Groups automatically know whether something is a request or a response\n- Security Groups have no explicit deny capability\n\t- They can only ALLOW or implicitly DENY (By not explicitly allowing traffic)\n\t- Because of this, Security Groups cannot be used to block specific bad actors or traffic\n- Supports IP/CIDR based rules\n- Supports references to other Security Groups and itself\n- Security Groups are not attached to instances, they are attached to ENI's (Elastic Network Interfaces) (even if the UI shows it this way)\n- Security Groups apply to all traffic that enters or leaves the Elastic Network Interface.\n- Security Groups are capable of using logical references, meaning that, if you had an backend API that receives inbound traffic from another application that has a security group, the backend API security group can reference the logical resource (the other Security Group) as it's source of traffic for it's inbound rule.\n\t- This has the added benefit of scaling very well (meaning that multiple instances that have the Security Group associated with them, now have access to this backend API)\n- Security Groups also allow self referential rules. Meaning, that it can reference itself as the source and allow all traffic. This allows instances that share the same Security Group to freely communicate with each other. IP changes are handled automatically, which makes it easy to use for services that use autoscaling.\n\n\n### Security Group VS. Network Access Control List (NACL):\n- Acts as a firewall for associated Amazon EC2 instances | Acts as a firewall for associated subnets\n- Controls both inbound and outbound traffic at the instance level | Controls both inbound and outbound traffic at the subnet level\n- You can secure your VPC instances using only security groups | NACLs are an additional layer of defense\n- Supports allow rules only | Supports allow rules and deny rules\n- Stateful (Return traffic is automatically allowed, regardless of any rules) | Stateless (Return traffic must be explicitly allowed by rules)\n- Evaluates all rules before deciding whether to allow traffic | Evaluates rules in number order when deciding whether to allow traffic, starting with the lowest numbered rule\n- Applies only to the instance that is associated to it | Applies to all instances in the subnet it is associated with\n- Has separate rules for inbound and outbound traffic | Has separate rules for inbound and outbound traffic\n- A newly created security group denies all inbound traffic by default | A newly create NACL denied all inbound traffic by default\n- A newly created security group has an outbound rule that allows all outbound traffic by default | A newly create NACL denied all outbound traffic by default\n- Instances associated with a security group can't talk to each other unless you add rules allowing it | Each subnet in your VPC must be associated with a NACL. If none is associated, the default NACL is selected\n- Security groups are associated with network interfaces | You can associate a NACL with multiple subnets; however, a subnet can be associated with only one NACL at a time","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/nat-gateway":{"title":"NAT Gateway","content":"\n### Related Notes:\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [Egress-Only Internet Gateway](/notes/aws/egress-only-internet-gateway.md)\n- [VPC Gateway Endpoints](/notes/aws/vpc-gateway-endpoints.md)\n- [VPC Interface Endpoints](/notes/aws/vpc-interface-endpoints.md)\n\n### Helpful Links\n- [VPC Pricing](https://aws.amazon.com/vpc/pricing/)\n \u003e If you choose to create a NAT gateway in your VPC, you are charged for each “NAT Gateway-hour\" that your gateway is provisioned and available. Data processing charges apply for each gigabyte processed through the NAT gateway regardless of the traffic’s source or destination. Each partial NAT Gateway-hour consumed is billed as a full hour. You also incur standard AWS data transfer charges for all data transferred via the NAT gateway. If you no longer wish to be charged for a NAT gateway, simply delete your NAT gateway using the AWS Management Console, command line interface, or API.\n\u003e\u003e Price per NAT gateway ($/hour) is $0.045\n\u003e\u003e\u003e Price per GB data processed is $0.045\n\n## **NAT Gateway**\n- A NAT Gateway allows for private IPv4 addresses (of say, an EC2 instance or ECS instance) in a VPC's private subnet to connect to the public internet, and/or public AWS services while also _preventing the Internet from initiating a connection with those private IPv4 addresses.\n- A NAT Gateway is a #highly-available, managed Network Address Translation service. NAT gateway is created in a specific AZ and implemented with redundancy in that zone. You must create a NAT gateway on a public subnet to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances.\n- If you have resources in multiple AZs and they share one NAT gateway, and if the NAT gateway's AZ is down, resources in other AZs lose Internet access. To create an AZ-independent architecture, create a NAT gateway in each AZ and configure your routing to ensure that resources use the NAT gateway in the same AZ.\n- **NOTE**: You would never need to configure more than a single NAT gateway in a given AZ. This is because NAT Gateway is already redundant in nature, meaning, AWS already handles any failures that occur in your NAT Gateway in an availability zone.\n- NAT Gateway works with IPv6, with the extremely important caveat that absent any other filtering, NAT Gateway (IPv6) will allow all IPs bi-directionally. If you need to prevent egress traffic, use an [[egress-only-internet-gateway]] instead.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/opsworks":{"title":"OpsWorks","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## OpsWorks\n- Chef \u0026 Puppet help you perform server configuration automatically, or repetitive actions.\n- They work great with both EC2 and on-premises VMs\n- AWS OpsWorks = Managed Chef \u0026 Puppet\n- Alternative to [Systems Manager](/notes/aws/systems-manager.md)\n- AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your EC2 instances or on-premises compute environments. OpsWorks has 3 offerings - AWS OpsWorks for Chef Automate, AWS OpsWorks for Puppet Enterprise, and AWS OpsWorks Stacks.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/organizations":{"title":"AWS Organizations","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Organizations**\n- AWS Organizations is an account management services that lets you consolidate multiple AWS accounts into an organization that you create and centrally manage. With Organizations, you can create member accounts and invite existing accounts to join your organization. You can organize those account into groups and attach policy-based controls.\n- Consolidation of payment methods\n- You take a single \"Standard\" AWS account (one which is not within an Organization)\n- You create an AWS Organization with that account, and that account now becomes the master/management account\n- The management account in special for regions\n\t- asdf\n\t- asdf\n- With the management account, you can invite other Standard accounts to the Organization, which makes them \"member\" accounts of that Organization\n- The Organization Root (container within AWS Organization, that can contain AWS accounts at the top of this structure), can contain both the management account and member accounts\n- The Organization Root can contain other Organizations, known as Organizational Units (OU)\n- AWS Organization allows for consolidated billing, member accounts pass their billing through to the management/master account\n- Certain services offer volume discounts, which are easier to achieve by using consolidated billing\n- You can also create new accounts directly within an Organization, all that is required is a valid email address (AKA, no invite process)\n- With Organizations, you don't need IAM users inside of every account. Instead, you can use roles across AWS accounts that are assumed when needed\n- Often, large enterprises will have a separate account, distinct from the master account, that is used to hold all identities in your Organization. You can either have your own internal AWS identities using IAM, or configure AWS to allow identity federation so that on-premises identities can be used to access designated login accounts. Doing this, you can use a feature called \"role switch\" to assume roles in other accounts.\n\n### Service Control Policies (SCPs)\n- A feature of AWS Organizations which allows restrictions to be placed on member AWS accounts in the form of boundaries\n- SCPs can be attached to AWS Organizations as a whole, one or more Organizational Units, or individual AWS accounts\n- SCPs inherit down the organizational tree, so if they're attached to the Root OU, they affect everything below it.\n- Even if the management account has an SCP attached, it is not affected by the SCP. As a security practice, generally, you should not use the management account for resources.\n- SCPs are account permissions boundaries, they limit what the account can do (including the account root user)\n\t- Fine point detail: You can never restrict the account root user directly, but by applying an SCP to the account, you are indirectly restricting what the account root user can do.\n- You might apply a SCP to restrict usage of any region outside of your chosen operating region\n- SCPs don't grant any permissions, they only limit what permissions can be applied to an account\n- SCPs have an allow list vs deny list, but they establish which permissions can be granted in an AWS account. When an SCP has an allow list, that means that, no matter what permissions identities in this account are provided with, they can only use what has specifically been allowed by the SCP that is applied to their account. Note that this is very secure, but causes much more admin overhead.\n- Example: If an SCP allows S3 and EC2, then those are the only services that will ever be usable in that AWS account, no matter what permissions identities are granted.\n- Deny list architectures have much lower admin overhead, but allow list architectures are much more secure.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/privatelink":{"title":"PrivateLink","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [VPC Interface Endpoints](/notes/aws/vpc-interface-endpoints.md)\n\n## PrivateLink\n- AWS PrivateLink provides private connectivity between VPCs, AWS services, and your on-premises networks, without exposing your traffic to the public internet. AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly simplify your network architecture.\n- [VPC Interface Endpoints](/notes/aws/vpc-interface-endpoints.md), powered by AWS PrivateLink, connect you to services hosted by AWS Partners and supported solutions available in AWS Marketplace. By powering [Gateway Load Balancer endpoints,](https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway-load-balancer.html) AWS PrivateLink brings the same level of security and performance to your virtual network appliances or custom traffic inspection logic.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/rds":{"title":"","content":"### Related Notes\n- [[aws-wiki]]\n- [[aurora]]\n\n### Useful Links\n- [RDS Pricing](https://aws.amazon.com/rds/pricing/)\n- [RDS Free Tier](https://aws.amazon.com/rds/free/)\n\n### How to connect To RDS with MySQL Client:\n- This simplified guide was pulled from this AWS Docs page: [Connecting to a DB instance running the MySQL database engine](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ConnectToInstance.html). If you need your RDS instance's master password, you cannot obtain that information from the console. Use the AWS CLI to obtain such information, guide [here](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/lightsail/get-relational-database-master-user-password.html).\n1. Install `mariadb` and `mariadb-client` so that you can connect to the RDS instance:\n```\nbrew install mariadb\n```\n```\nbrew install mariadb-client\n```\n2. Verify install:\n```\nmysql --version\n```\n3. Connect to your instance (`-p` prompts for password):\n```\nmysql -h $WRITER_INSTANCE -P $PORT -u $DB_USER -p\n```\n\n### Create User in RDS (MySQL engine)\n1. View grants for master user as a reference, carefully determine the permissions that make sense for your user:\n```sql\nSHOW GRANTS for master_user;\n```\n2. Create new user and password:\n```sql\nCREATE USER 'new_user'@'%' IDENTIFIED BY 'password';\n```\n3. Grant permissions to the new user with the `GRANT` command:\n```sql\nGRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, PROCESS, REFERENCES, INDEX, ALTER, SHOW DATABASES, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, EVENT, TRIGGER ON *.* TO 'new_user'@'%' WITH GRANT OPTION;\n```\n\nuser: Don't give CREATE USER, INVOKE SAGEMAKER, INVOKE COMPREHEND, SELECT INTO S3, LOAD FROM S3\n\n- [How do I create another master user for my Amazon RDS DB instance that is running MySQL?](https://aws.amazon.com/premiumsupport/knowledge-center/duplicate-master-user-mysql/)\n\n## **RDS - Relational Database Service**\n\n### Storage Options\n- Storage is allocated to RDS, much in the same way that it is allocated in EC2. You have 3 options:\n  - gp2 (General Purpose SSD, Default) - General Purpose SSD volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of time. Baseline performance for these volumes is determined by the volume's size.\n\t- io1 (Provisioned IOPS) - Provisioned IOPS storage is designed to meet the needs of I/O-intensive workloads, particularly database workloads, that require low I/O latency and consistent I/O throughput.\n\t- Magnetic (Amazon RDS also supports magnetic storage for backward compatibility, but it is not recommended)\n\n### RDS Backups And Restores\n- If Multi-AZ is enabled, backups occur from the standby instance (primary instance is never used)\n- If Multi-AZ is disabled, backups occur from the single running instance\n\n#### RPO - Recovery Point Objective\n- Time between the last backup and the incident\n- Amount of maximum data loss\n- Influences technical solution \u0026 cost\n- Generally lower values cost more (more backups, more often)\n- If an incident occurs 8 hours after the last incremental database backup was taken, that's an 8 hour RPO.\n\n#### RTO - Recovery Time Objective\n- Time between the DR event and full recovery\n- Influenced by process, staff, tech and documentation\n- Generally lower values cost more\n\n#### Manual Snapshots\n- Ran against an RDS database. The first snapshot is a full copy of the data used on the RDS volume. From then on, each snapshot is incremental.\n- There is a brief interruption between the compute resource (think ECS or Lambda) and the storage, when a snapshot is taken.\n\t- This can impact your application, but only if you're using Single-AZ\n\t- Incremental backups are usually much quicker, as long as you don't have massive changes in data\n\t- Manual snapshots live on past the lifetime of the RDS instance\n\t\n#### Automated Backups\n- Just snapshots, that occur automatically\n- Occurs during a backup window that you define\n- Window impacts RPO\n- Every 5 minutes, RDS writes transaction logs to S3 every 5 minutes. This allows you to pair with a snapshot, so that you can achieve a very low RPO value. These logs have a retention period from 0 to 35 days.\n\t- This works by first, restoring the backup, and then \"replaying\" transaction logs to bring DB to desired point in time\n\n#### RDS Restores\n- Creates a new RDS Instance - new address, meaning application code changes\n- Snapshots = single point in time, creation time\n- Automated = any 5 minute point in time\n- RDS Restores aren't fast, especially if the database is large - Think about RTO\n- Snapshots are the only way to protect from data corruption, with replication, corrupted data is also replicated\n\n### RDS Read-Replicas\n- Read Replicas, unlike the Multi-AZ standby instances, can be used for read operations.\n- Kept in sync using asynchronous replication. Written fully to the primary instance first, then propagated to the read replicas, which means there could be a slight lag.\n- Read Replicas provide performance improvements\n\t- 5 dedicated read-replicas per instance\n\t- Each providing an additional instance of read performance, allows scaling out read capacity\n\t- Applications code can be configured to perform reads against read replicas\n- Cross-region read replicas can be created, AWS handles all networking\n\t- Provides global performance and availability improvements by deploying cross region read replicas around the world\n- Read Replicas offer near zero RPO, if the primary instance fails, the Read Replica can be quickly promoted to be a new primary instance. This can be done in minutes. Works for database failure only - watch for data corruption.\n\n### Enhanced Monitoring\n- **Enhanced Monitoring** metrics are useful when you want to see how different processes or threads on a DB instance use the CPU when working with RDS. \n\n### IAM DB Authentication\n- **IAM DB Authentication** is used to authenticate to your DB instance. This works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you generate an authentication token, which has a lifetime of 15 minutes.\n\n### Multi-AZ ( #highly-available #high-availability )\n- RDS provides high availability and failover support for DB instances using **Multi-AZ** deployments (**Single-AZ** is also supported, if desired).\n- Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for Oracle, PostgreSQL, MySQL, and MariaDB DB instances use Amazon's failover technology. SQL Server DB instances use SQL Server Database Mirroring (DBM). In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different AZ. The primary DB instance is synchronously replicated across AZs to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and AZ disruption. Amazon RDS detects and automatically recovers from the most common failure scenarios for Multi-AZ deployments so that you can resume database operations as quickly as possible without administrative intervention. The high-availability feature is not a scaling solution for read-only scenarios; you cannot use a standby replicate to serve read traffic. To service read-only traffic, you should use a Read Replica.\n- Amazon RDS automatically performs a failover in the event of any of the following:\n\t1. Loss of availability in the primary Availability Zone.\n\t2. Loss of network connectivity to primary.\n\t3. Compute unit failure on primary.\n\t4. Storage failure on primary.\n- In Amazon RDS, failover is automatically handled so that you can resume database operations as quickly as possible without administrative intervention in the event that your primary database instance went down. When failing over, Amazon RDS simply flips the canonical name record (CNAME) for your DB instance to point at the standby, which is in turn promoted to become the new primary.\n\t- Note that a standby DB instance can't be directly used, unless a failover happens, meaning that it cannot be used to scale read access. A read replica must be used for this purpose.\n\t- Note that RDS Multi-AZ is highly available, but not fault tolerant. There will be 60-120 seconds of impact during the failover. \n\t- Note that RDS Multi-AZ happens in the same region only (other AZs in the VPC)\n\t- Note that by using RDS Multi-AZ, backups are taken from Standby DB instance, removing any potential performance impact.\n\n### RDS Storage Auto Scaling\n- RDS Storage Auto Scaling automatically scales storage capacity in response to growing database workloads, with zero downtime. Under-provisioning could result in application downtime, and over-provisioning could result in underutilized resources and higher costs. With RDS Storage Auto Scaling, you simply set your desired maximum storage limit, and Auto Scaling takes care of the rest. RDS Storage Auto Scaling continuously monitors actual storage consumption, and scales capacity up automatically when actual utilization approaches provisioned storage capacity. Auto Scaling works with new and existing database instances. You can enable Auto Scaling with just a few clicks in the AWS Management Console. There is no additional cost for RDS Storage Auto Scaling. You pay only for the RDS resources needed to run your applications.\n\n### Database Instance Types\n- `db.m5` - General Usage Type\n- `db.r5` - Memory Optimized Type\n- `db.t3` - Burst Instance Type\n\n### Security Considerations\n- Access to RDS is controlled via Security Group\n\n### #IAM #database #authentication for MySQL and PostgreSQL\nYou can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token.\nUse IAM DB Authentication and create database accounts using the AWS-provided `AWSAuthenticationPlugin` plugin in MySQL.\n\n#aws #relational-database #relational #sql","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/redshift":{"title":"Redshift","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Redshift**\n- Amazon Redshift is a data warehousing solution that can handle data on an exabytes scale. You may be considering the service for a number of use cases such as processing real-time analytics, combining multiple data sources, log analysis, or more.\n- Although Amazon Redshift is a fully-managed data warehouse, you will still need to configure cross-region snapshot copy to ensure that your data is properly replicated to another region. You can configure Amazon Redshift to copy snapshots for a cluster to another region. To configure cross-region snapshot copy, you need to enable this copy feature for each cluster and configure where to copy snapshots and how long to keep copied automated snapshots in the destination region. When cross-region copy is enabled for a cluster, all new manual and automatic snapshots are copied to the specified region.\n- Amazon Redshift also includes Redshift Spectrum, allowing you to directly run SQL queries against exabytes of unstructured data in Amazon S3. No loading or transformation is required, and you can use open data formats, including Avro, CSV, Grok, ORC, Parquet, RCFile, RegexSerDe, SequenceFile, TextFile, and TSV. Redshift Spectrum automatically scales query compute capacity based on the data being retrieved, so queries against Amazon S3 run fast, regardless of data set size.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/resource-access-manager":{"title":"Resource Access Manager","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **RAM - Resource Access Manager**\n- AWS RAM is a service that enables you to easily and securely share AWS resources with any AWS account, or within your AWS Organization. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM. ","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/route-53":{"title":"Route 53","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [Route 53 Health Checks](/notes/aws/route-53-health-checks.md)\n\n## **Route 53**\nRoute 53 provides 2 primary services\n1. Register Domains\n2. Host Zone files on managed name servers\n\t- A zone file is just a database which contains all of the DNS information for a particular domain\n\t- The zone file, known as a \"Hosted Zone\", in AWS terminology, is placed on 4 managed database name servers that are distributed globally by AWS\n- Global service\n- Globally resilient\n- Route 53 is basically DNS-as-a-Service\n- Hosted Zones can be public or private (linked to one or more VPCs)\n\n### Hosted Zones\n- A R53 Hosted Zone is a DNS DB for a domain e.g. connerverret.com\n- Globally resilient (multiple DNS servers)\n- Created with domain registration via R53 - can be created separately if you want to register a domain elsewhere\n- Host DNS Records (e.g. A, AAAA, MX, NS, TXT)\n- Hosted Zones are what the DNS system references - Authoritative for a domain e.g. connerverret.com\n\n#### Public Hosted Zone\n- A public hosted zone is a container that holds information about how you want to route traffic on the internet for a specific domain which is accessible from the public internet\n- DNS Database (zone file) hosted by R53 (Public Name Servers)\n- Accessible from the public internet (\u0026 VPCs, using Route 53 Resolver)\n- Hosted on \"4\" R53 Name servers (NS) specific for the zone\n- Use \"NS records\" to point at theses NS (connect to global DNS)\n- Resource Records (RR) created within the Hosted Zone\n- Externally registered domains can point at the R53 Public Zone\n- Monthly cost for running a public hosted zone\n\n#### Private Hosted Zone\n- A private hosted zone is a container that holds information about how you want Amazon Route 53 to respond to DNS queries for a domain and its subdomains within one or more VPCs that you create with the Amazon VPC service\n- A Hosted Zone that isn't public\n- Associated with VPCs - Only accessible in those VPCs\n- Using different accounts is supported via CLI/API\n- Split-view (overlapping public and private) for PUBLIC and INTERNAL use with the same zone name\n\n### CNAME vs R53 Alias\n- An \"A\" records maps a NAME to an IP Address\n\t- connerverret.com =\u003e 1.3.3.7\n- CNAME maps a NAME to another NAME\n\t- www.connerverret.com =\u003e connerverret.com\n- CNAME is invalid for naked/apex (connerverret.com)\n- Many AWS services use a DNS Name (ELBs)\n- With just CNAME - connerverret.come =\u003e ELB would be invalid\n\n### Route 53 Health Checks\n- #wip - on the study guide for the #aws-sysops exams\n- Cantrill lecture: https://learn.cantrill.io/courses/aws-certified-sysops-administrator-associate/lectures/27288053\n\n#### Alias Records\n- Implemented by AWS, outside the normal DNS standard (only usable if route 53 is hosting your domains)\n- ALIAS records map a NAME to an AWS resource\n- Can be used for both naked/apex and normal records\n- For non apex/naked - functions like CNAME\n- There is no charge for ALIAS requests pointing at AWS resource\n- For AWS Services - default to picking ALIAS\n- Should be the same \"Type\" as what the record is pointing at\n- API Gateway, CloudFront, Elastic Beanstalk, ELB, Global Accelerator \u0026 S3\n\n### Geolocation Routing VS. Geoproximity Routing\n- **Geolocation Routing** lets you choose the resources that serve your traffic based on the geographical location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. When you use geolocation routing, you can localize your content and present some or all of your website in the language of your users. You can also use geolocation routing to restrict distribution of content to only the locations in which you have distribution rights. Another possible use is for balancing load across endpoints in a predictable, easy-to-manage way, so that each user location is consistently routed to the same endpoint.\n-  lets you choose the resources that serve your traffic based on the geographical location of the resources.\n- **Geoproximity Routing** lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources. You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource.\n### Weighted Routing\n- **Weighted Routing** lets you associate multiple resources with a single domain name (conner.com) or subdomain name (subdomain.conner.com) and choose how much traffic is routed to each resource.\n\t- This can be useful for a variety of purposes, including load balancing and testing new versions of software. You can set a specific percentage of how much traffic will be allocated to the resource by specifying the weights.\n\t- For example, if you want to send a tiny portion of your traffic to one resource and the rest to another resource, you might specify weights of 1 and 255. The resources with a weight of 1 gets 1/256th of the traffic (1/1+255), and the other resource get 255/256ths (255/1+255).\n\t- You can gradually change the balance by changing the weights. If you want to stop sending traffic to a resource, you can change the weight for that record to 0.\n### Latency Routing\n- Latency Routing lets Amazon Route 53 serve user requests from the AWS Region that provides the lowest latency. It does not, however, guarantee that users in the same geographic region will be served from the same location.\n### S3 Bucket Website Routing\n- Here are the prerequisites for routing traffic to a website that is hosted in an Amazon S3 Bucket:\n\t- An S3 bucket that is configured to host a static website. The bucket must have the same name as your domain or subdomain. For example, if you want to use the subdomain portal.conner.com, the name of the bucket must be portal.conner.com\n\t- A registered domain name. You can use Route 53 as your domain registrar, or you can use a different registrar.\n\t- Route 53 as the DNS service for the domain. If you register your domain name by using Route 53, we automatically configure Route 53 as the DNS service for the domain.\n- You can create a new Route 53 with the failover option to a static S3 website bucket or CloudFront distribution as an alternative.\n### Routing Traffic to a Load Balancer\n- To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias record that points to your load balancer. An alias record is a Route 53 extension to DNS. It's similar to a CNAME record, but you can create an alias record both for the root domain, such as conner.com, and for subdomains, such as portal.conner.com. (You can create CNAME records only for subdomains). To enable IPv6 resolution, you would need to create a second resource record, conner.com ALIAS AAAA -\u003e myelb.us-west-2.elb.amazonnaws.com, this is assuming your Elastic Load Balancer has IPv6 support.## **Route 53**\n### Geolocation Routing VS. Geoproximity Routing\n- **Geolocation Routing** lets you choose the resources that serve your traffic based on the geographical location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. When you use geolocation routing, you can localize your content and present some or all of your website in the language of your users. You can also use geolocation routing to restrict distribution of content to only the locations in which you have distribution rights. Another possible use is for balancing load across endpoints in a predictable, easy-to-manage way, so that each user location is consistently routed to the same endpoint.\n-  lets you choose the resources that serve your traffic based on the geographical location of the resources.\n- **Geoproximity Routing** lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources. You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource.\n### Weighted Routing\n- **Weighted Routing** lets you associate multiple resources with a single domain name (conner.com) or subdomain name (subdomain.conner.com) and choose how much traffic is routed to each resource.\n\t- This can be useful for a variety of purposes, including load balancing and testing new versions of software. You can set a specific percentage of how much traffic will be allocated to the resource by specifying the weights.\n\t- For example, if you want to send a tiny portion of your traffic to one resource and the rest to another resource, you might specify weights of 1 and 255. The resources with a weight of 1 gets 1/256th of the traffic (1/1+255), and the other resource get 255/256ths (255/1+255).\n\t- You can gradually change the balance by changing the weights. If you want to stop sending traffic to a resource, you can change the weight for that record to 0.\n### Latency Routing\n- Latency Routing lets Amazon Route 53 serve user requests from the AWS Region that provides the lowest latency. It does not, however, guarantee that users in the same geographic region will be served from the same location.\n### S3 Bucket Website Routing\n- Here are the prerequisites for routing traffic to a website that is hosted in an Amazon S3 Bucket:\n\t- An S3 bucket that is configured to host a static website. The bucket must have the same name as your domain or subdomain. For example, if you want to use the subdomain portal.conner.com, the name of the bucket must be portal.conner.com\n\t- A registered domain name. You can use Route 53 as your domain registrar, or you can use a different registrar.\n\t- Route 53 as the DNS service for the domain. If you register your domain name by using Route 53, we automatically configure Route 53 as the DNS service for the domain.\n- You can create a new Route 53 with the failover option to a static S3 website bucket or CloudFront distribution as an alternative.\n\n### Routing Traffic to a Load Balancer\n- To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias record that points to your load balancer. An alias record is a Route 53 extension to DNS. It's similar to a CNAME record, but you can create an alias record both for the root domain, such as conner.com, and for subdomains, such as portal.conner.com. (You can create CNAME records only for subdomains). To enable IPv6 resolution, you would need to create a second resource record, conner.com ALIAS AAAA -\u003e myelb.us-west-2.elb.amazonnaws.com, this is assuming your Elastic Load Balancer has IPv6 support.\n\n### DNS Record Types\n- Nameserver (NS) Records - How delegations works\n- A and AAAA Records - Map hostnames to IP addresses\n\t- A maps host to IPv4 addresses\n\t- AAAA maps host to IPv6 addresses\n- CNAME Records - Creates host to host records\n\t- For example, you might have 3 CNAME records for a single IP address, that all resolve to the same server. This is typically done to reduce admin overhead. CNAME records map to an A or AAAA record.\n\t\t- ftp\n\t\t- mail\n\t\t- www\n- MX Records - Find a mail server (SMTP) for a domain\n\t- Consists of 2 parts\n\t\t- Priority\n\t\t- Value - A host inside the zone, or a host outside the zone\n- TXT Records - Prove domain ownership by adding a record to the domain so that a 3rd party can verify domain ownership, fights spam by indicating which identities are authorized\n- DNS records also have TTL values, which determine how long DNS is cached on the resolver server. High TTL values are recommended to be lowered prior to doing any DNS migration.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/route-53-health-checks":{"title":"Route 53 Health Checks","content":"\n### Related Notes\n- [Route 53](/notes/aws/route-53.md)\n\n## Route 53 Health Checks\n- Health checks are separate from, but are used by records\n- They're configured separately\n- Health checkers located globally\n- Health checkers check every 30s (every 10s costs extra)\n- TCP, HTTP/S, HTTP/S with String Matching\n- Moves between either Healthy or Unhealthy state\n- Endpoint, CloudWatch Alarm, Checks of Checks (Calculated)","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3":{"title":"S3","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [S3 Access Logs](/notes/s3/s3-access-logs.md)\n- [S3 Access Points](/notes/aws/s3/.md)\n- [S3 Cors](/notes/aws/s3/s3-cors.md)\n- [S3 Encryption](/notes/aws/s3/s3-encryption.md)\n- [S3 Events](/notes/aws/s3/s3-events.md)\n- [S3 Inventory](/notes/aws/s3/s3-inventory.md)\n- [S3 Lifecycle Configuration](/notes/aws/s3/s3-lifecycle-configuration.md)\n- [S3 Object Lock](/notes/aws/s3/s3-object-lock.md)\n- [S3 Object Storage Class](/notes/aws/s3/s3-object-storage-class.md)\n- [S3 Object Versioning and MFA Delete](/notes/aws/s3/s3-object-versioning-and-mfa-delete.md)\n- [S3 Presigned URLs](/notes/aws/s3/s3-presigned-urls.md)\n- [S3 Replication](/notes/aws/s3/s3-replication.md)\n- [S3 Select](/notes/aws/s3/s3-select.md)\n- [S3 Security](/notes/aws/s3/s3-security.md)\n- [S3 Static Website Hosting](/notes/aws/s3/s3-static-website-hosting.md)\n\n## **Simple Storage Service (S3)**\n- Amazon S3 now provides increased performance to support at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data, which can save significant processing time for no additional charge. Each S3 prefix can support these request rates, making it simple to increase performance significantly.\n- S3 supports Multi-Factor Authentication Deletes, allowing you to secure your S3 objects from accidental deletion or overwrite.\n- In S3, all objects are private by default. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your project, you must provide security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration. Anyone who receives the pre-signed URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a pre-signed URL.\n- The S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket.\n\t- Amazon S3 supports the following destinations where it can publish events:\n\t\t- Amazon SNS topic\n\t\t- Amazon SQS queue\n\t\t- AWS Lambda\n\t- In SNS, the fanout scenario is when a message publish to an SNS topic is replicated and pushed to multiple endpoints, such as Amazon SQS queues, HTTP(S) endpoints, and Lambda functions. This allows for parallel asynchronous processing. For example, you can develop an application that publishes a message to an SNS topic whenever an order is placed for a product. Then, SQS queues that are subscribed to the SNS topic receive identical notifications for the new order. An Amazon EC2 server instance attached to one of the SQS queues can handle the processing or fulfillment of the order. And you can attach another Amazon EC2 server instance to a data warehouse for analysis of all orders received.\n\t- Based on the given scenario, the existing setup sends the event notification to an SQS queue. Since you need to send the notification to the development and operations team, you can use a combination of Amazon SNS and SQS. By using the message fanout pattern, you can create a topic and use two Amazon SQS queues to subscribe to the topic. If Amazon SNS receives an event notification, it will publish the message to both subscribers.\n\t- Take note that Amazon S3 event notifications are designed to be delivered at least once and to one destination only. You cannot attach two or more SNS topics or SQS queues for S3 event notification. Therefore, you must send the event notification to Amazon SNS.\n- How would a medical company, storing Personally Identifiable Info of users in an S3 bucket ensure that the master keys and unencrypted data is never sent to AWS?\n\t- Use S3 client-side encryption with a client-side master key.\n\t\t- **Client-side encryption** is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options:\n\t\t\t- Use an AWS KMS-managed customer master key\n\t\t\t- Use a client-side master key\n- How would you aggregate 500 GB of data daily to a forecasting application hosted in the Northern Virginia region? Use Transfer Acceleration in the destination bucket and upload the collected data using Multipart Upload.\n- Amazon S3 has life cycle configurations. With S3 life cycle rules, you can transition files to S3 Standard IA or S3 Glacier. Using S3 Glacier expedited retrieval, you can quickly access your files within 1-5 minutes.\n- Amazon S3 IA (Infrequently Accessed) is for exactly what it sounds like, infrequent access (but rapid retrieval when accessed). S3-IA is not highly available, because unlike standard s3, it is only relies on one availability zone for storing data.\n- Amazon S3 Glacier, on the other hand, is for Deep Archival of data, and is the deepest level of archival you can achieve with S3. Even though this data is not meant to be retrieved frequently, you can make use of **Expedited Retrievals** to quickly access your data when occasional urgent requests for a subset of archives are required. For all but the largest archives (250 MB+), data accessed using Expedited retrievals are typically made available within 1-5 minutes. **Provisioned Capacity** is what ensures that retrieval capacity for Expedited Retrievals is available when you need it.\n\t- To make an Expedited, Standard, or Bulk retrieval, set the Tier parameter in the Initiate Job (POST jobs) REST API request to the option you want, or the equivalent in the AWS CLI or AWS SDKs. If you have purchases provisioned capacity, then all expedited retrievals are automatically served through your provisioned capacity.\n\t- Each unit of **Provisioned Capacity** provides at least 3 expedited retrievals that can be performed every 5 minutes and provides up to 150 MB/s of retrieval throughput. You should purchase provisioned retrieval capacity if your workload requires highly reliable and predictable access to a subset of your data in minutes. Without provisioned capacity Expedited retrievals are accepted, except for rare situations of unusually high demand. However, if you require access to Expedited Retrievals under all circumstances, you must purchase provisioned retrieval capacity.\n- If a company uses multiple AWS accounts that are consolidated with AWS Organizations, and wants to copy several S3 objects to another S3 bucket that belonged to a different AWS account (that they also own), how would that be accomplished?\n\t- So, cross-account permissions in S3 would be needed. This can be accomplished by creating an IAM customer-managed policy that allows an IAM user or role to copy objects from the source bucket in one account to the destination bucket in another account. By default, an S3 object is owned by the account that uploaded the object.That's why granting the destination account the permissions to perform the cross-account copy makes sure that the destination owns the copied objects. You can also change the ownership of an object by changing its access control list (ACL) to bucket-owner-full-control.\n- You can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints. A gateway endpoint is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. Interface endpoints extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on premises, or from a VPC in another AWS Region using VPC peering or AWS Transit Gateway.\n- S3 supports cross region replication to ensure that your S3 bucket is not affected even if there is an outage in one of the AZs or a regional outage. When you upload data to S3, your objects are stored redundantly on multiple devices across multiple facilities within the region only, where you created the bucket. Thus, if you want to be fault tolerant to a regional failure, you *must* enable cross region replication on your S3 bucket.\n- Amazon S3 is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon S3. CloudTrail captures a subset of API calls for Amazon S3 as events, including calls from the Amazon S3 console and code calls to the Amazon S3 APIs. AWS CloudTrail logs provide a record of actions taken by a user, role, or an AWS service in Amazon S3, while Amazon S3 server access logs provide detailed records for the requests that are made to an S3 bucket.\n\t- So, enable *server access logging* if you want to log every request access to their S3 buckets including requester, bucket name, request time, referrer, turnaround time, and error code info.\n- Objects must be stored **at least 30 days** in the current storage class before you can transition them to STANDARD_IA or ONEZONE_IA. For example, you cannot create a lifecycle rule to transition objects to the STANDARD_IA storage class one day after you create them. Amazon S3 doesn't transition objects within the first 30 days because newer objects are often accessed more frequently or deleted sooner than is suitable for STANDARD_IA or ONEZONE_IA storage. Glacier is different, you can transition objects to Glacier after 7 days.\n\n### Amazon S3 Glacier\n- An Amazon S3 Glacier vault can have one resource-based vault access policy and one Vault Lock policy attached to it. A *Vault Lock* policy is a vault access policy that you can lock. Using a Vault Lock policy can help you enforce regulatory and compliance requirements. Amazon S3 Glacier provides a set of API operations for you to manage the Vault Lock policies.\n\t- As an example of a Vault Lock policy, suppose that you are required to retain archives for one year before you can delete them. To implement this requirement, you can create a Vault Lock policy that denies users permissions to delete an archive until the archive has existed for one year. You can test the policy before locking it down, but after you lock the policy, it becomes immutable.\n\n### Glacier Vault Lock\nGlacier Vault Lock allows you to easily deploy and enforce compliance controls for individual Glacier vaults with a vault lock policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed. Hence, **using Amazon S3 Glacier Vault Lock** is the correct answer.\n\nGlacier enforces the controls set in the vault lock policy to help achieve your compliance objectives, for example, for data retention. You can deploy a variety of compliance controls in a vault lock policy using the AWS Identity and Access Management (IAM) policy language.\n\nA vault lock policy is different than a vault access policy. Both policies govern access controls to your vault. However, a vault lock policy can be locked to prevent future changes, providing strong enforcement for your compliance controls. You can use the vault lock policy to deploy regulatory and compliance controls, which typically require tight controls on data access. In contrast, you use a vault access policy to implement access controls that are not compliance related, temporary, and subject to frequent modification. Vault lock and vault access policies can be used together.\n\nFor example, you can implement time-based data retention rules in the vault lock policy (deny deletes) and grant read access to designated third parties or your business partners (allow reads).\n\nLocking a vault takes two steps:\n1. Initiate the lock by attaching a vault lock policy to your vault, which sets the lock to an in-progress state and returns a lock ID. While in the in-progress state, you have 24 hours to validate your vault lock policy before the lock ID expires.\n2. Use the lock ID to complete the lock process. If the vault lock policy doesn’t work as expected, you can abort the lock and restart from the beginning.\n\n### S3 Gotchas\nThese 2 ARNs are not the same, and some policies need just one, or both:\n`arn:aws:s3:::catgifs` - Acts on the bucket itself\n`arn:aws:s3:::catgifs/*` - Acts on the objects in the bucket, as a wildcard\n","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-access-logs":{"title":"","content":"### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 Access Logs\n- Typically, you'll have a Source Bucket and a Target Bucket\n- You must enable access logging inside of the Source Bucket to use this feature\n- Log Delivery Group needs write access to the Target Bucket\n- Log files are delivered in new line delimited files that contain records. Attributes in a record are space delimited.\n- You personally manage the lifecycle of the log files\n- Server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.\n\n#aws #s3 #aws-sysops ","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-access-points":{"title":"S3 Access Points","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 Access Points\n- Simply managing access at scale for applications using shared data sets on S3 buckets. Access Points are unique hostnames that customers create to enforce distinct permissions and network controls for any request made through the access point.\n- Rather than 1 bucket with 1 Bucket Policy\n- Create many access points, each with different policies, and each with different network access controls\n\t- You can have an Access Point with a VPC origin - This requires a VPC endpoint\n\t- You can have an Access Point with a Internet Origin\n- Each access point has its own endpoint address\n- Created via Console or `aws s3control create-access-point --name secretcats --account-id 123456789012 --bucket catpics`\n- Each Access Point has a unique DNS address for network address\n- Access Point policies control permissions for access via the Access Point and are functionally equivalent to a bucket policy. Access Point policies can restrict identities to certain prefix(s), tags, or actions based on need.\n- Any permissions defined on an Access Point need to be also defined on the Bucket Policy.\n\t- Normally, on the Bucket Policy, you'd grant wide open access to the Access Point\n\t- From there, you'd define more granular control over access to objects on the Access Point policy\n","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-cors":{"title":"S3 CORS","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 CORS\n- Same-origin requests are always allowed, defined on initial request.\n\t- Example is an S3 bucket that loads an index.html file and some javascript\n\t\t- This will be allowed, but if the index.html file or javascript calls ANOTHER origin, such as an API Gateway, or different S3 bucket, those requests will fail.\n- Cross origin requests are restricted by default in S3\n- CORS configurations need to be defined on the API Gateway or the other S3 bucket\n\t- The specific origin or \\* must be defined or the request will fail\n- Access-Control-Allow-Origin: Either contains a wild card or a particular origin\n- Access-Control-Max-Age: Indicates how long the results of a preflight request can be cached\n- Access-Control-Allow-Methods: Either contains a wild card or a list of methods that can be used for cross origin request - GET, POST, DELETE, PUT, etc.\n- Access-Control-Allow-Headers: Contained in a CORS Configuration and within the response to a preflight request. Indicates which HTTP headers can be used within the actual request.\n- Preflight \u0026 Preflighted requests\n\t- Your browser sends an HTTP request to the other origin, and it will determine if the request that you're actually making is safe to send","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-encryption":{"title":"S3 Encryption","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 Encryption\n- Buckets aren't encrypted, encryption is defined at the object level\n- 2 methods of encryption that S3 supports, both are encryption at rest. Encryption at transit is a separate topic altogether, and comes standard with S3.\n\t- Client side encryption - Objects are encrypted by the client before they are ever uploaded to an S3 bucket. The data is cipher text the entire time. It is received in scrambled form, and stored in scrambled form.\n\t- Server side encryption - Data is encrypted once it hits the S3 endpoint, and it is stored in it's encrypted form.\n\n### Types of S3 Server Side Encryption\n\n#### Server-Side Encryption with Customer-Provided Keys (SSE-C)\n- S3 manages cryptographic operations\n- This method offloads the CPU capacity of encryption/decryption to S3\n- You need to supply the key that will be used by S3 at the time that you upload an object.\n- A hash of the key is taken, this hash cannot be used to generate a new key, but it verifies that the decryption key passed to S3 during a decryption operation is valid. Essentially a safety feature.\n- You must provide encryption key information using the following request headers:\n\t- `x-amz-server-side-encryption-customer-algorithm`\n\t- `x-amz-server-side-encryption-customer-key`\n\t- `x-amz-server-side-encryption-customer-key-MD5`\n\n#### Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)\n- With this method, S3 handles both the encryption/decryption process, as well as the key generation and management.\n- Uses AES-256\n- Usually the best choice for most applications\n- Bad choice if storing health or medical data that requires strong role separation\n\n#### Server-Side Encryption with KMS Keys (SSE-KMS)\n- An AWS Managed KMS Key created, and a unique DEK (Data Encryption Key) is used to encrypt each object.\n- Gives fine-grained control over key rotation.\n- Allows for role separation, because specific KMS key permissions are required on a per-key basis\n\n### Default Bucket Encryption\n- When uploading objects to S3 using the `PutObject` operation, you can specify as specific header, known as `x-amz-server-side-encryption`. This is how you direct S3 to use server side encryption.\n- However, you can set this as a default at the bucket level in S3. A specified header will always take priority over a set default.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-events":{"title":"S3 Events","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 Events\n- A feature that allows you to create event notification configurations on a bucket\n- Supports SNS, SQS, Lambda functions, etc.\n- Object Create (Put, Post, Copy, CompleteMultiPartUpload)\n- Object Delete (Delete, DeleteMarkerCreated)\n- Object Restore (Post (initiated), Completed)\n- Replication (OperationMissedThreshold, OperationReplicatedAfterThreshold, OperationNotTracked, OperationFailedReplication)\n- EventBridge is probably the better option to use, S3 Events is an older feature in AWS","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-inventory":{"title":"S3 Inventory","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 Inventory\nAmazon S3 Inventory is one of the tools Amazon S3 provides to help manage your storage. You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs. You can also simplify and speed up business workflows and big data jobs using Amazon S3 Inventory, which provides a scheduled alternative to the Amazon S3 synchronous `List` API operation. Amazon S3 Inventory does not use the `List` API to audit your objects and does not affect the request rate of your bucket.\n\nAmazon S3 Inventory provides comma-separated values (CSV), [Apache optimized row columnar (ORC)](https://orc.apache.org/) or [Apache Parquet](https://parquet.apache.org/) output files that list your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or a shared prefix (that is, objects that have names that begin with a common string). If weekly, a report is generated every Sunday (UTC) after the initial report. For information about Amazon S3 Inventory pricing, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/).\n\nYou can configure multiple inventory lists for a bucket. You can configure what object metadata to include in the inventory, whether to list all object versions or only current versions, where to store the inventory list file output, and whether to generate the inventory on a daily or weekly basis. You can also specify that the inventory list file be encrypted.\n\nYou can query Amazon S3 Inventory using standard SQL by using [Amazon Athena](https://docs.aws.amazon.com/athena/latest/ug/what-is.html), Amazon Redshift Spectrum, and other tools such as [Presto](https://prestodb.io/), [Apache Hive](https://hive.apache.org/), and [Apache Spark](https://databricks.com/spark/about/). You can use Athena to run queries on your inventory files. You can use it for Amazon S3 Inventory queries in all Regions where Athena is available.\n\n- Helps you manage (at a high level) your storage\n- Inventory of objects and various optional fields\n- Some of the fields include\n\t- Encryption\n\t- Size\n\t- Last Modified\n\t- Storage Class\n\t- Version ID\n\t- Replication Status\n\t- Object Lock\n\t- etc.\n- You can generate reports daily or weekly. You cannot \"force\" generate a report, either. You have to create a configuration, specify whether you want daily or weekly, and then that process will run in the background. It can take up to 48 hours to get the first inventory report.\n- Outputs are CSV, Apache ORC, or Apache 1Parquet\n- Multiple inventories can be setup, and they go to a target bucket in the same or a different account. Needs a bucket policy.\n- Used for Audit, Compliance, Cost Management or any specific regulations","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-lifecycle-configuration":{"title":"S3 Lifecycle Configuration","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n- [S3 Object Storage Class](/notes/aws/s3/s3-object-storage-class.md)\n\n## S3 Lifecycle Configuration\n- A set of rules, consisting of actions\n- Rules can apply to an entire bucket, or groups of objects\n- Transition Actions move objects to a different Object Storage Class\n\t- Transitions can only happen in a downward direction\n\t- Be aware of the 30 day S3 waiting period before you can transition an object\n- Expiration Actions can delete Objects or Object Versions after a certain time period","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-object-lock":{"title":"S3 Object Lock","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 Object Lock\n\n- Object Lock enables a Write-Once-Read-Many (WORM) - No Delete, No Overwrite\n- Object Lock can easily be enabled on new buckets, but if you want to enable it on existing buckets, you have to file for AWS support\n- You cannot disable Object Lock\n- Versioning is enabled on a bucket when Object Lock is enabled\n\t- Individual versions are locked\n- Both, One, or the other, or none\n- A bucket can have default Object Lock settings for all of the below information\n- This is a very important S3 feature to understand for the SysOps Administrator Associate Certification Exam\n\n- Retention Period - Prevent Deletion\n\t- Specify Days and Years - A Retention Period\n\t- Compliance Mode - Object (and Retention Period settings) cannot be adjusted, deleted or overwritten, even by the account root user\n\t\t- Strictest version of Object Lock\n\t\t- Medical or Financial Data makes sense here\n\t- Governance Mode - Special permissions can be granted allowing lock setting to be adjusted with the `s3:BypassGovernanceRetention` and passing the `x-amx-bypass-governance-retention:true` header. (default if done through the console UI)\n- Legal Hold - Set to be ON or OFF for Object Versions, cannot be changed or deleted while ON\n\t- Set on an Object Version - ON or OFF\n\t- No Retention\n\t- NO Deletes or Changes until removed\n\t- Prevents accidental deletion of critical object versions\n\t- `s3:PutObjectLegalHold` is to be set when object is uploaded or when legal hold is required\n\n#aws #s3 #aws-sysops ","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-object-storage-class":{"title":"S3 Object Storage Class","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 Object Storage Classes\n\n### S3 Standard\n- Objects are replicated across at least 3 AZs in the AWS region\n- Data is available within milliseconds\n- Billed GB/m fee for data stored\n- Billed $ per GB for transfer OUT (IN is free) and a price per 1,000 requests\n- No specific retrieval fee\n- No minimum duration\n- No minimum size\n\n### S3 Standard-IA (Infrequent Access)\n- Same specs as S3 Standard, but object storage costs are significantly cheaper\n- New cost component, which is a retrieval fee, in the form of a per GB data retrieval fee.\n- Standard-IA has a minimum duration charge of 30 days - objects can be stored for less, but the minimum billing always applies\n- Standard-IA has a minimum capacity charge of 128KB per object\n\n### S3 One Zone-IA (Infrequent Access)\n- Same specs as S3 Standard-IA, but significantly cheaper\n- One Zone-IA does not provide the multi-AZ resilience model of Standard or Standard-IA. Instead only one AZ is used within the region.\n- Should only be used for non-critical or replaceable data\n\n### S3 Glacier - Instant Retrieval\n- Like S3 Standard-IA with regards to cheaper storage, but with more expensive retrieval and a longer minimum fee\n- Minimum duration charge of 90 days\n- Data is still redundantly stored across 3 AZ\n\n### S3 Glacier - Flexible\n- S3 Glacier objects cannot be made publicly available\n- S3 Glacier objects are not immediately available, you have to initiate a retrieval job\n- Once the retrieval job completes, you'll find your object temporarily stored in the S3 Standard-IA storage class\n- Data is still redundantly stored across 3 AZ\n- Minimum duration charge of 90 days\n- Minimum data size of 40 KB\n- Ideal for situations where archival data is stored\n\n#### Retrieval Job Types\nThe faster the retrieval job, the more expensive it is.\n- Expedited (1-5 minutes)\n- Standard (3-5 hours)\n- Bulk (5-12 hours)\n\n### S3 Glacier Deep Archive\n- 40 KB minimum size charge\n- 180 day minimum duration charge\n\n#### Retrieval Job Types\n- Standard (12 hours)\n- Bulk (up to 48 hours)\n\n### S3 Intelligent Tiering\n- Contains 5 tiers\n\t- Frequent Access - S3 Standard\n\t- Infrequent Access - S3 Standard-IA\n\t- Archive Instant Access - S3 Glacier Instant - Objects move here after 90 days\n\t- Archive Access - S3 Glacier Flexible (optional)\n\t- Deep Archive - S3 Glacier Deep Archive (optional)\n- Intelligent Tiering monitors and automatically moves any objects not accessed for 30 days to a low cost infrequent access tier and eventually to archive instance access, archive access, or deep archive tiers\n- A data tier management fee is applied when using S3 Intelligent Tiering.\n- Only useful if retrieval patterns are changing constantly","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-object-versioning-and-mfa-delete":{"title":"S3 Object Versioning and MFA Delete","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 Object Versioning\n- Controlled at the bucket level\n- This is disabled by default, but once it is enabled, it CANNOT be disabled, but it can be suspended (which is effectively the same thing)\n\t- You will continue to be billed for all Object versions, even when Object Versioning is suspended\n- Without Object Versioning being enabled, each object is solely identified by it's key, which is unique inside the bucket\n\t- If you modify an object, the previous version is deleted\n- Object Versioning allows you to store multiple versions of an object within a bucket\n\t- When an object with KEY = `cat.jpg` is created, it is allocated an id (ex: `11111`). When a new version is created, it will keep the same key, but the new version will be assigned a new ID (ex: `22222`). You can optionally specify an ID when requesting an S3 object. If you don't specify an ID, you'll receive the latest version.\n- How does Object Versioning affect Object Deletion?\n\t- If we indicate to S3 that we want to delete an object, and we don't give a specific version ID, a Delete Market is created.\n\t- A Delete Marker is essentially just a special version of the object that will hide all previous versions of the object\n\t- If the Delete Marker is deleted, all previous versions are now unhidden and queryable once again\n- So how do are objects actually deleted when Object Versioning is enabled?\n\t- When deleting an object, you must specify the version of the object that you want to delete by passing in the version ID.\n- If you notice a significant increase in the number of HTTP 503-slow down responses received for Amazon S3 PUT or DELETE object requests to a bucket that has versioning enabled, you might have one or more objects in the bucket for which there are millions of versions. When you have objects with millions of versions, Amazon S3 automatically throttles requests to the bucket to protect the customer from an excessive amount of request traffic, which could potentially impede other requests made to the same bucket.\n\t- To determine which S3 objects have millions of versions, use [S3 Inventory](/notes/aws/s3/s3-inventory.md). The inventory tool generates a report that provides a flat file list of the objects in a bucket.\n\n## MFA Delete\n- This is enabled within the versioning configuration of an S3 bucket\n- When enabled, MFA is required to change bucket versioning state\n- MFA is also required to fully delete versions\n- To change the bucket versioning state, or fully delete an object version, you must provide the following concatenated value to API calls: `the serial number of your MFA token + the code that MFA generates`","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-presigned-urls":{"title":"S3 PreSigned URLs","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 Presigned URLs\n- Give another person or application access to an object inside of an S3 bucket, using your credentials in a safe and secure way\n- Either you or an automated process can generated a presigned URL, which has specific access permissions encoded within it. These permissions can be for an entire bucket, or a specific object within that bucket. The URL is valid for a certain time period, after which it expires.\n- The end user of the URL acts as the original person who generated the URL\n- When might presigned URLs be used?\n\t- Imagine a S3 bucket, used as a media bucket for a web server.\n\t- Presigned URLs allow for the bucket to be private, and utilize an IAM user (service account) that generates presigned URLs so that the web server can utilize presigned URLs to give private videos to end users.\n\t- The end users in-browser client application can then use the presigned URL to access the private video\n- Oddly enough, You CAN create a URL for an object you have no access to. There is no use from it, but it is possible.\n- When utilizing the URL, the permissions match the identity which generated it\n- Access denied could mean the generating ID never had access .. or doesn't now. You inherit the permissions of the identity RIGHT NOW.\n- This means, you should NEVER generate presigned URLs with an IAM Role! IAM Role temporary credentials will generally expire long before the presigned URL does.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-replication":{"title":"S3 Replication","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 Replication\nS3 has replication features which allow objects to be replicated between a SOURCE and DESTINATION bucket in the same or different AWS accounts.\n- Cross-Region Replication\n- Same-Region Replication\n- Standard Replication Configuration is applied to the SOURCE bucket\n- You can either replicate all objects, or a subnet of objects\n- You can specify the storage class (default is to maintain the same)\n- You can specify AWS account ownership (default is the source account)\n- Replication Time Control - Adds a guaranteed 15 minute replication timeline to keep buckets in sync\n- Replication is not retroactive \u0026 Versioning needs to be ON\n- One-way replication ONLY\n- Replication supports unencrypted objects, SSE-S3 and SS3-KMS\n- NO system events, Glacier, or Glacier Deep Archive\n- NO DELETES are replicated\n- Why might you use S3 Replication?\n\t- SRR - Log Aggregation\n\t- SRR - PROD and TEST env sync\n\t- CRR - Global Resilience Improvements\n\t- CRR - Latency Reduction\n\n#aws #s3 #aws-sysops ","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-security":{"title":"S3 Security","content":"\n### Related Notes\n- [s3](/notes/aws/s3/s3.md)\n\n### Useful Links\n- [Bucket Policy Examples](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html)\n\n## S3 Security\n- S3 is private by default. Everything that is done to control S3 permissions is based upon this starting point\n- The only identity that has any access to an initial S3 bucket, is the account root user of the account which created that bucket. Other permissions must be explicitly granted.\n\n```json\n{\n  \"Version\": \"2012-10-17\"\n\t\"Statement\": [\n\t\t\"Sid\": \"PublicRead\",\n\t\t\"Effect\": \"Allow\",\n\t\t\"Principal\": \"*\", // The principal part of a policy defines who that statement applies to. (which identities, principal)\n\t\t\"Action\": [\"s3:GetObject\"],\n\t\t\"Resource\": [\"arn:aws:s3:::secretcatproject/*]\n\t]\n}\n```\n- A wildcard means that any principal can perform the action on this secret cat project bucket. This means that this applies to all AWS accounts, and even anonymous principals.\n\n### When To Use Which Policy\n- Identity Policies: Controlling different resources\n- Identity Policies: You have a preference for IAM\n- Identity Policies: Same account\n- Bucket Policies: Just controlling S3\n- Bucket Policies: Anonymous or Cross-Account\n- ACLs: NEVER - unless you must\n\n### S3 Permissions\n\n#### S3 Bucket Policy (Resource Policy)\n-  A form of resource policy\n- Like identity policies, but attached to a bucket\n- Permissions from a resource perspective\n- Identity policies have a significant limitation. They can only be attached to identities in your own account. identity policies can only control security in your own account. You have no way of giving an identity in another account access to your S3 bucket.\n- Resource policies, however, allow you to reference any other identity, whether they're in the same account or a separate account.\n\t- This is a major benefit of resource polices\n- Resource policies can also allow or deny anonymous principals.\n\t- This is in contrast to identity policies, which by design, have to be attached to valid identity in AWS.\n\t- Resource policies can be used to open a bucket to the world, by referencing all principals, even those not authenticated by AWS, Anonymous Principals.\n\t\n#### Block Public Access Settings\n- This was added because so many companies don't understand the S3 permission model\n- If enabled, this will override Resource Policies that grant public access\n\n#### Access Control Lists (ACLs) - Legacy\n- ACLs on objects and bucket\n- A subresource of the object or bucket\n- AWS doesn't recommend their usage and recommends identity policies or bucket policies\n\t- They've been replaced because they are inflexible and only allow very simple permissions\n\t\t- For example, they don't support conditions\n- There are just 5 permissions, that applied to either the entire bucket, or a specific bucket\n\t- READ\n\t- WRITE\n\t- READ_ACP\n\t- WRITE_ACP\n\t- FULL_CONTROL\n- ACLs cannot be used on a group of objects","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-select":{"title":"S3 Select","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [S3](/notes/aws/s3/s3.md)\n- [Athena](/notes/aws/athena.md)\n\n## **S3 Select**\n- Amazon S3 Select is designed to help analyze and process data within an object in Amazon S3 buckets, faster and cheaper. It works by providing the ability to retrieve a subset of data from an object in Amazon S3 using simple SQL expressions. Your applications no longer have to use compute resource to scan and filter the data from an object, potentially increasing query performance by up to 400%, and reducing query costs as much as 80%. You simple change your application to use SELECT instead of GET to take advantage of S3 Select.\n- Reduces situations where you have to download massive objects, and incur a large data OUT transfer cost\n- Supports CSV, JSON, Parquet, and BZIP2 compression for CSV and JSON\n- Glacier is also supported by Glacier Select","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/s3/s3-static-website-hosting":{"title":"S3 Static Website Hosting","content":"\n### Related Notes\n- [S3](/notes/aws/s3/s3.md)\n\n## S3 Static Website Hosting\n- Normally, you'd use AWS S3 APIs to access S3\n- However, when S3 Static Website Hosting is enabled, you can access S3 via standard HTTP\n\t- This allows you to host almost anything\n- When enabling, you must set an Index (default page) and Error (default, when something goes wrong) HTML document\n- When enabled, a Website Endpoint is created. This name is automatically generated.\n- Custom Domain via R53 - Bucket Name matters in this case, it needs to match domain name\n\n### Other Use Cases: Offloading\n- Hosting static media for your websites to take pressure off of your compute service\n- Cheaper for storage and delivery of static assets\n\n### Other Use Cases: Out-of-band pages\n- When scheduled maintenance is happening, and your server is down, customers can be pointed at an out of band page until the server is back up\n\n### Costs\n- When S3 Static Website Hosting is enabled, you're charged a certain amount for requests","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/secrets-manager":{"title":"Secrets Manager","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## Secrets Manager","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/simple-notification-service":{"title":"Simple Notification Service","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Simple Notification Service (SNS)**\n- SNS is a fully managed pub/sub messaging service. With Amazon SNS, you can use topics to simultaneously distribute messages to multiple subscribing endpoints such as Amazon SQS queues, AWS Lambda functions, HTTP endpoints, email addresses, and mobile devices (SMS, Push)\n- Messages in an SQS queue will continue to exist even after the EC2 instance has processed it, until you delete that message. You have to ensure that you delete the message after processing to prevent the message from being received and processed again once the visibility timeout expires.\n- A fanout scenario occurs when a message published to an SNS topic is replicated and pushed to multiple endpoints, such as Amazon SQS queues, HTTP(S) endpoints, and Lambda functions. This allows for parallel asynchronous processing. For example, you can develop an application that publishes a message to an SNS topic whenever an order is placed for a product. Then, 2 or more SQS queues that are subscribed to the SNS topic receive identical notifications for the new order An EC2 server instance attached to one of the SQS queues can handle the processing or fulfillment of the order. And you can attach another EC2 server instance to a data warehouse for analysis of all orders received. By default, an SNS topic receives every message published to the topic. You can use SNS message filtering to assign a filter policy to the topic subscription, and the subscriber will only receive a message that they are interested in. Using SNS and SQS together, messages can be delivered to applications that require immediate notification of an event. This method is known as fanout to Amazon SQS queues.\n- If you are after parallel asynchronous processing, you'll need to fan-out messages to multiple SQS queues, using a filter policy in SNS subscriptions to allow parallel asynchronous processing. If you don't set a filter policy in SNS, the subscribers would receives all the messages published to the SNS topic.\n- A \"fanout\" pattern is when an Amazon SNS message is sent to a topic and then replicated and pushed to multiple Amazon SQS queues, HTTP endpoints, or email addresses. This allows for parallel asynchronous processing. For example, you could develop an application that sends an Amazon SNS message to a topic whenever an order is placed for a product. Then, the Amazon SQS queues that are subscribed to that topic would receive identical notifications for the new order. The Amazon EC2 server instance attached to one of the queues could handle the processing or fulfillment of the order, while the other server instance could be attached to a data warehouse for analysis of all orders received.\n\t- When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Because Amazon SQS is a distributed system, there's no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application.) Thus, the consumer must delete the message from the queue after receiving it and successfully processing it.\n\t- Immediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a _visibility timeout_, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/simple-queue-service":{"title":"Simple Queue Service","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Simple Queue Service** (Amazon SQS) \n- Amazon SQS is a message queue service used by distributed applications to exchange messages through a polling model. It can be used to decouple sending and receiving components without requiring each component to be concurrently available.\n- SQS offers reliable, highly-scalable hosted queues for storing messages while they travel between applications or micro-services. SQS lets you move data between distributed applications components and helps you decouple these components. Amazon SWF is a web service that makes it easy to coordinate work across distributed application components.\n- Standard queues provide at-least-once delivery, which means that each message is delivered at least once. Standard queues do not, however, preserve the order of messages. This feature only applies to FIFO queues.\n- SQS uses short polling by default, querying only a subset of of the servers (based on a weighted random distribution) to determine whether any messages are available for inclusion in the response. Short polling works for scenarios that require higher throughput. However, you can also configure the queue to use long polling, to reduce cost.\n- Take note that you cannot assign a priority to individual items in an SQL queue. If you want higher priority requests, you'll have to implement 2 separate SQS queues, side by side. One for premium members, that is polled first, and one for free members, which is polled if the premium queue is empty.\n- Pairs well with Lambda in serverless stacks. Lambda supports the synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke a Lambda function. When you use an AWS service as a trigger, the invocation type is predetermined for each service.\n- You can use an AWS Lambda function to process messages in an Amazon Simple Queue Service (Amazon SQS) queue. Lambda event source mappings support standard queues and first-in, first-out (FIFO) queues. With Amazon SQS, you can offload tasks from one component of your application by sending them to a queue and processing them asynchronously.\n- Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days.\n### **Consuming messages using short polling**:\n- When you consume messages from a queue using short polling, Amazon SQS samples a subset of its servers (based on a weighted random distribution) and returns messages from only those servers. Thus, a particular ReceiveMessage request might not return all of your messages. However, if you have fewer than 1,000 messages in your queue, a subsequent request will return your messages. If you keep consuming from your queues, Amazon SQS samples all of its servers, and you receive all of your messages.\n- The following diagram shows the short-polling behavior of messages returned from a standard queue after one of your system components makes a receive request. Amazon SQS samples several of its servers (in gray) and returns messages A, C, D, and B from these servers. Message E isn't returned for this request, but is returned for a subsequent request.\n- Short polling occurs when the WaitTimeSeconds parameter of a ReceiveMessage request is set to 0 in one of two ways:\n\t- The `ReceiveMessage` call sets `WaitTimeSeconds` to 0.\n\t- The `ReceiveMessage` call doesn't set `WaitTimeSeconds`, but the queue attribute `ReceiveMessageWaitTimeSeconds` is set to 0.\n### **Consuming messages using long polling**:\n- When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). For information about enabling long polling for a new or existing queue using the Amazon SQS console, see the Configuring queue parameters (console). For best practices, see Setting up long polling.\n- Long polling offers the following benefits:\n\t- Long polling helps reduce your cost of using Amazon SQS by reducing the number of empty responses when there are no messages available in the queue before sending a response. Unless the connection times out, the response to the `ReceiveMessage` request contains at least one of the available messages, up to the maximum number of messages specified in the `ReceiveMessage` action.\n\t- Reduce empty responses by allowing Amazon SQS to wait until a message is available in a queue before sending a response. Unless the connection times out, the response to the ReceiveMessage request contains at least one of the available messages, up to the maximum number of messages specified in the ReceiveMessage action. In rare cases, you might receive empty responses even when a queue still contains messages, especially if you specify a low value for the ReceiveMessageWaitTimeSeconds parameter.\n\t- Reduce false empty responses by querying all—rather than a subset of—Amazon SQS servers.\n\t- Return messages as soon as they become available.\n### ApproximateNumberOfMessages Metric\n- The `ApproximateNumberOfMessages` metric in Amazon CloudWatch can scale out EC2 instances in your decoupled application components.\n- The number of messages in your Amazon SQS queue does not solely define the number of instances needed. In fact, the number of instances in the fleet can be driven by multiple factors, including how long it takes to process a message and the acceptable amount of latency (queue delay).\n- The solution is to use a *backlog per instance* metric with the target value being the *acceptable backlog per instance* to maintain. You can calculate these numbers as follows:\n\t- **Backlog per instance**: To determine your backlog per instance, start with the SQS metric `ApproximateNumberOfMessages` to determine the length of the SQS queue (number of messages available for retrieval from the queue). Divide that number by the fleet's running capacity, which for an Auto Scaling group is the number of instances in the `InService` state, to get the backlog per instance.\n\t- **Acceptable backlog per instance**: To determine your target value, first calculate what your application can accept in terms of latency. Then, take the acceptable latency value and divide it by the average time that an EC2 instance takes to process a message.\n- Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days\n\n### **ApproximateAgeOfOldestMessage** Metric\n- The `ApproximateAgeOfOldestMessage` metric is useful when applications have time-sensitive messages and you need to ensure that messages are processed within a specific time period. You can use this metric to set Amazon CloudWatch alarms that issue alerts when messages remain in the queue for extended periods of time. You can also use alerts to take action, such as increasing the number of consumers to process messages more quickly. With a target tracking scaling policy, you can scale (increase or decrease capacity) a resource based on a target value for a specific CloudWatch metric. To create a custom metric for this policy, you need to use AWS CLI or AWS SDKs. Take note that you need to create an AMI from the instance first before you can create an Auto Scaling group to scale the instances based on the `ApproximateAgeOfOldestMessage` metric.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/site-to-site-vpn":{"title":"AWS Site-to-Site VPN","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [Direct Connect](/notes/aws/direct-connect.md)\n\n## AWS Site-to-Site VPN\n- Offers the quickest way to create a network link between an AWS environment and an on-premises environment, or another cloud.\n- A logical connection between a VPC and on-premises network encrypted using IPSec, running over the **public internet**.\n- Highly available, if designed and implemented correctly\n- Quick to provision, in less than an hour (in constrast to the long provisioning times of Direct Connect)\n- Speed Limitations: 1.25 Gbps\n- Latency Considerations: inconsistent, because it travels over the public internet (Look into Direct Connect, if this is an issue)\n- Cost - AWS hourly cost, GB out cost, data cap (on-premises)\n- Speed of setup - hours .. all software configuration\n- Can be used as a backup for Direct Connect\n\n### Virtual Private Gateway (VGW)\n- Logical gateway object within AWS, which can be the target on one or more route tables\n- You create and associate it with a single VPC\n- Actually has physical endpoints with 2 public IPv4 addresses, each in different availability zones (Highly Available, by design)\n\t\n### Customer Gateway (CGW)\n- This can refer to two different things\n\t- The logical configuration entity in AWS\n\t- The physical device that this logical configuration represents\n\t\n### Static VPN Connection (between the VGW and CGW)\n- When a VPN is created in the AWS Public Zone, you need to link it to a VGW. This process links the VPN to both physical endpoints of the VGW.\n- The VPN also needs to be linked to the CGW. This allows 2 VPN tunnels to be created, which flow from the physical endpoints of the VGW to the singular CGW. (At this state, the VPN design is only partially highly available. If the availability zone housing one of the physical endpoints in AWS goes down, traffic will still flow through the other VPN tunnel. But if the CGW goes down, traffic will be halted).\n- A VPN tunnel is an encrypted channel through which data can flow through the VPC to the on-premises network, or vice-versa.\n- To resolve single point of failure, we just need to add another on-premises customer route, using a separate internet connection.\n\t- If this is done, it requires creating a second VPN connection on the AWS side, which will create an additional 2 physical endpoints on the same VGW, bringing the total up to 4 physical endpoints altogether, managed by the VGW.\n\t- This achieves high availability\n\n### Dynamic VPN Connection\n- Uses the Border Gateway Protocol\n- The main difference is how routes are communicated\n- A static VPN uses static network configuration. Static routes are added to the route tables, and networks for remote side statically configured on the VPN connection. But, there's no load balancing or multi-connection failover.\n- With Dynamic VPNs, BGP is configured on both the customer and AWS side using ASN. Networks are exchanged via BGP. Multiple VPN connections provide HA and traffic distribution. With Dynamic VPNs, you can still add static routes, or you can enable route propagation, which allows routes to be dynamically learned by the route table.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/storage-gateway":{"title":"Storage Gateway","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Storage Gateway** (*integrates*)\n- AWS Storage Gateway connects an on-premises software appliance with cloud-based storage to provide seamless integration with data security features between your on-premises IT environment and the AWS storage infrastructure. You can use the service to store data in the AWS Cloud for scalable and cost-effective storage that helps maintain data security.\n- A file gateway supports a file interface into S3 and combines a service and a virtual software appliance. By using this combo, you can store and retrieve objects in S3 using industry-standard file protocols such as **Network File System (NFS)** and **Server Message Block (SMB)**. The software appliance, or gateway, is deployed into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or Linux Kernel-based Virtual Machine (KVM) hypervisor.\n- Although you can copy data from on-premises to AWS with AWS Storage Gateway, it is not suitable for transferring large sets of data to AWS. Storage Gateway is mainly used in providing low-latency access to data by caching frequently accessed data on-premises while storing archive data securely and durably in Amazon cloud storage services. Storage Gateway optimizes data transfer to AWS by sending only changed data and compressing data.\n- You can use Amazon FSx File Gateway, as a part of AWS Storage Gateway, to support an SMB file share for an on-premises applications. It provides low latency access, and Amazon FSx File Gateway helps accelerate your file-based storage migration to the cloud to enable faster performance, improved data protection, and reduced cost.\n- AWS File Gateway is another part of AWS Storage Gateway, which presents a file-based interface to Amazon S3, which appears as a network file share. It enables you to store and retrieve Amazon S3 objects through standard file storage protocols. File Gateway allows your existing filed-based applications or devices to use secure and durable cloud storage without needing to be modified. With File Gateway, your configured S3 buckets will be available as Network File System (NFS) mounts points or Server Message Block (SMB) file shares. To store the backup data from on-premises to a durable cloud storage service, you can use File Gateway to store and retrieve objects through standard file storage protocols (SMB or NFS). File Gateway enables your existing file-based applications, devices, and workflows to use Amazon S3, without modification. File Gateway securely and durably stores both file contents and metadata as objects while providing your on-premises applications low-latency access to cached data.\n- AWS Volume Gateway is another part of AWS Storage Gateway. You can use Volume Gateways in stored mode or cached mode. Cached mode provides lower latency. By using cached volumes, you can use Amazon S3 as your primary data storage, while retaining frequently accessed data locally in your storage gateway. Cached volumes minimize the need to scale your on-premises storage infrastructure, while still providing your applications with low-latency access to frequently accessed data. You can create storage volumes up to 32 TiB in size and afterward, attach these volumes as iSCSI devices to your on-premises application servers. When you write to these volumes, your gateway stores the data in Amazon S3. It retains the recently read data in your on-premises storage gateway cache and uploads buffer storage.\n- The AWS Storage Gateway Hardware Appliance is a physical hardware appliance with the Storage Gateway software preinstalled on a validated sever configuration. The hardware appliance is a high-performance 1U server that you can deploy in your data center, or on-premises inside your corporate firewall. When you buy and activate your hardware appliance, the activation process associates your hardware appliance with your AWS account.\n- If new Storage should be available as iSCSI target, choose Storage Gateway. ","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/systems-manager":{"title":"Systems Manager","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Systems Manager**\n\n### Helpful Links\n- [Systems Manager Pricing](https://aws.amazon.com/systems-manager/pricing/)\n\n### Overview\n- AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. A managed instance is any Amazon EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. Run Command enables you to automate common administrative tasks and perform ad-hoc configuration changes at scale. You can use Run Command from the AWS console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost.\n\n- Allows you to view and control AWS and on-premises infrastructure\n- Especially helpful for hybrid environments\n- Agent based - installed on windows and Linux AWS AMI's\n\t- Handled manually on on-premises environments\n- Manages Inventory on instances that have the agent installed\n\t- What applications are installed\n\t- The files on the instance\n\t- The Network configuration\n\t- Operating System patches and hotfixes\n\t- Running services on the instance\n\t- Instance hardware details\n\t- Even allows custom configuration\n- Patching Automation\n  - Defines maintenance windows\n\t- Automatic patching\n- Runs commands and managed desired state\n- Securely connect to EC2, even in private VPCs\n\n### SSM configuration requirements for AWS fleet\n- Agent installed on each AWS instance\n- Connectivity to the AWS Public Zone endpoint (Systems Manager Endpoint)\n- IAM role providing permissions\n\n### SSM configuration for on-premises fleet\n- Managed instance activation: Process of adding on-premises servers to SSM\n\t- Activation Code\n\t- Activation ID\n\t- IAM Role\n- Activations securely join on-premises servers to Systems Manager and configure the IAM Role to use\n\n### SSM Run Command\n- In short, run any command you can define, across your entire fleet\n- Also, it supports other Systems Manager features, like Patch Management\n- Allows you to run [Command Documents](https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-ssm-docs.html) on managed instances\n- No SSH/RDP access required\n- Instances, Tags, or Resource Groups\n- Command Documents define what steps a command has\n\t- Run Shell commands\n\t- Joining machine to a domain\n\t- Anything you can define\n- Command Documents can be reused and can have parameters\n- Rate Control allows you to control the following:\n\t- Concurrency (how many instances should you run the command on at a single time)\n\t- Error Threshold (how many individual commands running on individual instance can fail, before the whole command fails)\n- Output Options - S3 or SNS\n- EventBridge (CloudWatch Events) Targets\n\n### SSM Documents\n- JSON or YAML documents, similar to CloudFormation Templates, but for configuration specific to Systems Manager itself\n- Systems Manager includes more than 100 pre-configured documents that you can use by specifying parameters at runtime\n- Stored in a place called the `SSM Document Store`\n- Ask for Parameters and include steps\n- SSM Documents includes Command Documents - used by Run Command, State Manager \u0026 Maintenance WIndows\n- Automation Documents - used by Automation, State Manager \u0026 Maintenance Windows\n\t- Used for creating/updating AMI's, for example\n- Package Documents - Distributor\n\t- Represents software or any other assets installed on instances\n\t- For example, there exists a commonly used SSM Document called `AWS-ConfigureAWSPackage`, which is commonly used to install the CloudWatch Agent on SSM managed EC2 instances\n\n### SSM Inventory \u0026 SSM Patching\n#### Patch Manager\nPatch Manager, a capability of AWS Systems Manager, automates the process of patching managed instance with both security related and other types of updates\n- Patch Baseline\n\t- Defines what should be installed\n\t\t- What patches, and what hotfixes\n- Patch Groups\n\t- Which specific resources should be patched?\n- Maintenance Windows\n\t- Define the time slot when patching can take place\n- Run Command - Used as the base level functionality to manage the patching process, actually performs the process\n- Concurrency and Error Threshold\n\t- Concurrency (how many instances should you run the command on at a single time)\n\t- Error Threshold (how many individual commands running on individual instance can fail, before the whole command fails)\n- Compliance: Systems Manager can determine after the fact, whether a patch has been applied successfully\n\n#### Patch Manager - Key Terms to Memorize for #aws-sysops #sysops\n- Predefined Patch Baselines - Various OS (you can also create your own) \n- For Linux - AWS-OSDefaultPatchBaseline, explicitly define patches\n- .. AWS-AmazonLinux2DefaultPatchBaseline\n- .. AWS-UbuntuDefaultPatchBaseline\n- Windows - AWS-DefaultPatchBaseline - Critical and Security Updates\n- AWS-WindowsPredefinedPatchBaseline-OS - Same as above\n- AWS-WindowsPredefinedPatchBaseline-OS-Applications = + MS App Updates\n- `AWS-RunPatchbaseline` is the Run Command that runs with a baseline and target to actually patch the machines\n\n### SSM Parameter Store\n- Stores configuration and secrets\n\n### Session Manager\n- Session Manager is a fully managed AWS Systems Manager capability. With Session Manager, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, and on-premises servers and virtual machines (VMs). You can use either an interactive one-click browser-based shell or the AWS Command Line Interface (AWS CLI). Session Manager provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. Session Manager also allows you to comply with corporate policies that require controlled access to managed nodes, strict security practices, and fully auditable logs with node access details, while providing end users with simple one-click cross-platform access to your managed nodes. To get started with Session Manager, open the [Systems Manager console](https://console.aws.amazon.com/systems-manager/session-manager). In the navigation pane, choose **Session Manager**.\n\n### Systems Manager Inventory\n- Collects a list of software running on managed instances\nAWS Systems Manager Inventory provides visibility into your AWS computing environment. You can use Inventory to collect _metadata_ from your managed nodes. You can store this metadata in a central Amazon Simple Storage Service (Amazon S3) bucket, and then use built-in tools to query the data and quickly determine which nodes are running the software and configurations required by your software policy, and which nodes need to be updated. You can configure Inventory on all of your managed nodes by using a one-click procedure. You can also configure and view inventory data from multiple AWS Regions and AWS accounts.To get started with Inventory, open the [Systems Manager console](https://console.aws.amazon.com/systems-manager/inventory). In the navigation pane, choose **Inventory**.\n\nIf the pre-configured metadata types collected by Systems Manager Inventory don't meet your needs, then you can create custom inventory. Custom inventory is simply a JSON file with information that you provide and add to the managed node in a specific directory. When Systems Manager Inventory collects data, it captures this custom inventory data. For example, if you run a large data center, you can specify the rack location of each of your servers as custom inventory. You can then view the rack space data when you view other inventory data.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/transit-gateway":{"title":"Transit Gateway","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [Direct Connect](/notes/aws/direct-connect.md)\n\n## **Transit Gateway**\n- The AWS Transit Gateway is a network gateway which can be used to significantly simplify networking between VPC's, VPN, and Direct Connect\n- It can be used to peer VPCs in the same account, different account, same or different region and supports transitive routing between networks\n- Network Transit Hub to connect VPCs to on-premises networks\n- Significantly reduces network complexity, especially as you scale, reduces number of VPN tunnels\n- Single network object - HA and Scalable\n- Highly available inter-VPC router, allows VPCs to talk to each other through the Transit Gateway\n- Support Transitive Routing\n- Share between accounts using AWS RAM\n- Transit Gateways can also be peered with Transit Gateways in other regions or cross account\n\t- This can be used to create a global network within AWS\n- Creates attachments to other network types\n\t- VPC\n\t- Site-to-Site VPN\n\t- Direct Connect Gateway\n- AWS Transit Gateway is a service that enables customers to connect their VPC Clouds and their on-premises networks to a single gateway. As you grow the number of workloads on AWS you need to be able to scale your networks across multiple accounts and Amazon VPCs to keep up with the growth. Today, you can connect parts of Amazon VPCs without peering. However, managing point-to-point connectivity across many Amazon VPCs without the ability to centrally manage the connectivity policies can be operationally costly and cumbersome. For on-premises connectivity you need to attach your AWS VPN to each individual Amazon VPC. This solution can be time-consuming to build and hard to manage when the number of VPCs grows into the hundreds. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway to each Amazon VPC, on-premises data center, or remote office across your network. Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks which act like spokes. This hub and spoke model significantly simplifies management and reduces operational costs because each network only has to connect to the Transit Gateway and not to every other network. Any new VPC is simply connected to the Transit Gateway and is then automatically available to every other network that is connected to the Transit Gateway. This ease of connectivity makes it easy to scale your network as you grow.\n- AWS Transit Gateway provides a hub and spoke design for connecting VPCs and on-premises networks. You can attach all your hybrid connectivity (VPN and Direct Connect connections) to a single Transit Gateway consolidating and controlling your organization's entire AWS routing configuration in one place. It also controls how traffic is routed among all the connected spoke networks using route tables. This hub and spoke model simplifies management and reduces operational costs because VPCs only connect to the Transit Gateway to gain access to the connected networks.\n- By attaching a transit gateway to a Direct Connect gateway using a transit virtual interface, you can manage a single connection for multiple VPCs or VPNs that are in the same AWS Region. You can also advertise prefixes from on-premises to AWS and from AWS to on-premises.\n- The AWS Transit Gateway and AWS Direct Connect solution simplify the management of connections between an Amazon VPC and your networks over a private connection. It can also minimize network costs, improve bandwidth throughput, and provide a more reliable network experience than Internet-based connections.\n- AWS Transit Gateway simplifies your network and puts an end to complex peering relationships.\n- How to improve the speed of slow Site-to-Site VPN connections? Associate the VPCs to an **Equal Cost Multipath Routing (ECMR)-enabled transit gateway** and attach additional VPN tunnels.\n- With AWS Transit Gateway, you can simplify the connectivity between multiple VPCs and also connect to any VPC attached to AWS Transit Gateway with a single VPN connection.\n- AWS Transit Gateway also enables you to scale the IPsec VPN throughput with equal-cost multi-path (ECMP) routing support over multiple VPN tunnels. A single VPN tunnel still has a maximum throughput of 1.25 Gbps. If you establish multiple VPN tunnels to an ECMP-enabled transit gateway, it can scale beyond the default limit of 1.25 Gbps.\n","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/trusted-advisor":{"title":"Trusted Advisor","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Trusted Advisor**\n- Account Level product (The service isn't free, but includes a free version with 7 core checks for basic and developer support)\n- AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices. It inspects your AWS environment and makes recommendations for saving money, improving system reliance and reliability, or closing security gaps.\n- Trusted Advisor includes an ever-expanding list of checks in the following five categories:\n\t- **Cost Optimization** – recommendations that can potentially save you money by highlighting unused resources and opportunities to reduce your bill.\n\t- **Security** – identification of security settings that could make your AWS solution less secure.\n\t- **Fault Tolerance** – recommendations that help increase the resiliency of your AWS solution by highlighting redundancy shortfalls, current service limits, and over-utilized resources.\n\t- **Performance** – recommendations that can help to improve the speed and responsiveness of your applications.\n\t- **Service Limits** – recommendations that will tell you when service usage is more than 80% of the service limit.\n\nFree Version Core Checks\n- S3 Bucket Permissions\n- Security Groups - Specific Ports Unrestricted\n- IAM Use\n- MFA on Root Account\n- EBS Public Snapshots\n- RDS Public Snapshots\n- 50 service limit checks\n\nBusiness \u0026 Enterprises support plans contains 115 further checks\n- 14 cost\n- 17 security\n- 24 fault tolerant\n- 10 performance\n- 50 sevice limit\n- Access via the AWS Support API to initiate checks and programmatically request support whenever required\n\t\n","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/vpc":{"title":"AWS VPC","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **Virtual Private Cloud (AWS VPC)**\n- Note: /32 after an IP address denotes one IP address, while /0 refers to the entire network.\n- What is the best way to block a suspicious IP address from attempting to access the resources inside of your VPC through port scans?\n\t- Modify the Network Access Control List (NACL) associated with all public subnets in the VPC to deny access from the IP Address block.\n\t- **NOTE**: This is something that you would *never* be able to do from an IAM policy, because an IAM policy does not control the inbound and outbound traffic of your VPC.\n- A VPC spans all the Availability Zones in the region. After creating a VPC, you can add one or more subnets in each Availability Zone. When you create a subnet, you specify the CIDR block for the subnet, which is a subset of the VPC CIDR block. Each subnet must reside entirely within one Availability Zone and cannot span zones. Availability Zones are distinct locations that are engineered to be isolated from failures in other Availability Zones. By launching instances in separate Availability Zones, you can protect your applications from the failure of a single location.\n- **Route Tables**\n\t- Your VPC has an implicit router and you use route tables to control where network traffic is directed. Each subnet in your VPC must be associated with a route table, which controls the routing for the subnet (subnet route table). You can explicitly associate a subnet with a particular route table. Otherwise, the subnet is implicitly associated with the main route table.\n\t- A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same subnet route table. You can optionally associate a route table with an internet gateway or a virtual private gateway (gateway route table). This enables you to specify routing rules for inbound traffic that enters your VPC through the gateway.\n\t- Be sure that the subnet route table also has a route entry to the internet gateway. If this entry doesn't exist, the instance is in a private subnet and is inaccessible from the internet.\n\t- In cases where your EC2 instance cannot be accessed from the Internet (or vice versa), you usually have to check 2 things:\n\t\t- Does it have an EIP or public IP address?\n\t\t- Is the route table properly configured?\n- You cannot have a VPC with IPv6 CIDRs only. The default IP addressing system in VPC is IPv4. You can only change your VPC to dual-stack mode where your resources can communicate over IPv4, or IPv6, or both, but not exclusively with IPv6 only.\n\n### VPC Flow Logs\n- Capture metadata (NOT CONTENTS)\n\t- Source IP, Destination IP, Source Port, Destination Port, Packet Size\n- Can be attached at different levels of a VPC (Flow logs capture metadata from the capture point down)\n\t- Attached to a VPC - All ENIs in that VPC \n\t- Subnet - All ENIs in that Subnet\n\t- ENIs directly\n- Flow Logs are NOT realtime\n- Log Destinations ... S3 or CloudWatch Logs\n- ... or Athena for querying...\n- Flow Logs can capture ACCEPTED, REJECTED or ALL metadata\n\n### **Gateway and Interface VPC Endpoints**\n- A **VPC Endpoint** enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.\n\t- When you create a VPC Endpoint, you can attach an endpoint policy that controls access to the service to which you are connecting. You can modify the endpoint policy attached to your endpoint and add or remove the route tables used by the endpoint. An endpoint policy does not override or replace IAM user policies or service-specific policies (such as S3 bucket policies.) It is a separate policy for controlling access from the endpoint to the specified service. You can use a bucket policy or an endpoint policy to allow traffic to trusted S3 buckets. However, it takes a lot of time to configure a bucket policy for each S3 bucket instead of using a single endpoint policy. Therefore, endpoint policies are the best approach to control the traffic to trusted Amazon S3 buckets.\n- There are 2 types of VPC endpoints: *interface endpoints* and *gateway endpoints*. You have to create the type of VPC endpoint required by the supported service.\n\t- An **interface endpoint** is an elastic network interface with a private IP address that serves as an entry point for traffic destined to a supported service. An interface endpoint is billed for hourly usage and data processing charges.\n\t- A **gateway endpoint** is a gateway that is a target for a specific route in your route table, used for traffic destined to a supported AWS service. You won't get billed if you use a Gateway VPC endpoint for your S3 bucket.\n\n### Cost - VPC is free\nThere are no additional charges for creating and using the VPC itself. VPC is free! Usage charges for other Amazon Web Services, including Amazon EC2, still apply at published rates for those resources, including data transfer charges.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/vpc-gateway-endpoints":{"title":"VPC Gateway Endpoints","content":"\n### Related Notes:\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [Virtual Private Cloud (VPC)](/notes/aws/vpc.md)\n- [VPC Interface Endpoints](/notes/aws/vpc-interface-endpoints.md)\n\n## VPC Endpoints (Gateway Endpoints)\n- Provides private access to supported AWS services like S3 and DynamoDB (public services)\n- Allows a private-only resource inside of a VPC to access S3 and DynamoDB\n- Allows any resource inside a private-only VPC to access S3 and DynamoDB.\n- Normally, to achieve the same functionality granted by VPC Endpoints you would need to create an [Internet Gateway](/notes/aws/internet-gateway.md) and attach it to a VPC. Then, you would have to grant the resource in that VPC a public IPv4 address or IPv6 address (always public), or implement one or more NAT Gateways which allow instance with private IP address to access these public services.\n- Gateway Endpoint allows you to provide access to these services without implementing the above public infrastructure.\n- Gateway Endpoints are created per service, per region and can be associated with one or more subnets in a particular VPC.\n- Gateway Endpoints are #highly-available across all availability zones in a region by default. No need to worry about AZ placement.\n- Gateway Endpoints do not actually go into a particular VPC subnets or availability zone, when you allocate the Gateway Endpoint to a particular subnet, a prefix list is added to the Route Tables for those subnets. The prefix list is a logical object that represents the services (DynamoDB, S3). The prefix list is the destination and the target is the Gateway Endpoint.\n- When configuring a Gateway Endpoint, Endpoint Policies are used to control what it can access.\n  - Allows you to, for example, only allow your Gateway Endpoint to connect to a particular subset of S3 Buckets.\n- Gateway Endpoints can only be used to access services in the same region\n- S3 buckets can be set to private only by allowing access ONLY from a Gateway Endpoint\n- Gateway Endpoints can ONLY be accessed from within the VPC they have been created to serve.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/vpc-interface-endpoints":{"title":"VPC Interface Endpoints","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [VPC Gateway Endpoints](/notes/aws/vpc-gateway-endpoints)\n\n## VPC Endpoints (Interface Endpoints)\n\n- Similar to [VPC Gateway Endpoints](/notes/aws/vpc-gateway-endpoints), Interface Endpoints allow private access to AWS Public Services.\n- Supports all services, excluding DynamoDB.\n- Interface endpoints are NOT highly available, they are added to a specific subnets within a VPC.\n\t- 1 subnet == 1 Availability Zone\n- Network access is controlled via Security Groups.\n- Supports Endpoint Policies to restrict what can be done with the endpoint.\n- Currently ONLY supports TCP and IPv4.\n- Uses [[privatelink]] to inject 3rd party applications or services into private VPC subnets.\n- Uses DNS and a private IP\n- Interface Endpoints provide a NEW service endpoint DNS\n\t- e.g. vpce-123-xyz.sns.us-east-1.vpce.amazonaws.com\n\t\t- Resolves to the private IP address of the Interface Endpoint and allows access to SNS without public IP addressing\n- PrivateDNS overrides the default DNS for services, so that even for applications that have not been re-configured to use the Interface Endpoint-specific DNS can still access the desired AWS service without an issue.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/vpc-peering":{"title":"VPC Peering","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [VPC](/notes/aws/vpc.md)\n\n## VPC Peering\n- A service that lets you create a direct, private, encrypted network link between 2 VPCs. No more than 2.\n- Works same/cross-region and same/cross-account\n-  Public Hostnames resolve to private IPs (optional)\n- Same region Security Group's can reference peer Security Groups (only works in the same region) using Security Group IDs\n- VPC Peering does NOT support transitive peering (Connecting VPC A to VPC C by going through an intermediary VPC A -\u003e B -\u003e C would not work, you would need to setup VPC Peering between A -\u003e C)\n- VPC Peering connections cannot be created where there is overlap in the VPC CIDRs - ideally NEVER use the same address ranges in multiple VPCs\n\n---\n\n- A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them privately. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, with a VPC in other AWS account, or with a VPC in a different AWS Region. AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck. This could allow you to do something like, push minor code releases from a DEV VPC to a PROD VPC to speed up time to market.\n\n- A VPC peering connection does not support edge to edge routing. This means that if either VPC in a peering relationship has one of the following connections, you cannot extend the peering relationship to that connection:\n\t- A VPN connection or an AWS Direct Connect connection.\n\t- An Internet connection through an Internet gateway.\n\t- An Internet connection in a private subnet through a NAT device.\n\t- A gateway VPC endpoint to an AWS service; for example, an endpoint to Amazon S3.\n\t- (IPv6) A ClassicLink connection. You can enable IPv4 communication between a linked EC2-Classic instance and instances in a VPC on the other side of a VPC peering connection. However, IPv6 is not supported in EC2-Classic, so you cannot extend this connection for IPv6 communication.\n\t\n## #sysops Scenarios\n- *QUESTION*: A media company has two VPCs: VPC-1 and VPC-2 with peering connection between each other. VPC-1 only contains private subnets while VPC-2 only contains public subnets. The company uses a single AWS Direct Connect connection and a virtual interface to connect their on-premises network with VPC-1. How could you increase the fault tolerance of the connection to VPC-1?\n- *ANSWER*: Establish a hardware VPN over the Internet between VPC-1 and the on-premises network.\n- *ANSWER*: Establish another AWS Direct Connect connection and private virtual interface in the same AWS region as VPC-1.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/vpc-router":{"title":"VPC Router","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n- [VPC](/notes/aws/vpc.md)\n- [Internet Gateway](/notes/aws/internet-gateway.md)\n\n## VPC Router\n- Every VPC has a VPC Router - Highly available\n- In every subnet .. 'network+1' address\n- Routes traffic between subnets\n- Controlled by 'Route Tables', each subnet has one\n- A VPC has a Main route table - subnet default\n\n### Route Tables\n- A Route Table controls what happens to data as it leaves the subnet or subnets that that Route Table is associated with.\n- Route Tables are a list of routes, attached to 0 or more subnets.\n- A subnet has to have a route table, which is either the main route table of the VPC, or a custom one that you've created.\n- Local routes are always there, uneditable and match the VPC IPv4 or IPv6 CIDR Range. For anything else, higher prefix values that are more specific, take priority.\n- Each entry on a route table has a Destination and Target\n- If the target is local, that means that the route is to the VPC itself\n- All route tables have at least one route, the local route. The local route matches the VPC CIDR range.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/vpn-cloudhub":{"title":"VPN CloudHub","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **AWS VPN CloudHub**\n- Capable of wiring multiple AWS Site-to-Site VPN connections together on a virtual private gateway. This is useful if you want to enable communication between different remote networks that uses a Site-to-Site VPN connection.\n- A Note About VPNs: Although it is true that a VPN provides a cost-effective, hybrid connection from your VPC to your on-premises data centers, it certainly does not bypass the public Internet. A VPN connection actually goes through the public Internet, unlike the AWS Direct Connect connection which has a direct and dedicated connection to your on-premises network.\n- A VPN connection is not capable of providing consistent and dedicated access to on-premises network services. Use Direct Connect for hybrid applications.\n- You can connect your VPC to remote networks by using a VPN connection which can be IPSec VPN connection, AWS VPN CloudHub, or a third party software VPN appliance. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/aws/workdocs":{"title":"WorkDocs","content":"\n### Related Notes\n- [AWS Wiki](/notes/aws/aws-wiki.md)\n\n## **WorkDocs**\n- Amazon WorkDocs is a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content. And because it's stored centrally on AWS, you can access it from anywhere on any device.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/bash":{"title":"Bash","content":"\n## Helpful Links\n- [Bash Hacker's Wiki](https://wiki.bash-hackers.org/)\n- [Wooledge's Wiki](https://mywiki.wooledge.org/BashFAQ)\n- [Shell Script Best Practices, from a decade of scripting things](https://sharats.me/posts/shell-script-best-practices/)\n\n## Infinite Loop\nBecause sometimes it's just necessary to have an infinite loop.\n\n```bash\n#!/bin/bash\n\nwhile true\ndo\n\techo \"Running some command\"\n\tsleep 30 # sleep for 30 seconds\ndone\n```\n\n## Auto-complete Shell script name via Terminal\n1. Write your script. This is a random example script that just performs some basic input validation, and then runs a jar.\n```bash\n#!/bin/sh\n\narg1=$1\narg2=$2\n\n## Directory where jar file is located    \ndir=/directory-path/to/jar-file/\n\n## Jar file name\njar_name=app.jar\n\n## Permform some validation on input arguments, one example below\nif [ -z \"$1\" ] || [ -z \"$2\" ]; then\n        echo \"Missing arguments, exiting..\"\n        echo \"Usage : $0 arg1 arg2\"\n        exit 1\nfi\n\njava -jar $dir/$jar_name arg1 arg2\n```\n2. Copy your Bash script to the `/usr/local/bin` directory.\n```\ncp run.sh /usr/local/bin\n```\n3. Give execute permission to the script.\n```\nchmod u+x /usr/local/bin/test.sh\n```\n4. Now you can type just the word `run` or `run.sh` on command line, followed by any arguments necessary to run your script. The shell will auto-complete the script name and allow execution by pressing the enter key.\n\n## Pipefail\n[Set -euxo pipefail](http://blog.kablamo.org/2015/11/08/bash-tricks-eux/)\n\n## Bash script\nBash script for quickly generating AWS session credentials for a role in a spring boot application.properties file for local development.\n\n```bash\n#!/bin/bash  \n  \n# Create an application-OVERRIDES.properties file in \n# environment/src/main/resources, then this script will\n# load credentials for AWS SDK usage in your environment.\n#\n# Usage: run `./setAWSProperties.sh` from scripts or root app directory  \n  \nAMAZON_ACCESS_KEY_PROPERTY_NAME=\"amazon.access.key=\"  \nAMAZON_SECRET_KEY_PROPERTY_NAME=\"amazon.access.secretkey=\"  \nAMAZON_SESSION_TOKEN_PROPERTY_NAME=\"amazon.access.sessiontoken=\"  \n  \nwrite_aws_credentials () {  \n # get vault creds as json  \n JSON=$(aws-vault exec \u003cYOUR_AWS_ROLE_NAME\u003e --json)  \n ACCESS_KEY_ID=$(echo \"$JSON\" | jq -r .AccessKeyId)  \n SECRET_ACCESS_KEY=$(echo \"$JSON\" | jq -r .SecretAccessKey)  \n SESSION_TOKEN=$(echo \"$JSON\" | jq -r .SessionToken)  \n  \n # write a newline so the credentials are not appended onto an existing property  \n echo \"\" \u003e\u003e \"$1\"  \n  \n # find and delete existing aws session credentials  \n sed -i -e \"/$AMAZON_ACCESS_KEY_PROPERTY_NAME/d\" \"$1\"  \n sed -i -e \"/$AMAZON_SECRET_KEY_PROPERTY_NAME/d\" \"$1\"  \n sed -i -e \"/$AMAZON_SESSION_TOKEN_PROPERTY_NAME/d\" \"$1\"  \n  \n # set new aws sessions credentials  \n echo \"$AMAZON_ACCESS_KEY_PROPERTY_NAME;$ACCESS_KEY_ID\" \u003e\u003e \"$1\"  \n echo \"$AMAZON_SECRET_KEY_PROPERTY_NAME$SECRET_ACCESS_KEY\" \u003e\u003e \"$1\"  \n echo \"$AMAZON_SESSION_TOKEN_PROPERTY_NAME$SESSION_TOKEN\" \u003e\u003e \"$1\"  \n echo \"wrote aws session credentials to application.OVERRIDES.properties\"\n}  \n  \nOVERRIDES_FILE_DIR=\"environment/src/main/resources/application-OVERRIDES.properties\"\n  \nif [ -e \"$OVERRIDES_FILE_DIR\" ]; then  \n write_aws_credentials $OVERRIDES_FILE_DIR  \nelif [ -e \"../$OVERRIDES_FILE_DIR\" ]; then  \n write_aws_credentials \"../$OVERRIDES_FILE_DIR\"  \nelse  \n echo \"application-OVERRIDES.properties does not exist. Create it and re-run the script.\"  \nfi\n```\n\n## Print scripting language\n```\necho $0\n```","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/blueberry-oatmeal":{"title":"Blueberry Oatmeal","content":"\n## Steps \n- Make oatmeal on stove or in microwave.\n- Throw Trader Joe's organic frozen blueberries in halfway through.\n- Mix in desired amount of chia seeds and serve.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/bone-broth-rice":{"title":"Bone Broth Rice","content":"\n## Ingredients\n- Use 1:2 ratio cups of rice to cups of bone broth.\n - For example, 1 cup of rice and 2 cups of bone broth.\n- 1-2 tbs of organic grass fed butter.\n\n## Cooking Instructions\n- Bring 2 cup of organic bone broth and 1 tbs of butter to a light boil\n- Add 1 cup of Spanish rice, stir for one minute, then reduce to a covered simmer for 15-17 minutes.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/creamy-tortellini-spinach-chicken-soup":{"title":"Creamy Tortellini Spinach Chicken Soup","content":"\n## Cooking equipment\n- Instant Pot\n- 1 tbsp measure\n- 1/2 tsp measure\n- 1 cup measure\n- 1/2 cup measure\n## Ingredients\n- 1 tbsp of Olive oil\n- 1 yellow onion, diced\n- 3 tsp garlic minced\n- 4 cups chicken broth\n- 1 to 1 1/2 lbs boneless, skinless chicken thighs\n- 1 Tbsp dried basil\n- 2 Tbsp tomato paste\n- 2 (14.5 oz) cans petite diced tomatoes\n- 1 tsp salt\n- 1/2 tsp pepper\n- 1 cup heavy cream or 1 cup half and half\n- 4 cups frozen cheese tortellini (or you can use fresh)\n- 3 cups packed spinach (you can definitely add more than this if you want)\n- 1/2 cup Parmesan cheese\n## Steps\n1. Turn the Instant Pot on to the saute function, on the highest setting. Heat the oil in the bottom of the pot. Add in the diced onion and stir for a couple of minutes. Add in the garlic and stir until the onions are translucent, another couple of minutes.\n2. Add in the basil, tomato paste, chicken broth, tomatoes, chicken, salt and pepper. Give a quick stir. Cover the Instant Pot and secure the lid. Make sure valve is set to “sealing.” Press the manual pressure cook button and set the timer to 15 minutes (high pressure). Once the timer beeps let out the pressure by gently moving the valve to “venting.”\n3. Spoon the chicken out and place it on a cutting board. Cut the chicken into bite size pieces and then add it back into the pot. Stir in the tortellini, spinach, Parmesan cheese, and half and half. Turn the IP to saute to heat up the tortellini quickly. Once the tortellini is warmed through ladle the soup into bowls and serve.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/curl":{"title":"Curl","content":"\n## curl resources\n- [Curl Man Page](https://curl.se/docs/manpage.html)\n- [Curl Command Cheat Sheet](https://reqbin.com/req/c-kdnocjul/curl-commands)\n\n## useful curl commands\n- Pipe curl command output to `json_pp` to pretty print json response\n```\n$CURL_COMMAND | json_pp\n```\n- Curl Timeouts\n  - `curl --max-time \u003cseconds\u003e` or `curl -m \u003cseconds\u003e`\n- There is no difference between `-v`, `-vv`, and `-vvv` in curl\n  - [Explanation via Stack Overflow](https://stackoverflow.com/questions/24402473/what-is-meaning-of-vvv-option-in-curl-request)\n```\ncurl -vvv -sSL -o /dev/null 'https://google.com/'\n```","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/gcp-wiki":{"title":"GCP Wiki","content":"\n- Firestore vs. Firebase\n- [Google Cloud Run](/notes/google-cloud-run.md)","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/git":{"title":"Useful Git Commands","content":"\n## Stash Commands\n- Stash tracked files with a stash message\n```shell\ngit stash save \"stash message\"\n```\n--- \n- Stash tracked and untracked files (`-u` is shorthand for `--include-untracked`) with a stash message\n```bash\ngit stash save -u \"stash message\"\n```\n--- \n- View all stashes\n```bash\ngit stash list\n```\n--- \n- Peek at a stash\n```bash\ngit stash show -p stash@{0}\n```\n--- \n- Apply stash by id (Doesn't delete from stash)\n```bash\ngit stash apply stash@{0}\n```\n---\n- Unstash a single, specific file from a stash\n```bash\ngit diff stash@{0}^1 stash@{0} -- \u003cfilename\u003e\n```\n---\n- The `-p` and `-u`  flag step-by-step walks you through each changed/new file in your repo and asks \"Do you want to stash this?\"\n```\ngit stash save -p -u\n```\n---\n## Cleaning / Deletion\n- Remove a file from source control without deleting it:\n```\ngit rm --cached mylogfile.log\n```\n- Remove a directory from source control without deleting it:\n```\ngit rm --cached mylogfile.log\n```\n- In case you have **_not_** pushed the commit publicly yet, and want to undo it, keeping the changes in your working directory:\n```\ngit reset HEAD~1 --soft   \n```\n---\n- Dry (`-n`) run for git clean for recursive directory (`-d`) deletion of untracked files:\n```shell\ngit clean -n -d\n```\n---\n- Delete all untracked files from the repository (ignores files in .gitignore):\n```shell\ngit clean -f\n```\n---\n- Discard uncommitted, local changes\n```shell\ngit reset --hard\n```\n---\n- To delete a local branch\n```shell\ngit branch -d \u003clocal-branch\u003e\n```\n\n### I committed the deletion and then I did more commits\nIf you deleted a file, committed, then continued work and did more commits, only to find that deleting the file was a mistake, Git still has you covered! To find the right commit, first check the history for the deleted file:\n```\n$ git log -- \u003cfilename\u003e\n```\n\nYou can either work with the last commit that still had the file, or the commit that deleted the file. In the first case, just checkout the file from that commit:\n```\n$ git checkout \u003ccommit hash\u003e -- \u003cfilename\u003e\n```\n\nIn the second case, checkout the file from one commit _before_ that:\n```\n$ git checkout \u003cdeletion commit hash\u003e~1 -- \u003cfilename\u003e\n```\n---\n## Merging\n- Merge main into feature \n```\ngit checkout main\ngit pull\ngit checkout feature\ngit merge main\ngit push\n```\n---\n## Rebasing\n- Does an interactive rebase, starting at the most recent commit, and working to the Nth commit from HEAD to produce a squashed commit. The squashed commit replaces each commit starting with HEAD, and ending at N, inclusive.\n```shell\ngit rebase -i HEAD~N\n```\n---\n## Misc. Commands\n- Revert singular files back to their state in a previous commit hash\n```shell\ngit checkout $COMMIT_SHA -- file1/to/restore file2/to/restore\n```\n- To view local and remote branches\n```shell\ngit branch -a\n```\n--- \n- To view local and remote branches, and the last commit to each \n```shell\ngit branch -a -v\n```\n---\n\n- Show staged changes\n```shell\ngit diff --cached\n```\nor\n```shell\ngit diff HEAD\n```\n---\n- See number of commits by each author for a branch and their email\n```bash\ngit shortlog -sne \n```\n--- \n- See number of commits since a certain date:\n```bash\n- git shortlog -s -n --since \"JAN 1 2022\"\n```\n--- \n\n## Resources\n- [Restoring Deleted Files](https://www.git-tower.com/learn/git/faq/restoring-deleted-files)\n- git cheat sheet:\n- https://gitexplorer.com/","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/gitlab-ci-cd":{"title":"Gitlab CI/CD","content":"\n\n## Using `gitlab-runner` to test ci locally\n- If on macOS, install gitlab-runner with the below command\n```\nbrew install gitlab-runner\n```\n- To start service, run the below command\n```\nbrew services start gitlab-runner\n```\n- To restart the service, run the below command\n```\nbrew services restart gitlab-runner\n```\n- Official Gitlab Runner Docs: [Install GitLab Runner](https://docs.gitlab.com/runner/install/)\n- Create a command\n\t- The following `.gitlab-ci.yml` file defines a task named `build`:\n```yaml\nbuild:\n  script:\n    - echo \"Hello World\"\n```\n\n- Run the command locally [(_limitations apply!_)](https://docs.gitlab.com/runner/commands/index.html#limitations-of-gitlab-runner-exec)\n\n```yaml\ngitlab-runner exec shell build\n```\n\n### Consuming an npm package from private GitLab Package Registry\n\n1. Set registry and ensure authentication is configured via project access token. Add the following to your .npmrc at the root directory of your project that will be consuming from the private registry.\n```bash\n@my_scope:registry=https://gitlab.com/api/v4/projects/$PROJECT_ID/packages/npm/\n//gitlab.com/api/v4/packages/npm/:_authToken=$PROJECT_ACCESS_TOKEN\n//gitlab.com/api/v4/projects/$PROJECT_ID/packages/npm/:_authToken=$PROJECT_ACCESS_TOKEN\n```\n\n2. To install the package, run \n```bash\nnpm install @my-scope/my-package\n```\n\n## Links\n- [Npalm Terraform Gitlab Runner](https://github.com/npalm/terraform-aws-gitlab-runner/releases)","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/google-cloud-run":{"title":"Google Cloud Run","content":"\nnothing yet...","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/hot-garlic-honey-glazed-salmon":{"title":"Hot Garlic Honey Glazed Salmon","content":"\n## Ingredients\n- 12 oz. (340 g) Salmon, cut into 2-3 fillet strips\n- Salt\n- Black Pepper\n- 1 pinch of Cayenne Pepper\n- 2 tablespoons of Honey\n- 1 tablespoon warm water\n- 1 tablespoon of Olive Oil\n- 1 tablespoon of Minced Garlic\n- 1 tablespoon of Parsley\n- 1 1/2 teaspoons of Apple Cider Vinegar or Lemon Juice\n- 1/2 lemon, sliced into wedges\n## Instructions\n1. Season the surface of the salmon with salt, black pepper and cayenne pepper. Set aside.\n2. Mix the honey, water, apple cider vinegar or lemon juice and a pinch of salt together. Stir to combine well.\n3. Heat up an oven-safe skillet (cast-iron skillet preferred) on high heat. Add the olive oil. Pan-fry the salmon, skin side down first, for about 1 minute. Turn the salmon over and cook for 1 minute. Turn it over again so the skin side is at the bottom.\n4. Add the garlic into the pan, saute until slightly browned. Add the honey mixture and lemon wedges into the skillet, reduce the sauce until it's sticky.\n5. Finish it off by broiling the salmon in the oven for 1 minute or until the surface becomes slightly charred (optional step).\n6. Top the salmon with parsley and serve immediately. Internal temperature should reach 125 to 130 degrees Fahrenheit.\n\n## Oven Version\nI just use oil. I coat the entire fillet (or steak) in veggie oil or olive oil, and put it skin-side-down in an old pan into a preheated 450+ degree oven. You'll have to try this like 10 times in order to learn your exact oven temperature and salmon cooking time. When I cook a 140 gram (1/3 lb) fillet for myself that's about 3 cm (1 1/4 inch) thick, I cook it about 9 minutes 50 seconds. (Don't open the oven while it's cooking.) Eat the salmon the instant it doesn't burn your mouth. It should melt in your mouth.\n\nI've also seen people sprinkle sugar on top of the oil before cooking, and that comes out nicely. When peaches or apricots or plums are in season, sometimes I chop one up and add it on top of the salmon _after_ the salmon is cooked. Or, as you say, salmon goes well with soy sauce, sesame oil, etc. But properly cooked salmon is delicious all by itself with only veggie oil, so everything else is optional.","lastmodified":"2022-11-06T05:20:38.310382554Z","tags":null},"/notes/ip-address":{"title":"IP Address","content":"\nWhen dealing with IP issues, this [tool](https://tehnoblog.org/ip-tools/ip-address-in-cidr-range/) for verifying IP Address in CIDR Range is useful.","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/jo":{"title":"Sed","content":"\n## Shell command to create JSON\n[Create JSON with jo](https://jpmens.net/2016/03/05/a-shell-command-to-create-json-jo/)","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/jq":{"title":"jq - json parsing cli tool","content":"\n##### Modifying a key-value in a JSON file using jq in-place\n\n```bash\ncontents=\"$(jq '.name = \"newValue\"' package.json)\" \u0026\u0026 \\\necho \"${contents}\" \u003e package.json\n```\n\n##### Bash Script that utilizes jq to alter package.json to prep it for GitLab Registry NPM Package Upload.\n```bash\n#!/bin/bash  \n  \n# Update package.json \"name\" property, which is required  \n# to include the GitLab Registry scope \"@scope\"  \n# and add a \"publicConfig\" object property that contains  \n# a reference to the scope registry that the sdk will be  \n# uploaded to.  \n  \nPACKAGE_DIR=\"app\"  \nPACKAGE_JSON_FILE_NAME=\"package.json\"  \nGITLAB_REGISTRY_LOCATION=\"https://gitlab.com/api/v4/projects/$PROJECT_ID/packages/npm/\"  \n  \ncd ../$PACKAGE_DIR  \n  \n# Prepend GitLab Registry scope to .name property in package.json  \nCONTENTS=\"$(jq '.name = \"@scope/\" + .name' $PACKAGE_JSON_FILE_NAME)\" \u0026\u0026 \\  \necho \"${CONTENTS}\" \u003e temp1.json  \n  \nPUBLISH_CONFIG=$(echo '{ \"publishConfig\": { \"@scope:registry\": \"https://gitlab.com/api/v4/projects/$PROJECT_ID/packages/npm/\"} }' | jq .)  \necho \"${PUBLISH_CONFIG}\" \u003e temp2.json  \n  \n# Performs a jq slurp merge, merging the altered contents of the package.json  \n# with the publishConfig object  \nOUTPUT=\"$(jq -s 'add' temp1.json temp2.json)\"  \n  \n# Output new package.json for GitLab Registry upload  \necho \"${OUTPUT}\" \u003e package.json  \n  \n# clean up temporary files used in jq slurp merge  \nrm temp1.json temp2.json\n```","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/maven":{"title":"Maven Wiki","content":"## Maven Resources\n- [Apache Maven Docs: Maven CLI Options Reference](https://maven.apache.org/ref/3.8.1/maven-embedder/cli.html)\n- [Apache Maven Docs: Maven CI Friendly Versions](https://maven.apache.org/maven-ci-friendly.html)\n- [Apache Maven Docs: Maven Plugins](https://maven.apache.org/plugins/index.html)\n\t- [Difference b/t Common Maven Plugins](https://stackoverflow.com/questions/38548271/difference-between-maven-plugins-assembly-plugins-jar-plugins-shaded-plugi)\n\n## Maven Commands\n- To view the versions of every dependency used throughout your maven project:\n```\nmvn dependency:tree\n```\n\t\n## Maven Build Lifecycle\n- [Quick Summary of Maven Build Lifecycle from the Apache Website](https://maven.apache.org/guides/getting-started/maven-in-five-minutes.html#maven-phases)\n### Clean\n- `clean` is it's own build lifecycle phase (which can be thought of as an action or task) in Maven. `mvn clean install` tells Maven to do the `clean` phase in each module before running the `install` phase for each module.\n\n- What this does is clear any compiled files you have, making sure that you're really compiling each module from scratch.\n\n- Once we have the database populated in each environment, we should run a build and test stage.\n### Verify\n`mvn verify` performs any integration tests that maven finds in the project.\n### Install \n- `mvn install` – install the package into the local repository, for use as a dependency in other projects locally.\n- `mvn install` implicitly runs `mvn verify` and then copies the resulting artifact into your local maven repository which you usually can find under `C:\\Users\\username\\.m2\\repository` if you are using windows.\n### Package\n- `mvn package` – take the compiled code and package it in its distributable format, such as a JAR, WAR, or Docker image.\n- Both of `mvn install` and `mvn package` will compile your code, clean the `/target` folder, and place a new packaged JAR or WAR into that /target folder. The main difference: `mvn install` will also install the package into your local maven repository, for use as a dependency in other projects locally.\n## Managing Parent POM and Child POMs\nTo match a parent POM, Maven uses two rules:\n1. There is a pom file in project’s root directory or in given relative path.\n2. Reference from child POM file contains the same coordinates as stated in the parent POM file.\n\nMaven parent pom can contain almost everything and those can be inherited into child pom files e.g\n\n- Common data – Developers’ names, SCM address, distribution management etc.\n- Constants – Such as version numbers\n- Common dependencies – Common to all child. It has same effect as writing them several times in individual pom files.\n- Properties – For example plugins, declarations, executions and IDs.\n- Configurations\n- Resources\n### Example Parent POM\n```xml\n\u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsdr\"\u003e\n \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e\n\t\t\t\t\t \n \u003cgroupId\u003ecom.howtodoinjava.demo\u003c/groupId\u003e\n \u003cartifactId\u003eMavenExamples\u003c/artifactId\u003e\n \u003cversion\u003e0.0.1-SNAPSHOT\u003c/version\u003e\n \u003cpackaging\u003epom\u003c/packaging\u003e\n\t\t\t\t\t \n \u003cname\u003eMavenExamples Parent\u003c/name\u003e\n \u003curl\u003ehttp://maven.apache.org/\u003c/url\u003e\n\t\t\t\t\t \n \u003cproperties\u003e\n\t \u003cproject.build.sourceEncoding\u003eUTF-8\u003c/project.build.sourceEncoding\u003e\n\t \u003cjunit.version\u003e3.8.1\u003c/junit.version\u003e\n\t \u003cspring.version\u003e4.3.5.RELEASE\u003c/spring.version\u003e\n \u003c/properties\u003e\n\t\t\t\t\t \n \u003cdependencies\u003e\n\t\t\t\t\t \n \u003cdependency\u003e\n\t \u003cgroupId\u003ejunit\u003c/groupId\u003e\n\t \u003cartifactId\u003ejunit\u003c/artifactId\u003e\n\t \u003cversion\u003e${junit.version}\u003c/version\u003e\n\t \u003cscope\u003etest\u003c/scope\u003e\n \u003c/dependency\u003e\n\t\t\t\t\t \n \u003cdependency\u003e\n\t \u003cgroupId\u003eorg.springframework\u003c/groupId\u003e\n\t \u003cartifactId\u003espring-core\u003c/artifactId\u003e\n\t \u003cversion\u003e${spring.version}\u003c/version\u003e\n \u003c/dependency\u003e\n\t\t\t\t\t \n \u003c/dependencies\u003e\n\u003c/project\u003e\n```\n### Example Child POM\n```xml\n\u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsdr\"\u003e\n \n\u003c!--The identifier of the parent POM--\u003e\n \u003cparent\u003e\n\t \u003cgroupId\u003ecom.howtodoinjava.demo\u003c/groupId\u003e\n\t \u003cartifactId\u003eMavenExamples\u003c/artifactId\u003e\n\t \u003cversion\u003e0.0.1-SNAPSHOT\u003c/version\u003e\n\u003c/parent\u003e\n\t\n \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e\n \u003cartifactId\u003eMavenExamples\u003c/artifactId\u003e\n \u003cname\u003eMavenExamples Child POM\u003c/name\u003e\n \u003cpackaging\u003ejar\u003c/packaging\u003e\n\t\t\t\t\t \n \u003cdependencies\u003e\n\t \u003cdependency\u003e\n\t\t \u003cgroupId\u003eorg.springframework\u003c/groupId\u003e\n\t\t \u003cartifactId\u003espring-security\u003c/artifactId\u003e\n\t\t \u003cversion\u003e${spring.version}\u003c/version\u003e\n\t \u003c/dependency\u003e\n \u003c/dependencies\u003e\n\u003c/project\u003e\n```\n## Maven Snapshots\nA snapshot version in Maven is one that has not been released.\n\nThe idea is that **before** a `1.0` release (or any other release) is done, there exists a `1.0-SNAPSHOT`. That version is what _might become_ `1.0`. It's basically \"`1.0` under development\".\n\nThe difference between a \"real\" version and a snapshot version is that snapshots might get updates. That means that downloading `1.0-SNAPSHOT` today might give a different file than downloading it yesterday or tomorrow.\n\nUsually, snapshot dependencies should **only** exist during development and no released version (i.e. no non-snapshot) should have a dependency on a snapshot version.\n\nThe snapshot is _not_ necessarily more stable: it is just the latest build. The snapshot _precedes_ the actual release, it does not come after it.","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/node":{"title":"Node","content":"\n### Writing an array of JSON to a file from an API in Node\n- Requires axios\n```javascript\nconst axios = require('axios');\nconst { writeFile } = require('fs');\nconst API_BASE_URL = 'https://my-api.com';\nconst PROFILE_ENDPOINT = `${API_BASE_URL}/v1/profile`;\n\nlet failedUsers = 0;\n\nconst fileOptions = {\n    flag: 'a'\n};\n\nconst writeUser = (response) =\u003e {\n    const { data: { profile: { email, password } } } = response;\n\n    const obj = {\n        email,\n        password\n    };\n\n    writeFile(outputFileName, JSON.stringify(obj, null, 2) + ',' + '\\n', fileOptions, err =\u003e err ? console.error(err) : '');\n};\n\nconst logError = (err) =\u003e {\n    if (err) {\n        console.error(err);\n        failedUsers++;\n    }\n}\n\nconst seedUsers = async () =\u003e {\n    console.log(`Creating ${desiredProfileCount} profiles and writing login credentials to ${outputFileName}`);\n    const promises = [];\n\n    const requestBody = {\n        serviceData: [\n            \"CREDIT_CARD\"\n        ]\n    };\n\n    for (let i = 0; i \u003c desiredProfileCount; i++) {\n        const promise = axios.post(PROFILE_ENDPOINT, requestBody)\n            .then(writeUser)\n            .catch(logError);\n\n        promises.push(promise);\n    }\n\n    await Promise.all(promises);\n\n    console.log(`\\nUsers have been added. ${failedUsers} request(s) failed.`);\n}\n\nconst validateInput = (args) =\u003e {\n    let result = true;\n\n    if (args[0] === undefined || isNaN(args[0])) {\n        console.log(\"ERROR: 1st argument is not a number or is missing, please pass desired number of profiles\");\n        result = false;\n    }\n\n    if (args[1] === undefined) {\n        console.log(\"ERROR: 2nd argument missing, please pass an output file name.\");\n        result = false;\n    }\n\n    if (!result) {\n        console.log(\"Example Usage:   \\\"node asyncCreateUsers.js 10 output.json\\\"\");\n        console.log(\"The above example writes 10 profiles to output.json\\n\");\n    }\n\n    return result;\n};\n\nconst args = process.argv.slice(2);\nconst validInput = validateInput(args);\n\nconst desiredProfileCount = args[0];\nconst outputFileName = args[1];\n\nif (validInput) {\n    seedUsers();\n}\n\n```\n\n### `n` – Interactively Manage Your NodeJS Versions\n[**n**](https://github.com/tj/n), is an extremely simple Node version manager that can be installed via npm.\n\nSay you want Node.js v0.10.x to build [Atom](https://github.com/atom/atom).\n\n```bash\nnpm install -g n   # Install n globally\nn 0.10.33          # Install and use v0.10.33\n```\n\n```bash\nUsage:\nn                            # Output versions installed\nn latest                     # Install or activate the latest node release\nn stable                     # Install or activate the latest stable node release\nn \u003cversion\u003e                  # Install node \u003cversion\u003e\nn use \u003cversion\u003e [args ...]   # Execute node \u003cversion\u003e with [args ...]\nn bin \u003cversion\u003e              # Output bin path for \u003cversion\u003e\nn rm \u003cversion ...\u003e           # Remove the given version(s)\nn --latest                   # Output the latest node version available\nn --stable                   # Output the latest stable node version available\nn ls                         # Output the versions of node available\n```\n","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/obsidian":{"title":"Obsidian","content":"\n## Plugins\n- Vim\n- Kanban\n\n## Essentials\n\n### Basics\n\n_Open Graph View_ — `Cmd-g`\n_Make new note_ — `Cmd-n`\n_Create note in new pane_ — `Cmd-shift-n`  \n_Open the link you’re hovering over in a new pane (while in edit mode)_ — `Cmd-click`  \n_Toggle edit/preview mode_ — `Cmd-e`  \n_Open quick switcher_ — `Cmd-o`  \n_Close active pane_ — `Cmd-w`\n_Open command palette_ — `Cmd-p`  \n_Search and replace in current file_ — `Cmd-f`  \n_Search in all files_ — `Cmd-shift-f`\n_Open settings_ — `Cmd-,`  \n\n### Formatting Basics\n\n_Undo, cut, copy, paste, bold, italicize_ — `Cmd-z,x,c,v,b,i`","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/openapi":{"title":"OpenAPI","content":"\n### Why use OpenAPI?\nAs a tool, OpenAPI allows you to generate client SDKs for your API across dozens of supported languages. Building SDKs are an integral part of the developer experience because they: \n- Save huge amounts of development time\n- Dramatically reduce \"time to first API call\"\n- Embed optimization \u0026 best practices into API usage\n\n### Generate Node SDK with OpenAPI and Publish to GitLab Registry\nThis documentation was developed using this GitLab [documentation](https://docs.gitlab.com/ee/user/packages/npm_registry/#use-the-gitlab-endpoint-for-npm-packages)\n\n1. Create a Temporary Project Access Token with full permissions for testing the SDK upload to Gitlab Registry locally\n2. Generate the SDK\n```bash\nmkdir javascript-sdk\ncd javascript-sdk\nexport JS_POST_PROCESS_FILE=\"/usr/local/bin/js-beautify -r -f\"\nopenapi-generator generate -i $SWAGGER_URL -g javascript -o .\n```\n3. Install packages and build\n```bash\nnpm install\n```\n4. Ensure secure communication for private registries\n```bash\nnpm config set always-auth true\n```\n5. Deploying to GitLab Registry\n\t1. For local GitLab Registry uploads, we need a personal access token, with full registry write permissions.\n```bash\nnpm config set @your_scope/npm_package_name:registry https://gitlab.com/api/v4/projects/$PROJECT_ID/packages/npm/\n```\n```bash\nnpm config set -- '//gitlab.com/api/v4/projects/$PROJECT_ID/packages/npm/:_authToken' $AUTH_TOKEN\n```\n6. Publish to GitLab Registry\n```bash\nnpm publish\n```\n\n### Publishing an npm package to Gitlab locally\n1. Add the following to your generated SDK's package.json file\n```json\n{  \n  \"name\": \"@scope/package-name\"\n}\t\n```\n```json\n \"publishConfig\": {  \n    \"@scope:registry\": \"$GITLAB_API_V4/projects/$PROJECT_ID/packages/npm/\"\n } \n```\n2. Add the following to your `.npmrc` file\n```bash\n//gitlab.com/api/v4/projects/$PROJECT_ID/packages/npm/:_authToken=$PERSONAL_ACCESS_TOKEN\n//gitlab.com/api/v4/packages/npm/:_authToken=$PERSONAL_ACCESS_TOKEN\n@scope:registry=https://gitlab.com/api/v4/packages/npm/\n```","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/python":{"title":"Python","content":"\n## Useful Links\n- [Pypi: Find, install and publish Python packages](https://pypi.org/)\n\n## Python Virtual Environment Setup\n\nUsing **venv**, you can easily work with multiple projects with various dependencies on the same machine at the same time.\n\nTo setup virtual environment or venv on Python, first you will need **PIP**. It is the widely used packet manager for Python.\n\n**PIP** comes bundled with Python installation. On a Mac, Homebrew makes it easier to install Python along with **pip**. Simply, 🔥fire up your terminal and enter the following command:\n\n\u003e brew install python@3.9\n\nThe above command installs Python (latest version at the time of the writing) on your Mac. If you already have Python installed on your machine, you can check the version using the following command on you terminal.\n\n\u003e python -V\n\nYou can install the latest version of **pip** using the following. This command will automatically install the latest **pip** version.\n\npython3.9 -m pip install --user --upgrade pip\n\nNow, its time for you install the **venv** or **virtual environment** in Python using the following command.\n\npython3.9 -m pip install --user virtualenv\n\n### Create your virtual environment\n\nTo create a virtual environment, head to your project directory and run the following command.\n\npython3.9 -m venv venv\n\n### Activating your virtual environment\n\nBefore using your virtual environment on your project, you need to activate it using\n\nsource venv/bin/activate\n\nCongratulations 🎉, you’ve successfully installed **venv** and activated it. Install em’ packages now 🛠","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/recipes":{"title":"Recipes","content":"\n- [Air Fried Frozen Broccoli](/notes/air-fried-frozen-broccoli.md)\n- [Bone Broth Rice](/notes/bone-broth-rice.md)\n- [Blueberry Oatmeal](/notes/blueberry-oatmeal.md)\n- [Creamy Tortellini Spinach Chicken Soup](/notes/creamy-tortellini-spinach-chicken-soup.md)\n- [Hot Garlic Honey Glazed Salmon](/notes/hot-garlic-honey-glazed-salmon.md)\n- [Shrimp And Grits](/notes/shrimp-and-grits.md)\n- [Stuffed Bell peppers](/notes/stuffed-bell-peppers.md)","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/rudyard-kipling-if":{"title":"If By Rudyard Kipling","content":"\nIf you can keep your head when all about you  \n Are losing theirs and blaming it on you;  \nIf you can trust yourself when all men doubt you,  \n But make allowance for their doubting too;  \nIf you can wait and not be tired by waiting,  \n Or, being lied about, don’t deal in lies,  \nOr, being hated, don’t give way to hating,  \n And yet don’t look too good, nor talk too wise;\n\nIf you can dream—and not make dreams your master;  \n If you can think—and not make thoughts your aim;  \nIf you can meet with triumph and disaster  \n And treat those two impostors just the same;  \nIf you can bear to hear the truth you’ve spoken  \n Twisted by knaves to make a trap for fools,  \nOr watch the things you gave your life to broken,  \n And stoop and build ’em up with wornout tools;\n\nIf you can make one heap of all your winnings  \n And risk it on one turn of pitch-and-toss,  \nAnd lose, and start again at your beginnings  \n And never breathe a word about your loss;  \nIf you can force your heart and nerve and sinew  \n To serve your turn long after they are gone,  \nAnd so hold on when there is nothing in you  \n Except the Will which says to them: “Hold on”;\n\nIf you can talk with crowds and keep your virtue,  \n Or walk with kings—nor lose the common touch;  \nIf neither foes nor loving friends can hurt you;  \n If all men count with you, but none too much;  \nIf you can fill the unforgiving minute  \nWith sixty seconds’ worth of distance run—  \n Yours is the Earth and everything that’s in it,  \nAnd—which is more—you’ll be a Man, my son!","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/sed":{"title":"Sed","content":"\n```bash\nsed -i '/^all:/ s/$/ anotherthing/' file\n```","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/shrimp-and-grits":{"title":"Shrimp \u0026 Grits","content":"\n## Ingredients\n\n### Shrimp \u0026 Bacon Ingredients\n- 4 pieces of bacon, cut in 1/4-inch pieces\n- 1 pound of unfrozen shrimp, peeled and deveined, can be cooked or uncooked\n- 1/2 tsp of Cajun seasoning  \n- 1/2 tsp of salt\n- 1/4 tsp of black pepper\n- Pinch of cayenne pepper\n- 1 tbsp of minced garlic\n- 2 tbsp of minced green onion\n- 1 tbsp of minced jalapeno\n- 1 tbsp of chopped parsley\n\n### Sauce Ingredients\n- 1/4 cup of water\n- 2 tbsp of cream\n- 2 tsp of lemon juice\n- A few drops of Worcestershire sauce\n\n### Grits Ingredients\n- 1 cup white grits  \n- 4 cups water\n- 1 teaspoon salt  \n- 2 tablespoons butter  \n- 1/2 cup grated white cheddar\n\n## Cooking Instructions\n1. Season Shrimp\n- Take one pound of thawed, thoroughly dried shrimp and throw in bowl with 1/2 tsp Cajun seasoning, 1/2 tsp of salt, 1/4 teaspoon of black pepper, and pinch of cayenne pepper.\n2. Cook Grits\n\t- Add 4 cups of water, 1 tsp of salt and 2 tbsp of butter to pot\n\t- When it comes to a boil, whisk in grits\n\t- Turn down, bring to simmer, cook uncovered to time on the package.\n\t- When grits are done, turn off the stove, and stir in 1/2 cup of grated white cheddar. Cover to keep warm.\n3. Saute Chopped Bacon \u0026 Cook Shrimp\n\t- If you're using uncooked shrimp, remove the bacon from the pan once it is almost crisp, while keeping the fat in the pan. Aim for about one tablespoon of fat in the pan to use for cooking shrimp. Add the shrimp, while making sure the pan is hot.\n\t- if you're using cooked shrimp, wait until the bacon pieces are already mostly cooked, then throw the cooked shrimp to reheat.\n\t- Immediately throw in 1 tbsp of minced garlic, 2 tbsp of green onion, and 1 tbsp of minced jalapeno. Stir diligently.\n\t- Toss bacon back in with 1/4 cup of water, 2 tbsp of cream, 2 tsp of lemon juice, and Worcestershire sauce.\n\t- As sauce forms, mix in 1 tbsp of parsley.\n\t- Serve in a bowl, sauce over shrimp over grits.\n\t\n[Link to Original Recipe](https://foodwishes.blogspot.com/2010/03/shrimp-and-grits-breakfast-for-dinner.html)","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/software-engineering":{"title":"Software Engineering Wiki","content":"\n## Languages \u0026 Frameworks\n- [Spring Boot](/notes/spring-boot.md)\n- [Node](/notes/node.md)\n- [Python](/notes/python.md)\n- [Rust](42_Rust_priv.md)\n## CLI\n- [Bash](/notes/bash.md)\n- [Unix Commands](unix-commands.md)\n- [Git](/notes/git.md)\n- [curl](/notes/curl.md)\n- [jed](/notes/sed.md)\n- [jo](/notes/jo.md)\n- [jq](/notes/jq.md)\n- [jEnv](https://github.com/jenv/jenv)\n\t- [jEnv.be](https://www.jenv.be/)\n\t- To have your current jEnv version set in maven:\n\t```bash\n\tjenv enable-plugin maven\n\t```\n\n## Application Monitoring\n- [SigNoz](https://github.com/SigNoz/signoz) (Open-source alternative to DataDog and New Relic)\n## Infrastructure as Code\n- [Terraform](/notes/terraform.md)\n## CI/CD \n- [GitLab CI/CD](/notes/gitlab-ci-cd.md)\n## Build Tools \u0026 Containers\n- [Docker](/notes/Docker.md)\n- [Maven](/notes/maven.md)\n- [Nix](https://nixos.org/)\n\t- [ant-dateutils example](https://github.com/vlktomas/nix-examples/tree/master/desktop/Java/ant-dateutils)\n## Documentation\n- [OpenAPI](/notes/openapi.md)\n### Architecture Diagrams\n- [PlantUML](https://plantuml.com/) (Architecture Diagrams as Code)\n- [Draw.io](https://app.diagrams.net/) (Traditional Architecture Diagrams)\n## Shortcuts\n- [Vim](/notes/vim.md)\n- [VS Code](/notes/vs-code.md)\n- [Intellij Shortcuts](intellij-shortcuts.md)\n## Cloud\n- [GCP Wiki](/notes/gcp-wiki.md)\n## Misc.\n- [Regex Generator](https://regex-generator.olafneumann.org/)\n- [SourceGraph Code Search](https://sourcegraph.com/search)\n- [Awesome Git Repositories](/notes/awesome-git-repositories.md)\n- [Tech Articles](/notes/tech-articles.md)\n- [Bitly.com](https://bitly.com/)","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/spring-boot":{"title":"Spring Boot","content":"\n- [Links to each Spring project](https://spring.io/projects)\n\n## Spring Data JPA\n- [JPQL Docs - Apache](https://openjpa.apache.org/builds/1.0.1/apache-openjpa-1.0.1/docs/manual/jpa_overview_query.html)\n- [Spring Data JPA Official Docs](https://docs.spring.io/spring-data/jpa/docs/current/reference/html/)\n- Sorting With Spring Data\n\t- [Baeldung: Sorting Query Results with Spring Data](https://www.baeldung.com/spring-data-sorting)\n\t- [Baeldung: Pagination and Sorting](https://www.baeldung.com/spring-data-jpa-pagination-sorting)\n\t- [Dynamic sorting with the Spring Data Sort object](https://attacomsian.com/blog/spring-data-jpa-sorting): A replacement for the `ORDER BY` clause used in classic SQL\n- Implementation Guides and Tutorials\n\t- [Baeldung: @Query](https://www.baeldung.com/spring-data-jpa-query)\n\t- [Medium: Pagination example queries in JPQL](https://medium.com/@sindepal/spring-data-jpa-query-and-pageable-15f8c3e7fe4e)\n\t- [Baeldung: Transaction Management](https://www.baeldung.com/transaction-configuration-with-jpa-and-spring)\n\t- [Wildcard Queries](https://www.amitph.com/spring-data-jpa-wildcard-query/)\n\t- [Baeldung: Setting up MetaModel](https://www.baeldung.com/hibernate-criteria-queries-metamodel)\n\t- [JPA Specifications - Interactive Tutorial](https://www.logicbig.com/tutorials/spring-framework/spring-data/combined-specifications.html)\n\t- [JPA Specifications - Interactive Tutorial](https://www.logicbig.com/tutorials/spring-framework/spring-data/combined-specifications.html)\n\t- [Hibernate docs - JPA MetaModel](https://docs.jboss.org/hibernate/jpamodelgen/1.0/reference/en-US/html_single/#whatisit)\n- Helpful Stack Overflow Posts\n\t- [Difference Between Criteria, Predicate and Specification](https://stackoverflow.com/questions/47469861/what-is-the-difference-between-a-criteria-a-predicate-and-a-specification)\n\t- [Criteria API vs QueryDSL vs JPA MetaModel](https://stackoverflow.com/questions/53325506/criteria-api-vs-querydsl-vs-jpa-metamodel)\n\t- [JPA Specifications](https://stackoverflow.com/questions/48647847/jpa-specifications-by-example)\n\n\n## Spring Boot Actuator Management Endpoints\n- [Spring Boot Actuator Endpoints Documentation](https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#actuator.endpoints)\n- [A useful tutorial for Spring Boot Actuator](https://howtodoinjava.com/spring-boot/actuator-endpoints-example/)\n\n## jEnv\n- [For easily managing Java versions across projects](https://www.jenv.be/)\n\n## Custom Banner in Spring Boot\nIn short, just create a `/src/main/resources/banner.txt` and/or `/src/main/resources/banner.gif`  \n- [Spring Banner Tutorial](https://springhow.com/spring-boot-startup-banner/)\n- You can even have different banners per environment, by adding the following to your application-\\*.yml\n\t- Having custom banners per env makes it easy to know what you are running.\n```yml\nspring:\n  banner:\n    location: banner-crt.txt\n```","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/stuffed-bell-peppers":{"title":"Stuffed Bell Peppers","content":"\nStuffed bell peppers with ground beef and rice and bake. Sprinkle cheese on top.","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/tech-articles":{"title":"Tech Articles","content":"\n- https://scrapeops.io/blog/the-state-of-web-scraping-2022/\n- https://freenetproject.org/\n- https://arxiv.org/abs/2201.00223\n- https://www.youtube.com/watch?v=cOWjwwKSR78\n- [Raspberry PI E-Ink Dashboard](https://lengrand.fr/complete-setup-epaper/)","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/terraform":{"title":"Terraform Wiki","content":"## Terraform Resources\n- [Resource Dependencies](https://www.terraform.io/language/resources/behavior#resource-dependencies)\n- [Terraform Workspaces](https://www.terraform.io/language/state/workspaces#workspaces)\n\u003e ### When to use Multiple Workspaces\n\u003e Named workspaces allow conveniently switching between multiple instances of a _single_ configuration within its _single_ backend. They are convenient in a number of situations, but cannot solve all problems.\n\u003e A common use for multiple workspaces is to create a parallel, distinct copy of a set of infrastructure in order to test a set of changes before modifying the main production infrastructure. For example, a developer working on a complex set of infrastructure changes might create a new temporary workspace in order to freely experiment with changes without affecting the default workspace.\n## Automation\n- [Automating Terraform](https://learn.hashicorp.com/tutorials/terraform/automate-terraform)\n\u003e ### Pass `terraform plan` output to `terraform apply` in CI\n\u003e When running in an orchestration tool, it can be difficult or impossible to ensure that the `plan` and `apply` subcommands are run on the same machine, in the same directory, with all of the same files present.\n## Terraform Commands\n- `terraform refresh` is effectively an alias for `terraform apply -refresh-only -auto-approve` which is why it should _NEVER_ be used. It is far too risky to run `terraform refresh` without first reviewing the proposed state changes.\n- Instead, use `terraform apply -refresh-only`. This alternative command will present an interactive prompt for you to review and confirm the proposed state changes. After confirming, your Terraform remote state will be updated to match the settings of your managed remote objects. More info can be found about this in the Hashicorp documentation [here](https://www.terraform.io/cli/commands/refresh#command-refresh).\n- `terraform init`\n\t- The `terraform init` command is used to initialize a working directory containing Terraform configuration files. This is the first command that should be run after writing a new Terraform configuration or cloning an existing one from version control. It is safe to run this command multiple times.\n- Terraform prints output values to the screen when a configuration is applied, but Terraform can also query all output values with the `terraform output` command, or selectively query certain output values using the below command template:\n```\nterraform output $RESOURCE_NAME\n```\n## Terraform Tips and Tricks\n- You can use `-target` to plan/apply a [specific Terraform resource](https://devops.stackexchange.com/questions/4292/terraform-apply-only-one-tf-file) within a module. **Be careful**, this is an advanced way to use Terraform and could break your infrastructure if done improperly.\n```\nterraform apply -target=aws_security_group.my_sg\n```\n- Turn on Verbose Logging by setting the TF_LOG environment variable to 1. 0 turns off logging.\n```\nexport TF_LOG=1\n```\n- `tfenv` is a useful cli tool that can be used to easily switch between different versions of Terraform.\n```\nbrew install tfenv\ntfenv install 1.0.0\ntfenv use 1.0.0\nterraform version # verify you're using the right version\n```\n\n- If you place a `.terraform-version` hidden file at the root of your terraform directory, `tfenv install` (no argument) will install the version written in it.","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/tor":{"title":"Tor","content":"\nThe core principle of Tor, [Onion routing](https://en.wikipedia.org/wiki/Onion_routing \"Onion routing\"), was developed in the mid-1990s by United States Naval Research Laboratory employees, mathematician Paul Syverson, and computer scientists Michael G. Reed and David Goldschlag, to protect U.S. communications online. Onion routing is implemented by encryption in the application layer of the communication protocol stack, nested like the layers of an onion. The alpha version of Tor, developed by Syverson and computer scientists Roger Dingledine and Nick Mathewson and then called The Onion Routing project (which later simply became \"Tor\", as an acronym for the former name, was launched on 20 September 2002. The first public release occurred a year later.\n\nIn 2004, the Naval Research Laboratory released the code for Tor under a free license, and the Electronic Frontier Foundation (EFF) began funding Dingledine and Mathewson to continue its development. In 2006, Dingledine, Mathewson, and five others founded The Tor Project, a Massachusetts-based 501(c)(3) research-education nonprofit organization responsible for maintaining Tor. The EFF acted as The Tor Project's fiscal sponsor in its early years, and early financial supporters of The Tor Project included the U.S. Bureau of Democracy, Human Rights, and Labor and International Broadcasting Bureau, Human Rights Watch, the University of Cambridge, and Google.\n\nOver the course of its existence, various Tor [attacks and weaknesses](https://en.wikipedia.org/wiki/Tor_(network)#Weaknesses) have been discovered and occasionally used. Attacks against Tor are an active area of academic research.","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/unix-commands":{"title":"Unix Commands ","content":"\n## Cheat Sheets\n- [Stanford's Basic Unix Commands](http://mally.stanford.edu/~sr/computing/basic-unix.html)\n\n## Users / Groups\nTo get the current user in a shell or script, run the below command\n```bash\nwhoami\n```\nor\n```bash\necho \"$USER\"\n```\n## SSH\n- Template command to remotely run a script and immediately exit, without caring about the result. This is useful for automation scripts that involve running multiple commands on remote servers.\n```\nssh -q $SSH_USER@$SERVER \"nohup /appl/bin/start.sh start \u003e /dev/null 2\u003e\u00261 \u0026 \"\n```\n\n## Verify DNS Resolution\n```shell\nnslookup www.google.com\n```\n\n## IP Address\n### Public IP Address of local machine \n```shell\ncurl ifconfig.me\n```\n\n### Private IP Address of local machine\n```shell\nifconfig -a\n```\n\n## Grand total size of all the subdirectories in the current directory\n- `du -sh -- *`\n\n## Process ID Number\n`lsof -p PID` will list all the files that have been touched by the currently running process\n\n## egrep\n- Search contents of every file for this matching text:\n`find . -type f -exec egrep -lH search_for_me '{}' ';'`\n\n## find command\n- Find every file named config.txt in your home directory:\n`find ~ -name \"config.txt\"`\n\n## scp file transfer commands\n- [scp command cheat sheet](https://linuxize.com/post/how-to-use-scp-command-to-securely-transfer-files/)\n* Copy a file from a local to a remote system:\n`scp file.txt ssh_user@hostname:/tmp`\n\n## list/installation/deletion of certs with keytool\n- List all certs in a keystore: `keytool -list -keystore \u003cPATH_TO_CACERTS\u003e -storepass changeit -noprompt`\n- Add a cert to a keystore: `keytool -import -trustcacerts -keystore \u003cPATH_TO_CACERTS\u003e -storepass changeit -noprompt -alias \u003cALIAS\u003e -file \u003cPATH_TO_NEW_CERT\u003e`\n- Delete cert in a keystore by alias: `keytool -delete -alias \u003cALIAS\u003e -keystore \u003cPATH_TO_CACERTS\u003e`\n\n## OpenSSL commands\n- Verify contents of a cert: `openssl x509 -in \u003cPATH_TO_CERT\u003e -text`\n- View certs of a client: `openssl s_client -showcerts -connect google.com:636`\n\n## Change Ownership of Symlink\n- `chown -h USER:GROUP jre` if you want to change the ownership of the symlink itself, not the destination directory\n \n## What's using this port?\n- Handy command for Mac to check what process is using a port \n```\nsudo lsof -i :3306\n```\n- If you're on Mac You can also look up and stop processes via the Activity Monitor GUI.\n\t- If you wanted to stop a `java` process, for example, you could go to the `Disk` section, and filter by `java` processes.\n","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/vim":{"title":"Vim","content":"\n## Why Vim?\nLearning to use Vim is worth it for several reasons. First, Vim (or vi) is almost guaranteed to be installed on any remote server you may need to ssh into. Second, although Vim was released in 1991, it’s widely available as a plugin on modern IDEs, like Intellij, VS Code, Eclipse, etc. Third, once you’re comfortable with the controls, Vim allows you to quickly navigate to any place on the page with precision. Combining that with the ability to record chains of commands and replay them, it allows for programmatic editing of files on the fly. Once you’re comfortable with Vim, you have a text editor at your service that you can on virtually any server.\n\n## Vim Commands\n- [Vim Cheat Sheet](https://vim.rtorr.com/)\n\n### Sort Commands\nThe following command sorts all lines and removes duplicate lines:\n```\n:sort u\n```\n\nhttps://stackoverflow.com/questions/26981192/how-do-i-remove-non-duplicate-lines-in-vim\n\n### Search Commands\nSearch for lines containing this pattern `- **`: `?` + `- \\*\\*`\n\nFind and Replace: `%s/foo/bar/g`\n\nThe traditional approach to find lines not matching a pattern is using the :v command:\n  - `:v/Warning/p`\n  - `:v/Warning/d` (NOTE: this deletes every line that does NOT match this pattern)\n\n### Navigation Commands\n- `)` or `shift + 0` to drop to the next newline\n- `u` is undo and `ctrl + r` to redo\n\n### Record Macro Commands\n- Press `q` then any letter to start recording in a register (ex: `qq`)\n- Perform command(s)\n- Press `q` to stop recording\n- Press `N@q` to run the macro stored in the q register N times\n\n### Window Panes in Vim\n- Add this to the .vimrc in your home user's directory to use the mouse in vim: `set mouse=a`\n- Use `:Vex` to open a filesystem navigation window in vim\n- Use `:ter` or `:vert ter` to open up a shell window\n- The mouse access allows you to resize windows and click through a filesystem menu\n\n### Regex for Number Searches\nRegex syntax is a little crazy in Vim, but being able to record macros that search for certain patterns is very useful for extracting data.\n\nSearch numbers of fixed length say 5 (matches `12345`, `123456`). Numbers more than 5 digits contain substring of 5 digits, and are also found by this regex.\n```\n/\\d\\{5\\}\n```\n\nword boundary start:\n```\n\\\u003c\n```\n\nword boundary end:\n```\n\\\u003e\n```\n\nThen use below to search numbers of exact 5 digits (match 12345 but not 123456):\n```\n/\\\u003c\\d\\{5\\}\\\u003e\n```\n\nUse below to search numbers of 5 or more digits:\n```\n/\\\u003c\\d\\{5,\\}\\\u003e\n```\n\nUse below to search numbers of 5 to 8 digits:\n```\n/\\\u003c\\d\\{5,8\\}\\\u003e\n```\n\nUse below to search numbers of 8 or less digits:\n```\n/\\\u003c\\d\\{,8\\}\\\u003e\n```\n\nShortcut numbers of 1 or more digits\n\n```\n/\\d\\+\n```\n\n## .vimrc file\n```\nset nu\nset relativenumber\nset scrolloff=8\nset mouse=a\nset ttymouse=sgr\nset clipboard=unnamed\n```","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null},"/notes/vs-code":{"title":"VS Code","content":"\nSearch for files: `Cmd ⌘+p`","lastmodified":"2022-11-06T05:20:38.31838283Z","tags":null}}