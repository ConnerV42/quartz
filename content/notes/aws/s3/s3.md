---
title: "S3"
tags:
- aws
- s3
disableToc: false
---

### Related Notes
- [AWS Wiki](/notes/aws/aws-wiki.md)
- [[s3-access-logs]]
- [[s3-access-points]]
- [[s3-cors]]
- [[s3-encryption]]
- [[s3-events]]
- [[s3-inventory]]
- [[s3-lifecycle-configuration]]
- [[s3-object-lock]]
- [[s3-object-storage-class]]
- [[s3-object-versioning-and-mfa-delete]]
- [[s3-presigned-urls]]
- [[s3-replication]]
- [[s3-select]]
- [[s3-security]]
- [[s3-static-website-hosting]]

## **Simple Storage Service (S3)**
- Amazon S3 now provides increased performance to support at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data, which can save significant processing time for no additional charge. Each S3 prefix can support these request rates, making it simple to increase performance significantly.
- S3 supports Multi-Factor Authentication Deletes, allowing you to secure your S3 objects from accidental deletion or overwrite.
- In S3, all objects are private by default. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your project, you must provide security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration. Anyone who receives the pre-signed URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a pre-signed URL.
- The S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket.
	- Amazon S3 supports the following destinations where it can publish events:
		- Amazon SNS topic
		- Amazon SQS queue
		- AWS Lambda
	- In SNS, the fanout scenario is when a message publish to an SNS topic is replicated and pushed to multiple endpoints, such as Amazon SQS queues, HTTP(S) endpoints, and Lambda functions. This allows for parallel asynchronous processing. For example, you can develop an application that publishes a message to an SNS topic whenever an order is placed for a product. Then, SQS queues that are subscribed to the SNS topic receive identical notifications for the new order. An Amazon EC2 server instance attached to one of the SQS queues can handle the processing or fulfillment of the order. And you can attach another Amazon EC2 server instance to a data warehouse for analysis of all orders received.
	- Based on the given scenario, the existing setup sends the event notification to an SQS queue. Since you need to send the notification to the development and operations team, you can use a combination of Amazon SNS and SQS. By using the message fanout pattern, you can create a topic and use two Amazon SQS queues to subscribe to the topic. If Amazon SNS receives an event notification, it will publish the message to both subscribers.
	- Take note that Amazon S3 event notifications are designed to be delivered at least once and to one destination only. You cannot attach two or more SNS topics or SQS queues for S3 event notification. Therefore, you must send the event notification to Amazon SNS.
- How would a medical company, storing Personally Identifiable Info of users in an S3 bucket ensure that the master keys and unencrypted data is never sent to AWS?
	- Use S3 client-side encryption with a client-side master key.
		- **Client-side encryption** is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options:
			- Use an AWS KMS-managed customer master key
			- Use a client-side master key
- How would you aggregate 500 GB of data daily to a forecasting application hosted in the Northern Virginia region? Use Transfer Acceleration in the destination bucket and upload the collected data using Multipart Upload.
- Amazon S3 has life cycle configurations. With S3 life cycle rules, you can transition files to S3 Standard IA or S3 Glacier. Using S3 Glacier expedited retrieval, you can quickly access your files within 1-5 minutes.
- Amazon S3 IA (Infrequently Accessed) is for exactly what it sounds like, infrequent access (but rapid retrieval when accessed). S3-IA is not highly available, because unlike standard s3, it is only relies on one availability zone for storing data.
- Amazon S3 Glacier, on the other hand, is for Deep Archival of data, and is the deepest level of archival you can achieve with S3. Even though this data is not meant to be retrieved frequently, you can make use of **Expedited Retrievals** to quickly access your data when occasional urgent requests for a subset of archives are required. For all but the largest archives (250 MB+), data accessed using Expedited retrievals are typically made available within 1-5 minutes. **Provisioned Capacity** is what ensures that retrieval capacity for Expedited Retrievals is available when you need it.
	- To make an Expedited, Standard, or Bulk retrieval, set the Tier parameter in the Initiate Job (POST jobs) REST API request to the option you want, or the equivalent in the AWS CLI or AWS SDKs. If you have purchases provisioned capacity, then all expedited retrievals are automatically served through your provisioned capacity.
	- Each unit of **Provisioned Capacity** provides at least 3 expedited retrievals that can be performed every 5 minutes and provides up to 150 MB/s of retrieval throughput. You should purchase provisioned retrieval capacity if your workload requires highly reliable and predictable access to a subset of your data in minutes. Without provisioned capacity Expedited retrievals are accepted, except for rare situations of unusually high demand. However, if you require access to Expedited Retrievals under all circumstances, you must purchase provisioned retrieval capacity.
- If a company uses multiple AWS accounts that are consolidated with AWS Organizations, and wants to copy several S3 objects to another S3 bucket that belonged to a different AWS account (that they also own), how would that be accomplished?
	- So, cross-account permissions in S3 would be needed. This can be accomplished by creating an IAM customer-managed policy that allows an IAM user or role to copy objects from the source bucket in one account to the destination bucket in another account. By default, an S3 object is owned by the account that uploaded the object.That's why granting the destination account the permissions to perform the cross-account copy makes sure that the destination owns the copied objects. You can also change the ownership of an object by changing its access control list (ACL) to bucket-owner-full-control.
- You can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints. A gateway endpoint is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. Interface endpoints extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on premises, or from a VPC in another AWS Region using VPC peering or AWS Transit Gateway.
- S3 supports cross region replication to ensure that your S3 bucket is not affected even if there is an outage in one of the AZs or a regional outage. When you upload data to S3, your objects are stored redundantly on multiple devices across multiple facilities within the region only, where you created the bucket. Thus, if you want to be fault tolerant to a regional failure, you *must* enable cross region replication on your S3 bucket.
- Amazon S3 is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon S3. CloudTrail captures a subset of API calls for Amazon S3 as events, including calls from the Amazon S3 console and code calls to the Amazon S3 APIs. AWS CloudTrail logs provide a record of actions taken by a user, role, or an AWS service in Amazon S3, while Amazon S3 server access logs provide detailed records for the requests that are made to an S3 bucket.
	- So, enable *server access logging* if you want to log every request access to their S3 buckets including requester, bucket name, request time, referrer, turnaround time, and error code info.
- Objects must be stored **at least 30 days** in the current storage class before you can transition them to STANDARD_IA or ONEZONE_IA. For example, you cannot create a lifecycle rule to transition objects to the STANDARD_IA storage class one day after you create them. Amazon S3 doesn't transition objects within the first 30 days because newer objects are often accessed more frequently or deleted sooner than is suitable for STANDARD_IA or ONEZONE_IA storage. Glacier is different, you can transition objects to Glacier after 7 days.

### Amazon S3 Glacier
- An Amazon S3 Glacier vault can have one resource-based vault access policy and one Vault Lock policy attached to it. A *Vault Lock* policy is a vault access policy that you can lock. Using a Vault Lock policy can help you enforce regulatory and compliance requirements. Amazon S3 Glacier provides a set of API operations for you to manage the Vault Lock policies.
	- As an example of a Vault Lock policy, suppose that you are required to retain archives for one year before you can delete them. To implement this requirement, you can create a Vault Lock policy that denies users permissions to delete an archive until the archive has existed for one year. You can test the policy before locking it down, but after you lock the policy, it becomes immutable.

### Glacier Vault Lock
Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual Glacier vaults with a vault lock policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed. Hence, **using Amazon S3 Glacier Vault Lock** is the correct answer.

Glacier enforces the controls set in the vault lock policy to help achieve your compliance objectives, for example, for data retention. You can deploy a variety of compliance controls in a vault lock policy using the AWS Identity and Access Management (IAM) policy language.

A vault lock policy is different than a vault access policy. Both policies govern access controls to your vault. However, a vault lock policy can be locked to prevent future changes, providing strong enforcement for your compliance controls. You can use the vault lock policy to deploy regulatory and compliance controls, which typically require tight controls on data access. In contrast, you use a vault access policy to implement access controls that are not compliance related, temporary, and subject to frequent modification. Vault lock and vault access policies can be used together.

For example, you can implement time-based data retention rules in the vault lock policy (deny deletes) and grant read access to designated third parties or your business partners (allow reads).

Locking a vault takes two steps:
1. Initiate the lock by attaching a vault lock policy to your vault, which sets the lock to an in-progress state and returns a lock ID. While in the in-progress state, you have 24 hours to validate your vault lock policy before the lock ID expires.
2. Use the lock ID to complete the lock process. If the vault lock policy doesn’t work as expected, you can abort the lock and restart from the beginning.

### S3 Gotchas
These 2 ARNs are not the same, and some policies need just one, or both:
`arn:aws:s3:::catgifs` - Acts on the bucket itself
`arn:aws:s3:::catgifs/*` - Acts on the objects in the bucket, as a wildcard
